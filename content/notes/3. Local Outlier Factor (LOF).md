---
sticker: emoji//1f4d5
tags:
  - Anomaly_Detection
  - 이상탐지
  - 밀도기반
  - density-based
---
# 🎲Local Outlier Factors (LOF)

- 단일 논문인데도 인용이 만 번이상 되었을 만큼 이상탐지 기법에서 중요한 알고리즘

#### Motivation 

- 특정 instance에 대한 Novelty score 를 계산할 때 그 데이터 주변의 **local density**를 고려하자.

![[Pasted image 20240117170522.png|center]]

$o_1$는 누가봐도 abnormal data다. $o_2$와 $o_3$를 살펴보면, $o_2$ 가 갖는 이웃간의 거리들의 평균이 $o_3$ 가 갖는 이웃간 거리의 평균보다 짧다. 그런데 LOF 에서는 $o_2$의 abnormal score가 $o_3$의 abnormal score보다 더 크게 계산된다. 이는 Local density를 반영한 결과이기 때문이다.

#### Definition 1: $k$ - distance of an object $p$

어떤 자연수 $k$ 가 있고, 우리가 보고 있는 특정 영역의 한 점인 객체 $p$ 가 있다고 하자. 그렇다면 $p$ 의 $k$ - distance라는 것 ($k$ - distance$(p)$라고 표기함)은 $p$와 $D$의 객체 $o$ 간의 거리 $d(p,o)$ 로 정의된다. 여기서 두 가지 조건을 만족해야 하는데, 아래와 같다 : 
1) $D\backslash\{p\}$ 의 객체 $o^\prime$ 에 대해서 적어도 $k$ 개가 ($k$개 이상이) 아래 식을 만족해야 함 : $$d(p,o^\prime)\le d(p,o)$$
	- 여기서 $D\backslash\{p\}$는 객체 $p$ 를 제외한 $D$를 의미함
	- 즉 $D\backslash\{p\}$에 위 식을 만족하는 $o^\prime$이 최소한 $k$ 개 존재해야 함을 의미


2) $D\backslash\{p\}$ 의 객체 $o^\prime$에 대해서 최대로 $k-1$ 개에 대해서 아래를 만족할 수 있다 : $$d(p,o^\prime)<d(p,o)$$
	- 즉, $D\backslash\{p\}$에 위 식을 만족하는 객체 $o^\prime$ 이 최대 $k-1$ 개까지 존재할 수 있음을 의미

이 개념은 복잡해 보일 수 있으나, 단순하게 생각하면 동률 (ties)을 고려했을 때 $k$ 번째 nearest neighbor 이다. 

![[Pasted image 20240117172559.png|center]]

위 5개의 거리를 오름차순으로 나열해 보면 $\{0.5, 1.0, 1.5, 1.5, 1.5\}$ 이다. 이 때 객체 $p$ 를 제외하고 3-distance of $p$ : 1.5 보다 작거나 같은 $o^\prime$ 의 개수는 5개이다. 이는 $k = 3$보다 크거나 같다 ⇒ 첫 번째 조건 만족. 그리고 3-distance of $p$ : 1.5 보다 작은 $o^\prime$ 의 개수는 2개이다. 이는 $k=3$ 보다 작다 ⇒ 두 번째 조건 만족. 따라서 최종적으로 $3$ - distance$(p)$ = 1.5 가 된다. 


![[Pasted image 20240117173153.png|center]]
![[Pasted image 20240117173243.png|center]]

#### Definition 2: $k$ - distance neighborhood of an object $p$

- $k$ - distance of $p$ 는 거리 개념이고, 이 두 번째 정의에서 말하는 $k$ - distance neighborhood 는 객체들의 집합을 말하는 것이다 : 
$$N_k(p) = \{q\in D\backslash\{p\}|d(p,q)\le k\text{ - distance}(p)\}$$

![[Pasted image 20240117173703.png|center]]

- $|N_3(p)|=5$

![[Pasted image 20240117173720.png|center]]

- $|N_3(p)|=4$

![[Pasted image 20240117173818.png|center]]

- $|N_3(p)|=4$

여기까지가 어떠한 객체 $p$ 의 $k$ - distance ($k$ - distance$(p)$)와 그 객체 $p$ 의 $k$ - distance of neighborhood ($N_k(p)$)의 개념이다. 

#### Definition 3: reachability distance
- "도달 가능 거리"
- 주어진 hyperparameter $k$에 대한 $p$ 에서 $o$ 까지의 reachability distance : $$\text{reachability-distance}_k(p,o)=\max\{k\text{ - distance}(o), d(p,o)\}$$
	- <font style="color:red">주의</font> : max()에서 k-distance는 $p$ 가 아니라 $o$ 에 관한 것!
	- $o$ 의 $k$ - distance와 $p$ 와 $o$ 의 실제 거리 중 큰 값 

![[Pasted image 20240117174835.png|center]]

- k-distance 안쪽에 있는 이웃들의 거리 ($d(p,o)$)를 k-distance로 치환하는 역할을 하는 것이 rechability distance 인 것이다. 
- 반면에 k-distance 바깥쪽에 있는, 즉 k-distance 보다 긴 거리를 갖는 이웃들에 대해서는 실제 거리인 $d(p,o)$ 값을 갖게 한다. 

#### Definition 4: local reachability density of an object $p$

$$lrd_k(p) = \frac{|N_k(p)|}{\sum_{o\in N_k(p)}\text{reachability-distance}_k(p,o)}$$
- 객체 $p$ 를 기준으로 했을 때의 local reachability distance를 말하는데, 이는 $p$ 의 $k$ - distance neighborhood에 속하는 $o$ 들의 reachability distance들이다. 

![[Pasted image 20240117180300.png|center]]

검정색 점을 $p$, 파란색 점들을 $o$ 라고 할 때, 검정색 실선은 $p$ 에 대한 k-distance neighborhood의 반경을 나타내고, 각 파란색 점선은 각 $o$ 에 대한 k-distance neighborhood가 된다. 이 Case 1의 경우, $p$ 는 각 $o$ 의 k-distance neighborhood에 모두 속한다. 즉, $o$는 $p$ 의 관점에서 이웃이고, $p$ 도 $o$ 의 관점에서 이웃이되는 것이다. 

![[Pasted image 20240117180902.png|center]]

반면에 Case 2의 경우에는, $p$ 의 k-distance 반경은 넓지만 각 $o$ 의 k-distance 반경은 상대적으로 (밀집되어 있기 때문에) 좁다. 이럴 경우 $p$ 에서 $o$ 들까지의 reachability distance는 보라색 선들의 길이 합이 된다. 

즉, local reachability density 는 Case 2의 $p$ 에서 $o$ 까지의 reachability distance 들의 합을 분모로 하고, $p$ 의 k-distance neighborhood의 수를 분모로 하는 값이다.  

- Case 1의 경우, $p$ 가 밀도가 높은 지역에 위치하여 분모가 작기 때문에 큰 $lrd_k(p)$ 값을 가진다.
- Case 2의 경우, $p$ 가 밀도가 높은 clusters 사이에 sparse 한 지역에 위치하여 분모가 크기 때문에 작은 $lrd_k(p)$ 값을 갖게된다. 

여기서 중요한 점은 $lrd_k(p)$ 의 분자에 있는 이웃들의 개수 $|N_k(p)|$ 는 동률 (tie) 발생시 보정하는 역할을 한다는 것이다. 

#### Definition 5: local outlier factor of an object $p$

- 실제로 우리가 계산하는 값

$$LOF_k(p)=\frac{\sum_{o\in N_k(p)}\frac{lrd_k(o)}{lrd_k(p)}}{|N_k(p)|} = \frac{\frac{1}{lrd_k(p)}\sum_{o\in N_k(p)}lrd_k(o)}{|N_k(p)|}$$
- 분모 : $p$ 의 k-distance neighborhood의 개수 $|N_k(p)|$로 보정
- 분자 : $p$ 의 lrd와 $o$ 의 lrd 합으로 구성되어 있다. 

![[Pasted image 20240117182027.png|center]]

|  Case  | $lrd_k(p)$ | $lrd_k(o)$ | $LOF_k(p)$ |
|:------:|:----------:|:----------:|:----------:|
| CASE 1 |   Large    |   Large    |   Small    |
| CASE 2 |   Small    |   Large    |   Large    |
| CASE 3 |   Small    |   Small    |   Small    |

- LOF 의 목적 : 
	- 밀도가 높거나 낮은 영역의 가운데 있는 객체의 abnormal score는 작게 만들어주자
	- 반면에, 밀도가 높은 영역(들) 사이에 밀도가 낮은 객체에 대해서는 abnormal score를 크게 만들자

이러한 관점대로 계산한 LOF score의 예시가 아래 그림이다. 표시된 숫자는 해당 객체에 대한 abnormal scores이다. 주황색 점들 (2점대의 scores)은 밀도가 높은 영역/군집으로부터 그렇게 높지 않지만 abnormal scores 가 상대적으로 높다. 반면에 파란색 점은 애초에 밀도가 듬성듬성한 지역에 위치해 있기 때문에 주변 이웃들과 멀리 떨어져 있음에도 불구하고 상대적으로 낮은 abnormal score를 갖는다. 보라색과 같은 (몇 개 되지 않지만) collective outliers는 $k$ 값을 적절히 조정하게 되면 이 두 개 점들이 outlier 에 가깝게 abnormal scores 를 갖게 할 수 있다. 

![[Pasted image 20240117183602.png|center]]

이처럼 LOF는 개별적인 포인트들에 대해 local outlier scores를 모두 계산하는 non-parametric method 이다. 앞서 배운 Parzen window 에서는 각 포인트가 gaussian의 중심이라는 가정을 했는데, 여기서는 그러한 가정조차도 하지 않는 방법이다. 주어진 데이터로만 밀도를 추정하게 된다. 

- 단점 : 
	- 계산 복잡도가 굉장히 높다
	- Abnormal scores가 normalization 되어 있지 않음 : 
		- 서로 다른 데이터셋의 abnormal scores 간 비교를 할 수 없다는 의미
		- "데이터셋 내에서 $p$ 가 $q$ 보다 abnormal일 가능성이 더 높다" 라는 해석은 맞다

- LOF contour plot : 

![[Pasted image 20240117184334.png|center]]
