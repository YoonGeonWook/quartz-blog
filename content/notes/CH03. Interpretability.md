---
sticker: emoji//1f525
tags:
  - XAI
  - 해석가능성
  - Interpretability-Machine-Learning
  - ML
---
> [!done] Miller (2017):
> "해석 가능성(interpretability)란 인간이 어떠한 결정의 원인을 이해할 수 있는 정도이다."

> [!done] Another one:
> "해석 가능성은 인간이 모델의 결과를 일관되게 예측할 수 있는 정도이다."

어떤 모델의 해석 가능성이 높을수록 특정 결정이나 예측을 내린 이유를 더 쉽게 이해할 수 있는 것입니다. 모델의 결정이 다른 모델이 내리는 결정보다 더 쉽게 이해할 수 있다면 그 모델은 해석 가능성이 높은 것입니다. 특정 맥락에서는 개별 예측에 대해서 설명(explanability) 가능성이라고 지칭하기도 합니다. 

## **03-01. 해석 가능성의 중요성** 

> ML 모델의 성능이 좋지만 왜 우리는 그 모델을 신뢰하지 않고 모델의 결정을 무시할까요? 
> **"분류 정확도(accuracy)와 같은 단일 지표로는 대부분의 real-world에서의 task를 완전히 설명할 수 없기 때문입니다."**
> <cite>Doshi-Velez and Kim 2017</cite>


예측 모델링(predictive modeling)에 *있*어서 1. 예측된 결과만을 알고 싶은 경우와 2. 예측의 이유가 궁금하여 약간의 성능 저하를 지불하고 해석 가능성을 얻는 경우에 대해 trade-off가 있습니다. 많은 경우에서 '왜'를 알면 모델이 실패하는 이유에 대해 더 많이 알 수 있습니다. 일부 모델은 low-risk 환경에서 사용되어 실수가 심각한 결과를 초래하지 않거나(예: 영화 추천 시스템), 이미 연구가 많이 진행되어(예: 광학 문자인식) 설명(explanations)이 필요하지 않을 수 있습니다. 해석 가능성의 필요성은 특정 문제나 작업에 대해 예측(the what)을 얻는 것만으로는 충분하지 않다는 것에서 기인합니다. 정확한 예측은 문제를 부분적으로만 해결하기 때문에 모델이 예측에 어떻게 도달했는지(the why)도 설명해야 합니다. 해석/설명 가능성의 필요성은 아래의 이유들에서 비롯됩니다:

###### Human curiosity and learning:

Black-box와 같은 불투명한 ML 모델을 사용할 경우 모델이 설명(explanations) 없이 예측만 제공할 경우 과학적 발견이 숨겨집니다. 특정 예측이나 결과가 모델에 의해 생성된 이유에 대해서 호김심을 충족시키기 위해서는 해석 가능성과 설명이 중요합니다. 
- Find meaning in the world
학습(learning)과 관련이 있는 것은 의미를 찾고자 하는 인간의 욕구입니다. 우리는 기존에 알고 있는 지식 구조 사이의 모순이나 불일치를 조화시키고자 합니다. 그렇기 때문에 ML 모형의 결정이 인간의 삶에 많은 영향을 미칠수록 모델이 그 행동을 설명하는 것은 중요합니다. 
인간은 예상치 못한 사건에 대해서 원인/설명(explanations)을 찾음으로써 환경에 대한 자신의 정신(mental) 모델을 업데이트 합니다. ML 모델이 예측만 제공하고 설명을 제공하지 않으면, 모델에 의해 생성된 특정 예측이나 행동에 대한 호기심을 충족시키지 못하고 학습을 촉진하기 어렵습니다.

###### Safety measures and testing

예를 들어, 자율 주행 자동차가 딥러닝 시스템을 기반으로 자전거 타는 사람을 감지하는 경우, 시스템이 학습한 추상화(abstraction)에 오류가 없는지 100% 확신할 필요가 있습니다(위험한 일이기 때문). 설명은 시스템이 어떻게 결정을 내렸는지 이해하는 데 도움이 됩니다.

###### Detecting bias

ML 모형은 학습 데이터에서 편향(bias)을 포착할 수 있으며, 이는 모델이 소수 집단(minority)에 대해 차별적으로 행동하게 만들 수 있습니다. 해석 가능성은 ML 모형이 이러한 편향을 감지하는 데 유용한 디버깅 도구입니다.

###### Social acceptance & interactions

기계나 알고리즘이 내뱉는 예측을 설명할 수 있다면 더 많은 수용(acceptance)을 얻을 수 있습니다. 예를 들어 로봇 청소기가 '왜' 특정 행동을 하는지 설명하면 사용자는 그 행동을 더 잘 이해하고 받아들일 수 있습니다.

###### Debugging and audited

ML 모델은 해석 가능할 때만 디버깅할 수 있고 감사(audit)할 수 있습니다. 예를 들어 허스키와 늑대를 구분하는 classifier가 틀린 분류를 한 경우, 해석 가능한 ML 방법을 사용하면 오류의 원인을 이해하고 시스템을 수정하는 방향을 제시할 수 있습니다.


ML 모형이 해석 가능성을 갖추었다면 아래와 같은 특성들도 확인할 수 있습니다:
- 공정성(Fairness): 
	- 예측이 비편향(unbiased)인 것과 암묵적으로든 명시적으로든 소수 집단(minority)을 차별하지 않음을 보장할 수 있습니다.
	- 해석 가능한 모델은 특정 사람이 대출을 받아서는 안 된다고 결정한 이유를 알려줄 수 있으며, 해당 결정이 인구통계학적(예: 인종) 편견에 기반한 것인지 여부를 판단할 수 있습니다.
- 개인정보보호(Privacy): 
	- 민감한 정보가 보호되도록 보장합니다.
- 신뢰성/강건성(Reliability or Robustness): 
	- Input의 작은 변화가 예측의 큰 변화로 이어지지 않도록 보장합니다.
- 인과성(Causality)
- 신뢰도(Trust): 
	- 해석을 제공하지 못하는 black-box 모델보다는 해석 가능성을 겸비한 모델을 더 신뢰하기 쉽습니다.


## 03-02. 해석 가능성 방법론의 분류

ML Interpretability에 대한 방법론들은 다양한 기준들로 분류됩니다.

#### 1. 내재적(Intrinsic) 또는 사후(Post hoc) 해석

이 기준은 Interpretability가 ML 모델의 복잡성을 제한함으로써 달성되는지(intrisic), 아니면 학습 후 모델을 분석함으로써 달성되는지(post hoc)로 구분합니다. Intrinsic interpretability(Ch05 에서 다룸)는 짧은 DT or sparse linear models와 같이 구조가 단순하여 해석이 가능한 모델을 말합니다. Post hoc interpretability(CH06 에서 다룸)는 모델 학습 후 해석 방법론을 적용하는 것을 의미합니다. 예를 들어 Permutation feature importance 는 post hoc 방법에 해당됩니다.

###### Result of the interpretation method

다양한 해석 방법들이 있는데 이는 그것들이 내뱉는 결과에 따라 대략적으로 구분할 수 있습니다.

- **Feature summary statistic**: 
	- 각 feature 에 대한 요약 통계치를 제공합니다.
	- 몇몇 방법들은 Feature importance와 같이 단일 숫자 값을 반환하거나 feature interaction의 강도와 같은 결과를 반환합니다.
- Feature summary visualization:
	- 몇몇 요약들은 시각화에 의해서만 정보를 제공하기도 합니다. 
	- Parial dependence plot 이 바로 이 경우에 해당됩니다. 
- Model internals (e.g. learned weights): 
	- 내재적으로 해석 가능한 모델에서의 해석이 이 방식에 해당됩니다.
		- 예: 선형모델의 가중치/계수 or DT의 트리 구조(분할에 사용되는 임계값)
	- CNN에서 학습한 feature detectors 도 모델 내부를 출력하는 또 다른 방법입니다. 
- Data point: 
	- 모델을 해석 가능케 하기 위해 데이터 포인트(존재하거나 새로 생성된)를 반환하는 방법들이 이 종류에 해당합니다.
	- 이 방법은 특정 인스턴스의 예측을 설명하기 위해, 예측 결과를 변형시키는 일부 features 를 관련된 방식으로 바꾸어 유사한 데이터 포인트를 찾습니다(e.g. a filp in the predicted class). 
	- 또 다른 예는 예측된 클래스의 prototypes 를 식별하는 것입니다. 
		- 이 방법은 이미지와 텍스트에는 효과적이지만 수백 개의 features가 있는 tabular data 에는 유용하지 않습니다.
- Intrinsically interpretability model: 
	- Black-box 모델을 해석하는 방법은 interpretable model을 사용하여 globally or locally 근차시를 구하는 것입니다.
	- Interpretable model 자체는 내부 모델 parameters 나 feature summary statistics를 보고 해석합니다.

#### 2. 모델 명시적(model-specific) 또는 모델 무관한(model-agnostic) 해석

Model specific 해석 툴은 특정 모델 클래스로 한정됩니다. 선형 모델에서 회귀 가중치가 이에 해당되며, 내재적 해석 가능한 모델은 항상 model specific interpretation이 됩니다. 반면에 model agnostic tools는 모든 ML 모델에 사용이 가능하며 모델이 학습된 후(post hoc) 적용됩니다. 이러한 model agnostic 방법은 보통 feature의 input과 output 쌍을 분석하는 방식으로 작동합니다.

#### 3. Local or Global interpretation

Interpretation method 가 개별 예측을 설명하는지, 전체 모델 행동을 설명하는지 아니면 중간 정도의 범위를 설명하는지에 따라 구분됩니다.


## 03-03. Scope of Interpretability

#### 1. Algorithm Transparency

###### *알고리즘은 어떻게 모델을 생성하나요?*

알고리즘이 데이터에서 모델을 학습하는 방법과 어떤 종류의 관계를 학습하는지에 관한 것이 투명성입니다. 이는 알고리즘에 대한 지식만 있으면 되고 데이터나 학습된 모델에 대한 지식은 필요로 하지 않습니다. 

#### 2. 전역적(Global) & 전체적(Holistic) 모델 해석 가능성

###### *학습된 모델은 어떻게 예측을 하나요?*

전체 모델을 한 번에 이해할 수 있다면 그 모델은 해석 가능하다고 말할 수 있습니다. Global model outcome을 설명하기 위해서는 학습된 모델, 알고리즘과 데이터에 대한 지식이 필요합니다. 이러한 interpretability 는 학습된 각 구성 요소(features, 가중치, parameters, 구조 등)에 대한 전체적인 관점을 바탕으로 모델이 어떻게 의사결정을 내리는지 이해하는 것입니다. 또한 Global model interpretability 는 features 를 통해 target의 분포를 이해하는 데 도움을 줍니다. 

#### 3. 모듈 수준에서의 전역 모델 해석 가능성

###### *모델의 각 부분이 예측에 어떤 영향을 미치나요?*

수백 개의 features를 가진 Naive Bayes model 은 한 번에 이해하기 어렵습니다. 하지만 하나의 가중치는 이해할 수 있습니다. 
이와 같이 전체 모델을 해석하는 것은 불가능하지만, 적어도 일부 모델은 모듈 수준에서 이해할 수 있습니다. 

#### 4. 단일 예측에 대한 Local 해석 가능성

###### *모델이 인스턴스에 대해 특정 예측을 한 이유는 무엇인가요?*

개별 예측을 살펴보면 복잡한 모델의 동작이 더 적절하게 작동하는 것을 알 수 있습니다. 국지적으로 보면, 예측은 복잡한 의존성보다는 일부 features에만 선형적 혹은 단조롭게 의존하기도 합니다. 따라서 local explanations가 global explanations보다 정확할 수 있습니다. 이 책에서는 model agnostic 방법에 대한 섹션에서 개별 예측의 해석 가능성을 높일 수 있는 방법을 소개합니다.

#### 5. 예측 그룹에 대한 Local 해석 가능성

###### *모델이 인스턴스 그룹에 대해 특정 예측을 한 이유는 무엇인가요?*

여러 인스턴스에 대한 모델 예측은 global interpretation methods (on a modular level) 또는 explanations of individual instances 로 설명할 수 있습니다. Global method는 인스턴스 그룹을 전체 데이터 집합으로 취급하고 이 하위 집합에 전역 방법을 적용합니다. Individual explanation method는 각 인스턴스에 적용한 다음 전체 그룹에 대해 나열하거나 집계하여 적용합니다.


## 03-04. Evaluation of Interpretability

Doshi-Velez와 Kim(2017)은 해석 가능성 평가를 위한 세 가지 주요 수준을 제안합니다:

#### 1. Application Level Evaluation: 

설명을 제품과 통합시켜서 최종 사용자(전문가)가 테스트하는 것을 말합니다.

#### 2. Human Level Evaluation:

Application level evaluation을 간소화한 것입니다. 차이는 테스트를 도메인 전문가가 아닌 일반인이 수행한다는 것입니다. 이는 테스트 비용을 더 저렴하게 하고 더 많은 테스터를 찾을 수 있습니다. 

#### 3. Function Level Evaluation:

이는 인간이 필요하지 않은 방법입니다. 이미 다른 사람이 Human level evaluation 에서 평가된 모델을 사용할 때 효과적입니다. 
예를 들어 최종 사용자가 DT를 이해할 수 있다고 할 때, 설명의 품질의 프록시는 tree의 깊이가 될 수 있습니다. 더 짧은 트리는 더 나은 해석 가능성 점수를 받을 것입니다.

다음 챕터에서는 Function level에서 개별 예측에 대한 설명을 평가하는 것에 중점을 둡니다. 

## 03-05. Properties of explanations

ML 모델의 예측을 설명하기 위해선, explainer를 생성하는 알고리즘의 방법에 의존합니다. Explainer는 보통 사람이 이해할 수 있는 방식으로 인스턴스의 feature 값을 모델 예측과 연관시킵니다. 

Explanation methods과 explantions의 속성에 대해 살펴보겠습니다. 아래 나타나는 속성들은 explanation method 또는 explanations 가 얼마나 좋은지 판단하는 데 쓰입니다. 

#### 1. Properties of Explanation Methods

- **Expressive Power(표현력)**: 
	- Methods가 생성할 수 있는 설명의 '언어' 또는 구조입니다.
	- 예: IF-THEN 규칙, 결정 트리, 가중치 합계 등
- **Translucency(투명성)**: 
	- 설명 방법이 parameter와 같은 모델 내부를 얼마나 들여다 보는지를 설명합니다. 예를 들어 선형 회귀 모델과 같은 내재적 해석 가능성 모델은 translucency가 매우 높습니다. 
		- 높은 translucency 는 더 많은 정보에 의존해 설명을 생성할 수 있다는 장점이 있습니다.
		- 낮은 translucency 는 설명 방법의 이식성이 높다는 장점이 있습니다.
- **Portability(이식성)**:
	- 설명 방법이 쓰일 수 있는 ML 모델의 범위를 말합니다.
	- 낮은 투명성을 가진 method는 ML 모델을 black box로 취급하기 때문에 이식성이 더 높습니다. 
	- Surrogate model은 이식성이 가장 높은 설명 방법일 것입니다. 
	- 반대로 특정 모델에서만 작동하는 설명 방법은 이식성이 낮은 것입니다.
- **Algorithmic Complexity**: 
	- Explainer를 생성하는 방법의 계산 복잡도를 말합니다. 

#### 2. Properties of Individual explanations

- **Accuracy(정확도)**: 
	- Explainer 가 unseen data 를 얼마나 잘 예측하는지를 나타냅니다.
- **Fidelity(충실도)**:
	- Explainer가 black box 모델의 예측에 얼마나 잘 근사하는지를 나타냅니다. 
	- Black box 모델의 정확도가 높고 explainer의 충실도가 높으면 explainer의 정확도도 높습니다.
	- 일부 explainer는 local fidelity 만 제공하는데, 이는 explainer 데이터의 하위 집합(예: local surrogate model, LIME) 또는 개별 인스턴스(예: Shapley values)에 대해서만 모델 예측에 잘 근접하는 것을 의미합니다. 
- **Consistency(일관성)**: 
	- 같은 작업에 대해 훈련된 서로 다른 모델 간에 설명이 얼마나 다른지를 나타냅니다.
- **Stability(안정성)**: 
	- 유사한 인스턴스에 대한 설명이 얼마나 유사한지를 나타냅니다.
	- 일관성은 모델 간 설명을 비교하는 반면, 안정성은 고정된 모델에 대해 유사한 인스턴스 간 설명을 비교합니다. 
	- 안정성이 부족하면 설명 방법의 분산이 높을 수 있습니다. 즉, 설명할 인스턴스의 feature 값이 약간만 변해도 설명 방법이 크게 영향을 받는다는 뜻입니다. 
- **Comprehensibility(이해 가능성)**:
	- 인간이 주어진 설명을 얼마나 잘 이해하는지를 나타냅니다. 
- **Certainty(확실성)**:
	- 설명이 ML 모델의 확실성을 얼마나 잘 반영하는지를 나타냅니다. 
	- 많은 ML 모델은 예측이 정확하다는 모델의 확신에 대한 설명 없이 예측만 제공합니다.
		- 모델이 한 환자에게 암 발생 확률이 4%라고 예측했다면 다른 feature를 갖는 환자에게도 4%의 확률을 예측하는 것만큼 확실할지는 이 확실성에 따라 다를 수 있습니다.
- **Novelty**: 
	- 설명하고자 하는 인스턴스가 학습 데이터 분포에서 멀리 떨어져 있는 '새로운' 영역의 것인 경우 이를 잘 반영하는지를 나타냅니다. 
	- 이러한 경우에는 모델이 부정확할 수 있으며 따라서 설명이 쓸모 없을 것입니다. 
	- Novelty가 높을수록 데이터의 부족으로 인해 모델의 확실성이 낮을 가능성이 높습니다.
- **Representativeness(대표성)**:
	- 설명이 얼마나 많은 인스턴스를 커버하는지를 나타냅니다.

## 03-06. Human-friendly Explanations

이번 섹션에서는 인간이 '좋은' 설명으로 여기는 것이 무엇인지, 그리고 해석 가능한 ML에 대한 함의를 탐구합니다. 

#### 1. What is an Explanation?

설명(explanation)은 왜라는 질문에 대한 답입니다:
- 치료가 환자에게 효과가 없었던 이유는 무엇인가?
- 대출이 거부된 이유는 무엇인가?
- 왜 아직 외계 생명체로부터 연락을 받지 못했는가?

처음 두 질문은 '일상적인' 설명으로 답할 수 있고, 세 번째 질문은 '과학적 현상과 철학적 질문'에 속하는 질문입니다. '일상적인' 유형의 설명에 초점을 맞춘 이유는 이러한 설명이 해석 가능한 ML과 관련이 있기 때문입니다. 

#### 2. What is a Good Explanation?

###### Contrastive Explantions(대조적인 설명):

- 사람들은 특정 예측이 왜 이루어졌는지보다는 다른 예측 대신 왜 이 예측이 이루어졌는지에 대해 묻는 경향이 있습니다. 
- "입력 $X$가 달랐다면 예측이 어떻게 되었을까?"
- 주택 가격 예측의 경우 주택 소유자는 예상했던 낮은 가격에 비해 예측된 가격이 높은 이유가 궁금할 수 있습니다. 
- 대출 신청이 거절된 경우, 그렇게된 모든 요인을 듣기 보다는 대출을 받기 위해 변경해야 하는 요소에 관심이 있습니다.
- 가장 좋은 설명은 관심 대상과 기준 대상 간의 가장 큰 차이를 강조하는 설명입니다.
- 
###### Explanations are selected

- 사람들은 사건의 원인에 대해 실제적이고 완전한 목록을 모두 포함하는 설명을 기대하지 않습니다. 
- 다양한 원인 중 한두 가지 원인을 '설명'으로 선택하는 데 익숙합니다. 
- Rashomon Effect: 하나의 사건을 다양한 원인으로 설명할 수 있는 것
- 설명은 짧게 하고, 1~3개의 이유만을 제시하세요. LIME은 이를 잘 수행합니다.

###### Explanations are social

- 설명은 설명하는 사람(explainer)과 설명을 받는 사람 간의 대화 또는 상호작용입니다. 
- 즉 사회적 맥락에 따라서 설명의 내용과 성격이 달라집니다.
	- 기술 담당자에게 디지털 암호화폐가 가치있는 이유를 설명한다면, "중앙 기관이 통제할 수 없는 탈중앙화된 분산형 블록체인 기반 장부는 부를 안전히 지키려는 사람들의 공감을 불러일으키고, 이것이 높은 수요와 가격을 설명합니다."처럼 얘기할 것입니다.
	- 반면 할머니께는, "암호화폐는 컴퓨터 금과 비슷해요. 사람들은 금을 좋아해서 많은 돈을 지불하며, 젊은이들은 컴퓨터 금을 좋아하여 많은 돈을 지불한답니다."라고 말할 것입니다.
- 애플리케이션의 사회적 환경과 target 고객에 주의를 기울여야 합니다. 

###### Explanations focus on the abnormal

- 사람들은 사건을 설명할 때 비정상적인 것에 더 집중합니다. 
- 이러한 비정상적 원인을 제거했다면 결과는 크게 달려졌을 것입니다(counterfactual explanation). 
- Input features 중 하나가 어찌됐든 abnormal이고(예: 범주형 feature의 드문 수준) 그 feature가 예측에 영향을 미쳤다면, 다른 'normal' features가 abnormal features와 예측에 동일한 영향을 미치더라도 설명에 포함되어야 합니다. 

###### Explanations are truthful

- 좋은 설명은 현실(즉, 다른 상황)에서도 사실로 증명됩니다. 하지만 이것이 '좋은' 설명의 가장 주요한 요소는 아닙니다.
	- 오히려 선택성이 더 중요할 수 있습니다.
- 설명은 가능한 한 진실되게 사건을 예측해야 하며, 이를 충실도(fidelity)라고 합니다. 

###### Good Explanations are consistent with prior beliefs of the explainee

- 사람은 자신이 가지고 있는 신념에 맞지 않는 정보는 무시하는 경향이 있습니다.
	- 이러한 효과를 confirmation bias(확증 편향)이라고 합니다.

###### Good explanations are general and probable

- 많은 이벤트를 설명할 수 있는 원인은 매우 일반적이면서 좋은 설명일 수 있습니다. 
	- 이는 abnormal cause가 좋은 설명을 만든다는 주장과 모순된다는 것을 유의하세요.
	- 주관적으로는 abnormal cause가 일반적인 원인을 능가합니다.
	- abnormal cause가 발생하지 않는 다면 일반적인 원인이 좋은 설명이라고 간주됩니다.
- 일반성은 해당 설명이 적용되는 인스턴스의 수를 전체 인스턴스의 수로 나눈 값으로 측정 가능합니다.
