---
sticker: emoji//1f525
---
Global methods는 ML 모델에서 평균적인 동작을 설명합니다. 이에 대응하는 것이 Local methods입니다. Global methods는 데이터 분포에 따른 기대값으로 보통 표현됩니다. 예를 들어 feature effect plot 인 PDP는 다른 모든 피쳐들을 제외했을 때 기대되는 예측치입니다. Global methods는 평균적인 동작을 설명하기 때문에 모델 작성자가 데이터의 일반적인 메커니즘을 이해하거나 모델을 디버깅할 때 유용합니다. 

이 책에서는 아래와 같은 model-agnostic global 해석 기법들을 다룹니다 : 
- **Partial dependence plot; pdp**는 feature effect method 입니다
- **Accumulated local effect plots** 는 피쳐가 종속적일 때 작동하는 또 다른 feature effect method 입니다
- **Feature interaction (H-statistic)** 은 예측치가 피쳐들의 joint effect 로 인한 결과임을 정량화합니다. 
- **Functional decomposition**은 interpretability의 핵심 개념으로, 복잡한 예측 함수를 더 작은 부분으로 분해하는 기법입니다. 
- **Permutation feature importance**는 피쳐가 뒤섞일 때 손실(loss)의 증가로 변수 중요도를 측정합니다. 
- **Global surrogate models**는 해석을 위해 기존 모델을 더 간단한 모델로 대체합니다. 
- **Prototypes and criticisms**는 어떠한 분포의 대표성있는 데이터 포인트이며 해석 가능성을 향상시킬 수 있습니다. 

# 📕 08-01. Partial Dependence Plot (PDP)
&#9495; 부분 의존도 플롯
---
PDP 또는 PD plot은 하나 또는 두 개의 피쳐가 ML 모델 예측치에 미치는 주변 효과(marginal effect)를 보여줍니다. PDP는 target과 features 간 관계가 선형인지, 단조로운지 또는 복잡한지를 보여줄 수 있습니다. 선형 회귀 모델에서 PDP는 항상 선형 관계를 보여줍니다. 

Regression에 대한 PD function은 다음과 같습니다 : 
$$
\hat{f}_S\left(x_S\right)=E_{X_C}\left[\hat{f}\left(x_S, X_C\right)\right]=\int \hat{f}\left(x_S, X_C\right) d \mathbb{P}\left(X_C\right)
$$
여기서 $x_S$는 PDP를 그려야 하는 피쳐이고, $X_C$는 ML 모델 $\hat{f}$에서 사용되는 다른 피쳐들이며, 확률변수로 취급됩니다. 보통 집합 $S$ 에는 하나 또는 두 개의 피쳐가 존재합니다. $S$ 에 있는 피쳐들은 예측치에 미치는 영향을 알고자 하는 피쳐들입니다. 피쳐 벡터 $x_S$와 $x_C$를 합하면 전체 피쳐 공간 $x$ 가 됩니다. PD는 집합 $C$ 의 피쳐 분포에 대해 ML model output을 주변화(marginalize)하여 관심 있는 집합 $S$ 의 피쳐와 예측된 결과 사이의 관계를 표시하도록 작동합니다. 다른 피쳐들을 이용해 주변화하면 다른 피쳐들과의 상호작용을 포함하여 $S$ 의 피쳐에만 의존하는 함수를 얻을 수 있습니다. 

Partial function $\hat{f}_S$는 학습 데이터의 평균을 몬테카를로 방법으로 추정합니다 : 
$$
\hat{f}_S\left(x_S\right)=\frac{1}{n} \sum_{i=1}^n \hat{f}\left(x_S, x_C^{(i)}\right)
$$
이 partial function은 주어진 피쳐 집합 $S$ 의 값에 대해 예측치의 평균 주변 효과(average marginal effect)를 알려줍니다. 위 공식에서 $x_C^{(i)}$는 우리가 관심 두지 않는 피쳐에 대한 데이터셋의 실제 피쳐값이고, $n$는 인스턴스 수입니다. PDP는 $C$ 의 피쳐가 $S$ 의 피쳐와 상관관계가 없다는 가정을 합니다. 이 가정을 위반하면 PDP로 계산된 평균이 무의미 해집니다. 

클래스 확률을 출력하는 분류 문제의 경우, PDP는 $S$ 의 피쳐에 대해 서로 다른 값이 주어졌을 때 특정 클래스에 대한 확률을 표시합니다. 여러 클래스를 처리하는 쉬운 방법은 클래스당 하나의 선을 그리는 것입니다. 

PDP는 global method 입니다 : 모든 인스턴스를 고려하고 피쳐와 예측치의 global relationship에 대한 설명을 제공하기 때문입니다. 

#### Categorical features

지금까지는 숫자형 피쳐만을 고려했습니다. 범주형 피쳐도 PDP를 계산하기 쉽습니다. 각 범주/수준에 대해 모든 인스턴스가 동일한 수준을 갖도록 강제하여 PDP 추정치를 얻습니다. 예를 들어, 자전거 대여 데이터셋을 보고 계절(`season`)에 대한 PDP가 궁금하다면 각 계절에 대해 하나씩 4개의 값을 얻을 수 있습니다. "여름"에 대한 값을 계산하려면 모든 인스턴스의 계절을 "여름"으로 바꾸고 예측치의 평균을 구하면 됩니다. 

## 1. PDP-based Feature importance

Greenwell et al (2018)은 간단한 PD-based feature importance 측정법을 제안했습니다. 아이디어는 PDP가 평평하면 피쳐가 중요하지 않음을 의미하고, PDP가 다채로울수록 중요함을 의미한다는 것입니다. 숫자형 피쳐의 경우 중요도는 각 피쳐의 unique value의 평균 곡선으로부터의 (표준) 편차로 정의됩니다 : 
$$
I\left(x_S\right)=\sqrt{\frac{1}{K-1} \sum_{k=1}^K\left(\hat{f}_S\left(x_S^{(k)}\right)-\frac{1}{K} \sum_{k=1}^K \hat{f}_S\left(x_S^{(k)}\right)\right)^2}
$$
여기서 $x_S^{(k)}$는 피쳐 $X_S$의 $K$개 unique values 입니다. 범주형 피쳐는 아래와 같습니다 : 
$$
I\left(x_S\right)=\left(\max _k\left(\hat{f}_S\left(x_S^{(k)}\right)\right)-\min _k\left(\hat{f}_S\left(x_S^{(k)}\right)\right)\right) / 4
$$
이것은 유니크한 범주들에 대해서 PDP 값의 범위를 4로 나눈 것입니다. 이 편차 계산 방법을 range rule 이라고 합니다. 범위(range)만 알고 있을 때 편차의 대략적인 추정치를 얻는 데 도움이 됩니다. 분모 4는 표준 정규분포에서 95%의 데이터가 +/- 2의 표준편차에서 나오는 것에서 따온 것입니다. 따라서 이 범위를 4로 나눈 것은 실제 편차를 과소 평가하는 rough 한 추정치 얻게 하기 위함입니다. 

이러한 PDP 기반 변수 중요도는 신중하게 해석해야 합니다. 이는 피쳐의 주효과(main effect)만 포착하고 피쳐간 상호작용 효과는 무시합니다. Permutataion feature importance 와 같은 다른 방법에서는 어떤 피쳐가 매우 중요할 수 있지만 PDP 에서는 그 피쳐가 다른 피쳐들과의 상호작용으로 인해 예측치에 영향을 주기 때문에 평평할 수 있습니다. PDP 기반 변수 중요도의 또 다른 단점은 변수의 unique values 에 의해 정의된다는 점입니다. 이는 unique values 의 분포는 무시되기 때문입니다. 

## 2. Example

한 개의 피쳐는 2D plot을 만들고, 2개면 3D plot을 만들기 때문에 실제로 피쳐 집합 $S$ 에는 보통 하나 또는 최대 두 개의 피쳐만 포함됩니다. 그 이상은 매우 까다롭습니다. 

주어진 날짜에 자전거 대여 수를 예측하는 회귀 문제를 다시 보겠습니다. 먼저 ML 모델을 fitting한 다음, PD를 분석합니다. 이 예제에서는 대여 수를 예측하기 위해 random forest 를 적합하고 PDP를 사용하여 모델이 학습한 관계를 시각화 했습니다. 날씨 피쳐가 예측 대여 수에 미치는 영향을 아래 그림에 시각화 했습니다 : 

![](Pasted%20image%2020240101214109.png) Figure 8.1 : 자전거 대여 수 예측 모델에서 `temp`, `hum`, `windspeed`에 대한 PDP. 온도에서 가장 큰 차이를 볼 수 있습니다. 기온이 높을수록 대여 수가 많아집니다. 이 추세는 섭씨 20도까지 이어지다가 30도부터는 평평해지고 약간 감소합니다. X축의 표시는 데이터의 분포를 나타냅니다. 

따뜻하지만 너무 덥지 않은 날씨의 경우 이 모델은 평균적으로 자전거 대여 건수가 많을 것으로 예측됩니다. 습도(`hum`)가 60%를 초과하면 이용자들은 자전거 대여를 꺼리게 됩니다. 또한 바람이 많이 불수록 자전거를 타는 사람이 줄어드는 것도 당연한 결과입니다. 흥미롭게도 풍속이 25km/h 에서 35km/h 로 증가해도 예측된 자전거 대여 건수는 감소하지 않습니다. 하지만 이는 학습 데이터가 많지 않아서 ML 모델이 이 범위에 대한 의미 있는 학습을 하지 못했을 가능성이 있습니다. 적어도 직관으로는 풍속이 매우 높으면 대여 수가 감소할 것으로 예상됩니다. 

범주형 피쳐에 대한 PDP를 설명하기 위해 `season` 피쳐가 예측 대여 수에 미치는 영향을 살펴보겠습니다 : 

![](Pasted%20image%2020240101214250.png)Figure 8.2 : 대여 수 예측 모델과 `season`에 대한 PDP. 예상외로 모든 계절에서 예측치가 비슷한 영향을 보이고 있고, 겨울에만 대여 건수가 약간 적게 예측됩니다. 

자궁경부암 분류 문제에 대해서도 PDP를 계산해 보겠습니다. 이번에는 Random Forest를 적용해 위험 요인에 따라 여성의 자궁 경부암 발병 여부를 예측하겠습니다. Random Forest에 여러 피쳐에 대한 암 확률의 PD를 계산하고 시각화했습니다 : 

![](Pasted%20image%2020240101215649.png) Figure 8.3 : 나이와 호르몬 피임약 복용 기간에 따른 암 발생 확률의 PDP. 나이의 PDP는 40세까지는 확률이 낮고 그 이후에는 증가함을 보여줍니다. 호르몬 피임약을 복용한 기간이 길수록, 특히 10년이 넘어가면 암 발생 위험이 높아집니다. 두 피쳐 모두 큰 값을 갖는 데이터 포인트가 많지 않아서 높은 값에 대한 PD 추정치의 신뢰도가 낮습니다.

또한 두 개의 피쳐에 대한 PDP를 한 번에 시각화 할 수도 있습니다 :

![](Pasted%20image%2020240101220250.png) Figure 8.4 : 암 발생 확률의 PDP와 연령 및 임신 횟수의 상호작용입니다. 45세에서 암 확률이 증가함을 볼 수 있습니다. 25세 미만인 경우, 임신 횟수가 1~2회인 여성은 0회 또는 2회 이상인 여성의 비해 암 발생 위험이 낮습니다. 하지만 결론을 내릴 때 주의해야 합니다 : 이는 인과관계가 아닌 상관관계 입니다!

## 3. Advantages

PDP 의 계산은 직관적입니다 : 특정한 피쳐에 대한 PD 함수는 모든 데이터 포인트를 해당 피쳐값으로 가정할 경우의 평균 예측치를 나타냅니다. 

PDP를 계산한 피쳐가 다른 피쳐와 상관관계가 없는 경우, PDP는 피쳐가 평균적으로 예측치에 어떤 영향을 미치는지 완벽하게 나타냅니다. 상관관계가 없는 경우에는 해석이 명확합니다. PDP는 $j$ 번째 피쳐가 변경될 때 데이터셋의 평균 예측치가 어떻게 변하는지를 보여줍니다. 하지만 상관관계가 있는 경우 단점이 있습니다. 

구현이 쉽습니다. 

PDP에 대한 계산은 인과 관계로 해석됩니다. 피쳐에 개입하여 예측치의 변화를 측정하기 때문입니다. 이 과정에서 피쳐와 예측치 간의 인과관계를 분석합니다. 이 관계는 outcome 을 피쳐의 함수로 명확히 모델링하기 때문에 모델에서는 인과관계이지만 실제 세계에서는 인과관계가 아닐 수 있습니다!

## 4. Disadvantages

PD 함수의 현실적인 최대 피쳐 수는 2개입니다. 이는 PDP의 결함이 아니라 3차원 이상을 상상하지 못하는 우리의 무능 때문입니다. 

일부 PDP에는 피쳐의 분포가 표시되지 않습니다. 분포를 생략하면 데이터가 거의 없는 영역을 과도하게 해석하게 되기 때문에 오해의 소지가 있습니다. 이 문제는 `rug` (X 축의 데이터 포인트에 대한 bool 옵션) 또는 히스토그램을 표시하면 쉽게 해결 가능합니다. 

피쳐 간의 독립성 가정은 PDP의 가장 큰 문제점입니다. 예를 들어 몸무게와 키가 주어졌을 때 얼마나 빨리 걷는지 예측하도 싶다고 해보겠습니다. 키와 같은 피쳐 중 하나의 PD를 계산할 때 다른 피쳐(체중)는 키와 상관관계가 없다고 가정해야하는데, 이는 명백히 잘못된 가정입니다. 특정 키(예: 200cm)에서 PDP를 계산하기 위해선 체중의 주변 분포를 평균화하는데, 여기에는 2m인 사람에게는 비현실적인 50kg 미만의 체중이 포함될 수 있습니다. 즉 피쳐간 상관관계가 있는 경우, 실제 확률이 매우 낮은 피쳐의 분포에 새로운 데이터 포인트를 생성하게 됩니다. 이 문제의 해결책은 주변 분포 대신 조건부 분포로 작동하는 **Accumulated Local Effect plots** 또는 **short ALE plots** 입니다. 

PDP는 평균 주변 효과만 나타내므로 이분산적인/이질적인 효과가 숨어있을 수 있습니다. 어떤 피쳐에 대해 데이터 포인트의 절반이 예측치와 양의 상관관계를 가지고 있고, 나머지 절반은 음의 상관관계를 갖는다고 해보겠습니다. 데이터셋의 양쪽 절반의 효과가 서로 상쇄될 수 있으므로 PD curve는 수평선이 될 수 있습니다. 그런 다음 피쳐가 예측치에 영향을 미치지 않는다고 결론을 내버릴 수 있습니다. 이렇게 집게된 line graph 대신에 **개별 조건부 기대 곡선(individual conditional expection; ICE curves)**를 사용하면 이러한 이질적인 효과를 찾을 수 있습니다. 

## 5. Software and Alternatives

PDP를 구현할 수 있는 R 패키지에는 여러 가지가 있습니다. 예제에서는 `iml` 패키지를 썼지만, `pdp`나 `DALEX`도 있습니다. Python에서는 PDP가 scikit-learn에 내장되어 있으며 `PDPBox`를 사용할 수 있습니다. 

이 책에서 소개하는 PDP의 대안으로는 ALE plots와 ICE curves 입니다. 

---

# 📕 08-02. Accumulated Local Effects; ALE plot

Accumulated Local Effects; ALE는 피쳐가 ML 모델 예측에 평균적으로 어떤 영향을 미치는지 설명합니다. ALE plot은 PDP에 비해 더 빠르고 비편향인 대안법입니다. 

PDP가 더 이해하기에 쉽고 ALE, PDP 모두 피쳐가 예측치에 평균적으로 어떤 영향을 미치는지를 설명한다는 동일한 목표를 갖습니다. 이제부터는 피쳐간 상호 연관되어 있을 때 PDP는 좋지 않음을 설명하겠습니다. 

## 1. Motivation and Intuition

피쳐들 간에 상관관계가 있는 경우, PDP는 신뢰를 잃습니다. 다른 피쳐와 강한 상관관계가 있는 피쳐에 대한 PDP를 계산하기 위해 인위적인 데이터를 만들어 인스턴스 예측치의 평균을 계산했습니다. 이렇게 하면 추정된 feature effect가 크게 편향될 수 있습니다. 방의 수와 거실 면적에 따라 주택 가치를 예측하는 ML 모델의 PDP를 계산해 보겠습니다. 여기서 우리는 거실 면적이 예측치에 미치는 영향에 관심이 있다고 하겠습니다. 
- [ㅌ] PDP를 만드는 방법 : 
	1) 피쳐를 선택, 
	2) grid 정의, 
	3) grid 값별로 
		a) 피처를 grid 값으로 교체하고, 
		b) 평균 예측치를 계산합니다. 
	4) curve를 그립니다. 

PDP의 첫 번째 grid 값(예: 30 ㎡)을 계산하기 위해 방이 10개인 주택의 경우에도 모든 인스턴스의 거실 면적을 30㎡로 대체합니다. PDP는 이러한 비현실적인 주택을 feature effect 추정에 포함시키고 모든 것이 정상인 것처럼 가정합니다. 아래 그림은 2개의 상호 연관된 피쳐와 PDP가 어떻게 드문 사례의 예측을 평균화하는지 보여줍니다 : 
![](Pasted%20image%2020240101232730.png) Figure 8.5 : 강한 상관관계가 있는 x1과 x2. `x1=0.75`에서 feature effect를 계산하기 위해 PDP는 모든 인스턴스의 x1을 0.75로 대체하여 `x1=0.75`에서 x2의 분포가 x2의 주변 분포(세로선)와 같다고 잘못 가정합니다. 이렇게 하면 x1과 x2의 가능성이 낮은 조합(예: x1=0.75, x2=0.2)이 발생하며, PDP는 평균 효과를 계산하는 데 이를 이용합니다. 

피쳐들의 상관관계를 고려한 feature effect 추정치를 얻으려면 어떻게 해야 할까요? 피쳐의 조건부 분포에 대한 평균을 구하면 됩니다. 즉, grid 값이 x1일 때 x1 값을 가진 인스턴스의 예측치를 평균화합니다. 조건부 분포를 사용하면 feature effect를 계산하는 솔루션을 marginal plot 또는 M-plot이라고 합니다(실제로는 주변분포가 아닌 조건부 분포를 사용하는데 헷갈리는 이름을 사용하고 있습니다). 하지만 M-plot은 저희가 찾고 있는 해결책이 아닙니다. M-plot이 문제를 해결하지 못하는 이유는 무엇일까요? 약 30㎡의 모든 주택에 대한 예측치를 평균내면 상관관계가 있기 때문에 거실 면적과 방의 수의 **combined effect** 만 추정할 수 있습니다. 거실 면적에 따라 방의 수가 증가하기 때문에 M-plot은 여전히 거실 면적이 예측치를 증가시킨다는 것을 보여줍니다. 아래 그림은 상호 연관된 2가지 피쳐에 대해 M-plot이 어떻게 작동하는지 보여줍니다 : 
![](Pasted%20image%2020240101234122.png) Figure 8.6 : 강한 상관관계를 갖는 x1과 x2. M-plot은 조건부 분포에 대한 평균입니다. 여기서는 `x1=0.75`에서 x2의 조건부 분포입니다. Local 예측치를 평균내면 두 가지 피쳐의 effect가 섞입니다. 

M-plot은 가능성이 낮은 인스턴스들의 예측치 평균화는 피하지만, 피쳐의 효과를 상관있는 모든 피쳐 효과와 섞어버립니다. ALE plot은 피쳐의 조건부 분포를 기반으로 평균 대신 예측치의 차이를 계산해 이 문제를 해결합니다. 30㎡ 거실 면적 효과는 ALE에서는 약 30㎡의 모든 주택을 사용하고, 이 주택이 31㎡인 것으로 가정한 모델 예측에서 29㎡인 것으로 가정한 예측치를 뺀 값을 구합니다. 이렇게 하면 거실 면적의 순수한 효과를 얻을 수 있으며, 이 효과를 상관관계가 있는 피쳐들의 효과와 섞이지 않게 합니다. 차이값을 사용하면 다른 피쳐의 효과가 차단됩니다. 아래 그림은 ALE plot이 계산되는 방식을 직관적으로 보여줍니다 : 
![](Pasted%20image%2020240102000247.png) Figure 8.7 : x2와 상관관계가 있는 x1에 대한 ALE 계산. 먼저 피쳐 x1을 간격(세로선)으로 나눕니다. 간격의 인스턴스(points)에 대해 피쳐를 간격의 상한과 하한(가로선)으로 대체할 때 예측치의 차이를 계산합니다. 이러한 차이값이 나중에 누적되고 중심화되면 ALE curve가 됩니다. 

각 유형의 플롯 (PDP, M, ALE)이 특정 grid 값 v에서 피쳐의 효과를 계산하는 방법을 요약하면 아래와 같습니다 : 
- **Partial Denpendence Plots** : 각 데이터 인스턴스에 해당 피쳐 값 v가 있을 때 모델이 평균적으로 무엇을 예측하는지 보여드립니다. 값 v가 모든 인스턴스에 적절한지는 무시합니다. 
- **M-plots** : 해당 피쳐에 대해 v에 가까운 값을 갖는 인스턴스에 대해 모델이 평균적으로 예측하는 것을 보여줍니다. 이 효과는 해당 피쳐의 영향일 수도 있지만 상관관계에 있는 피쳐들 때문일 수도 있습니다. 
- **ALE Plots** : v 값 주변의 작은 "창(window)"에서 해당 창에 있는 인스턴스들에 대해 모델 예측치가 어떻게 변하는지 보여줍니다. 

## 2. Theory

PDP, M-plot, ALE plot은 수학적으로 어떻게 다른지 알아보겠습니다. 3가지 방법의 공통점은 복잡한 예측 함수 $f$ 를 하나 또는 두 개의 피쳐에만 의존하는 함수로 줄인다는 것입니다. 세 가지 방법 모두 다른 피쳐들의 효과를 평균내어 함수를 축소하지만, 예측치의 평균 또는 예측치의 차이, 평균화를 주변분포에 대해 수행할지, 조건부 분포에 대해 수행할지는 다릅니다. 

PDP의 주변 분포에 대한 예측치의 평균을 구합니다 : 
$$
\begin{aligned}
\hat{f}_{S, P D P}(x) & =E_{X_C}\left[\hat{f}\left(x_S, X_C\right)\right] \\
& =\int_{X_C} \hat{f}\left(x_S, X_C\right) d \mathbb{P}\left(X_C\right)
\end{aligned}
$$
이는 피쳐값 $x_S$에서 예측 함수 $f$ 의 값으로, $X_C$의 모든 피쳐(확률변수)에 대해 평균을 낸 값입니다. 평균을 낸다는 것은 집합 $C$ 의 피쳐들에 대한 marginal expectation $E$ 를 계산하는 것을 의미하며, 이는 확률 분포에 의해 가중된 예측치에 대한 적분입니다. 주변 분포에 대한 기대값을 계산하려면 모든 인스턴스를 가져와서 집합 $S$ 의 피쳐에 대해 grid 값을 강제하고, 이 조작된 데이터셋에 대한 예측치의 평균을 구하는 것입니다. 이 절차를 통해 피쳐의 주변 분포에 대한 평균을 구하는 것입니다. 

M-plots은 조건부 분포를 통해 예측치의 평균을 냅니다 : 
$$
\begin{aligned}
\hat{f}_{S, M}\left(x_S\right) & =E_{X_C \mid X_S}\left[\hat{f}\left(X_S, X_C\right) \mid X_S=x_s\right] \\
& =\int_{X_C} \hat{f}\left(x_S, X_C\right) d \mathbb{P}\left(X_C \mid X_S=x_S\right)
\end{aligned}
$$
PDP와 비교해서 다른 점은 각 grid 값의 주변 분포를 가정하는 대신 관심 있는 피쳐의 각 grid 값에 조건부로 예측치를 평균낸다는 것입니다. 예를 들어 30㎡가 예측된 주택 가격에 미치는 영향을 계산하려면 28㎡ ~ 32㎡ 사이의 모든 주택에 대한 예측치 평균을 구하는 것처럼, 이웃을 정의해야 합니다. 

ALE plots은 예측치의 변화를 평균내어 grid에 누적합니다 : 
$$
\begin{aligned}
\hat{f}_{S, A L E}\left(x_S\right) & =\int_{z_{0, S}}^{x_S} E_{X_C \mid X_S=x_S}\left[\hat{f}^S\left(X_s, X_c\right) \mid X_S=z_S\right] d z_S-\text { constant } \\
& =\int_{z_{0, S}}^{x_S}\left(\int_{x_C} \hat{f}^S\left(z_s, X_c\right) d \mathbb{P}\left(X_C \mid X_S=z_S\right) d\right) d z_S-\text { constant }
\end{aligned}
$$
이 공식은 M-plot과 3가지 차이점을 갖습니다. 1) 예측치 자체가 아니라 예측치의 변화를 평균냅니다. 이 변화는 편미분(partial derivative)로 정의됩니다 (하지만 실제 계산에서는 간격에 따른 예측치의 차이로 대체됨).

$$
\hat{f}^S\left(x_s, x_c\right)=\frac{\partial \hat{f}\left(x_S, x_C\right)}{\partial x_S}
$$
두 번째 차이점은 2) $z$ 에 대한 추가적인 적분입니다. 집합 $S$ 의 피쳐 범위에 대한 local partial derivatives를 누적하여 피쳐의 예측치에 영향을 측정합니다. 실제 계산에서 $z$ 는 예측치의 변화를 계산하는 간격들의 grid로 대체됩니다. ALE는 예측치를 직접 평균내는 대신, 피쳐 $S$ 에 따라 조건부로 예측치의 차이를 계산하고 피쳐 $S$ 에 대한 미분을 적분하여 효과를 추정합니다. 미분과 적분을 서로 상쇄되는데 왜 이렇게 하는걸까요? 미분 (또는 간격 차이)은 관심 있는 피쳐의 효과를 분리하고 상호 연관된 피쳐들의 효과를 차단하기 때문입니다. 

세 번째 차이점은 3) 상수를 뺀다는 것입니다. 이 단계는 데이터에 대한 평균 효과가 0이 되도록 ALE plot의 중앙에 배치합니다. 

한 가지 문제가 남아 있습니다. 모든 모델에 gradient가 있는 것은 아닌데, 예를 들어 Random Forest 에는 gradient가 없습니다. 그러나 실제 계산은 gradient 없이 작동하며 간격을 사용합니다. ALE plot의 추정에 대해 더 자세히 살펴보겠습니다. 

## 3. Estimation

먼저 한 개의 숫자형 피쳐에 대해 ALE plot의 추정법을 설명하고, 이후 2개의 숫자형 피쳐와 단일 범주형 피쳐의 경우도 설명하겠습니다. 피쳐를 여러 개의 구간으로 나누고 각 구간에서 예측치의 차이를 계산함으로써 지역적 효과(local effects)를 추정할 수 있습니다. 이는 미분에 근사화하는 것으로써 미분이 불가능한 모델에서도 작동합니다. 

먼저 비중심(uncentered) 효과를 추정해 보겠습니다 :
$$
\hat{\tilde{f}}_{j, A L E}(x)=\sum_{k=1}^{k_j(x)} \frac{1}{n_j(k)} \sum_{i: x_j^{(i)} \in N_j(k)}\left[\hat{f}\left(z_{k, j}, x_{-j}^{(i)}\right)-\hat{f}\left(z_{k-1, j}, x_{-j}^{(i)}\right)\right]
$$
**누적 지역 효과(Accumulated Local Effects)**라는 이름은 위 공식의 개별 요소들을 잘 반영하고 있습니다. ALE method의 핵심은 예측치의 차이값을 계산하는 것인데, 관심 있는 피쳐를 grid value $z$ 로 대체합니다. 예측치의 차이는 피쳐가 특정한 간격(interval) 내의 개별 인스턴스에 대해 갖는 효과(Effect) 입니다.  오른쪽의 합계는 간격 내의 모든 인스턴스의 효과들을 더한 것으로, 위 공식에서 이웃 $N_j(k)$로 나타납니다. 이 합계를 구간 내 인스턴스의 수 $n_j(k)$로 나누면 해당 구간에 대한 평균 예측치 차이값을 구할 수 있습니다. 해당 간격의 평균은 ALE에서 Local을 나타냅니다. 왼쪽 합계는 모든 구간(총 $k_j(x)$개의 구간이 존재)에 걸쳐 각 구간에 평균 효과를 누적합 한다는 것입니다. 이는 ALE에서 Accumulated 를 나타냅니다. 

이러한 효과를 중심화시킴으로써 피쳐의 평균 효과가 0이 될 수 있습니다 :
$$
\hat{f}_{j, A L E}(x)=\hat{\tilde{f}}_{j, A L E}(x)-\frac{1}{n} \sum_{i=1}^n \hat{\tilde{f}}_{j, A L E}\left(x_j^{(i)}\right)
$$
ALE 값은 데이터의 평균 예측치와 비교해서 피쳐의 특정 값에서 주효과를 해석할 수 있습니다. 예를 들어, $x_j=3$에서 ALE 추정치가 -2 라는 것은 $j$번째 피쳐값이 3일 때 예측치가 평균 예측치에 비해 2만큼 낮다는 것입니다. 

피쳐의 간격/구간을 정할 때 사용하는 grid는 피쳐 분포의 사분위수를 활용합니다. 사분위수를 사용하면 각 간격에 동일한 개수의 인스턴스가 들어가게 할 수 있습니다. 다만 사분위수를 사용하면 간격의 길이가 다를 수 있다는 단점이 존재합니다. 따라서 관심 피쳐가 심하게 skewed 되어 있는 경우 ALE plot이 이상해질 수 있습니다. 

#### ALE plots for the interaction of two features

ALE plot은 두 개의 피쳐 간 상호작용 효과를 보여줍니다. 원리는 단일 피쳐와 동일하지만, 효과를 2차원으로 누적해야 하므로 간격 대신 직사각형의 셀로 작업합니다. 전체 평균 효과를 조정하는 것 뿐아니라 두 피쳐의 주효과도 조정해야 합니다. 즉, 두 피쳐에 대한 ALE는 피쳐의 주 효과를 포함하지 않는 2차 효과를 추정하는 것입니다. 두 피쳐의 추가적인 상호작용 효과만 표시합니다. 2D ALE plot에 대한 공식은 길고 읽기 불편하기 때문에 생략하겠습니다. 2차 ALE plot 계산에 대한 직관력을 위해 시각화 해보겠습니다 : 
![](Pasted%20image%2020240102140529.png)Figure 8.8 : 2D-ALE 계산. 두 피쳐 위에 grid 를 배치합니다. 각 grid cell에서 그 안의 모든 인스턴스에 대한 2차 차이값을 계산합니다. 먼저 x1과 x2의 값을 셀 모서리의 값으로 바꿉니다. a,b,c,d 가 조작된 인스턴스의 모서리 예측치를 나타낸다고 하고, 2차 차이는 (d-c) - (b-a)가 됩니다. 각 셀의 평균 2차 차이값은 grid에 누적되어 중앙에 위치합니다. 

위 그림은 두 피쳐 간 상관관계로 인해 많은 셀이 비어 있습니다. ALE plot에서는 이를 회색으로 표시하거나 상자를 어둡게 표시해서 시각화할 수 있습니다. 또는 빈 셀의 누락된 ALE 추정치를 가장 가까운 비어 있지 않은 셀의 ALE 추성치로 대체하기도 합니다. 

두 피쳐에 대한 ALE 추정치는 피쳐의 2차 효과만을 표시하기 때문에 해석에 주의가 필요합니다. 2차 표과는 피쳐의 주 효과를 설명한 후 추가적인 상호작용 효과입니다. 두 개의 피쳐가 서로 상호작용하지 않지만 각각이 예측치에 선형적인 영향을 미친다고 가정해 보겠습니다. 각 피쳐에 대한 1D ALE plot에서는 직선으로 예상 ALE curve 를 볼 수 있습니다. 그러나 2D ALE 추정치를 그려보면 2차 효과는 상호작용의 추가적인 효과일 뿐이므로 0에 가까워야 합니다. 이러한 점에서 ALE plot이 PDP와 다른 것입니다 : PDP는 항상 전체 효과를 표시하고, ALE plot 은 1차 또는 2차 효과를 표시합니다. PDP에서 하위 효과를 빼서 주효과 또는 2차 효과를 얻을 수도 있고, 하위 효과를 차감하지 않고 전체 ALE plot을 얻을 수도 있습니다. 

ALE는 임의로 더 높은 차수 (3개 이상 피쳐 간 상호작용)에 대해서도 계산이 가능하지만, 앞서 말한 것처럼 이를 제대로 해석할 수 없기 때문에 최대 2개까지만 의미를 갖습니다. 

#### ALE for categorical features

ALE method는 특정한 방향으로 효과를 누적하기 때문에 정의상 피쳐 값에 순서가 있어야 합니다. 범주형 피쳐에는 자연스러운 순서가 없습니다. 즉 범주형 피쳐에 대한 ALE plot을 그리기 위해선 어떻게든 순서를 만들거나 찾아야 합니다. 범주의 순서는 ALE 계산과 해석에 영향을 미칩니다. 

한 가지 방법은 다른 피쳐들을 기준으로 유사성(similarity)에 따라 범주들의 순서를 정하는 것입니다. 피쳐별 거리는 KS(Kolmogorov-Smirnov) distance (숫자형 피쳐) 또는 상대 빈도 테이블 (relative frequency tables, 범주형 피쳐)로 비교합니다. 모든 범주 간 거리를 파악한 후 다차원 스케일링 (multi-dimensional scaling)을 사용해 거리 행렬을 1차원 거리 측정값으로 축소합니다. 이렇게 하면 범주 간 유사성 기반 순서를 알 수 있습니다. 

예를 들어보겠습니다 : "계절"과 "날씨"라는 2가지 범주형 피쳐와 "온도"라는 숫자형 피쳐가 있다고 가정해 보겠습니다. 첫 번째 범주형 피쳐 (계절)에 대해 ALE를 계산하려고 합니다. 이 피쳐에는 "봄", "여름", "가을", "겨울" 레벨이 있습니다. 수준 "봄"과 "여름" 사이의 거리를 계산해 보겠습니다. 이 거리는 피쳐의 온도와 날씨에 대한 거리의 합입니다. 온도의 경우 "봄"인 모든 인스턴스를 가져와서 empirical cdf를 계산하고 "여름"인 인스턴스에 대해서도 동일 작업을 수행한 다음 KS statistics로 거리를 측정합니다. 날씨 피쳐의 경우 모든 "봄" 인스턴스에 대해 각 날씨 범주에 대한 확률/상대 빈도를 계산하고, "여름"에 대해서도 동일 작업 수행 후 확률 분포의 절대 거리를 합산합니다. "봄"과 "여름"의 온도와 날씨가 매우 다르면 총 레벨 거리가 커집니다. 다른 계절 쌍에 대해서도 이를 반복하고 다차원 스케일링을 통해 거리 행렬을 1차원으로 축소합니다. 

## 4. Examples

ALE plot 실습을 진행하겠습니다. PDP가 실패하는 시나리오를 짜보았습니다. 강한 상관관계가 있는 두 개의 피쳐와 예측 모델로 구성했습니다. 예측 모델은 선형 회귀 모델이지만 인스턴스를 관찰한 적이 없는 두 피쳐 조합에서 이상한 일이 발생합니다 : 
![](Pasted%20image%2020240102142900.png) Figure 8.9 : 두 가치 피쳐와 예측치. 이 모델은 두 피쳐의 합을 예측하지만, x1 > 0.7 & x2 < 0.3 이먄 항상 2를 예측합니다. 이 영역은 데이터 분포와 는 거리가 멀기 때문에 모델 성능에 영향을 미치지 않으며, 해석에 영향을 미치면 안됩니다. 

모델을 학습할 때 알고리즘은 학습 데이터 인스턴스에 대한 손실을 최소화합니다. 학습 데이터 분포 외부에서는 이상한 일이 발생할 수 있지만, 이러한 영역에서 이상한 일이 발생해도 모델에 불이익이 가해지지 않기 때문입니다. 데이터 분포에서 벗어나는 것을 외삽(extrapolation)이라고 하며, 이는 10.4 섹션에서 다루는 것처럼 ML 모델을 속이는 데에도 쓰일 수 있습니다. 아래 그림에서 PDP와 ALE plot이 어떻게 작동하는지를 비교해 보겠습니다 : 
![](Pasted%20image%2020240102143815.png) Figure 8.10 : PDP와 ALE로 계산된 피쳐효과의 비교. PDP는 데이터 분포를 벗어난 모델의 이상한 동작(점프)의 영향을 받습니다. ALE plot은 ML 모델이 데이터가 없는 영역을 무시하고 피쳐와 예측치 간 선형 관계가 있음을 정확히 식별합니다. 

이렇게 극단적인 외삽이 비현실적으로 보일 수 있지만, test set의 분포는 약간 다를 수 있고 일부 인스턴스가 실제로 이상한 범위에 포함될 수 있기 때문에 피쳐 효과 계산에 피쳐 효과 계산에 이러한 영역을 포함시키는 것이 옳은 것으로 보일 수 있습니다. 그러나 아직 관찰하지 않은 영역을 포함시키는 것은 신중해야 하며, PDP와 같이 하면 안됩니다. 모델이 다른 분포를 갖는 데이터와 사용될 경우 ALE plot을 사용하여 예상되는 데이터의 분포를 시뮬레이션하는 것이 좋습니다. 

이제 실제 데이터셋으로 전환해서, 날씨와 요일에 따라 자전거 대여 수를 예측하고 ALE plot이 잘 작동하는지 확인해 보겠습니다. CART를 훈련하여 주어진 날짜에 대여 수를 예측하고, 온도, 상대 습도 및 풍속이 예측치에 어떤 영향을 미치는지 분석해 보겠습니다 : 

![](Pasted%20image%2020240102152957.png) Figure 8.11 : 온도는 예측치에 큰 영향을 미칩니다. 평균 예측치는 온도가 상승함에 따라 증가하지만 섭씨 25도 이상에서는 다시 하락합니다. 습도는 부정적인 영향을 미칩니다 : 60% 이상일 때 상대 습도가 높을수록 예측치가 낮아집니다. 풍속은 예측치에 큰 영향을 미치지 않습니다. 

온도, 습도, 풍속과 다른 모든 피쳐 간 상관관계를 살펴보겠습니다. 데이터에는 범주형 피쳐도 있으므로 피어슨 상관 계수만을 사용할 수 없습니다. 대신 다른 피쳐 중 하나를 입력으로 하여 온도를 예측하는 선형 모델을 훈렵시킨 다음 선형 모델에서 다른 피쳐가 설명하는 분산이 얼마나 되는지 측정하고 제곱근을 구합니다. 이는 숫자형 피쳐를 입력으로 할 시 표준 피어슨 상관 계수의 절대값과 동일합니다. 그러나 이 model-based  방식의 분산 설명(ANOVA)은 다른 피쳐가 범주형일 때도 작동합니다. 다른 모든 피쳐들과 함께 온도, 습도, 풍속의 설명 분산량을 계산하겠습니다. 설명 분산(상관관계)이 높을수록 PDP 사용 시 문제가 커짐을 의미합니다. 아래 그림은 날씨가 다른 피쳐들과 얼마나 강한 상관관계를 갖는지를 시각화한 것입니다. 
![](Pasted%20image%2020240102153155.png) Figure 8.12 : 온도, 습도 및 풍속과 모든 피쳐 간 상관관계의 강도를 설명 분산량으로 측정한 것입니다. 

이러한 상관관계 분석을 통해 특히 온도의 경우 PDP에 문제가 발생할 수 있음을 알 수 있습니다 : 
![](Pasted%20image%2020240102153216.png) Figure 8.13 : 온도, 습도, 풍속에 대한 PDP. ALE plot과 비교했을 때, PDP는 고온 또는 고습도에서 예측 대여 수가 더 적게 감소하는 것을 보여줍니다. PDP는 예를 들어 계절이 "겨울"인 인스턴스일지라고 고온, 고습도의 영향을 계산하기 때문입니다. ALE plot이 더 신뢰성이 있습니다. 

다음으로 범주형 피쳐에 대한 ALE plot을 살펴보겠습니다. 월(`mnth`)이 예측 대여 수에 미치는 영향을 보겠습니다. `mnth`는 이미 피쳐 순서로 정렬되어 있지만, 먼저 유사성별로 다시 정렬한 다음 효과를 계산하면 어떤지 보겠습니다. `mnth`을 온도 또는 휴일 여부와 같은 다른 피쳐들을 기준으로 각 월의 유사도에 따라 정렬됩니다. 
![](Pasted%20image%2020240102153944.png) Figure 8.14 : 월별 다른 피쳐 분포를 기준으로 `mnth`는 서로의 유사성에 따라 정렬됩니다. 1월, 3월, 4월, 그리고 특히 11월, 12월은 다른 월에 비해 예측치에 미치는 영향이 더 낮습니다. 

많은 피쳐가 날씨와 관련이 있기 때문에 월별 순서는 해당 월의 날씨가 얼마나 비슷한지를 강하게 반영하고 있습니다. 추운 달은 왼쪽(2~4월)에, 따듯한 달은 오른쪽(10월 ~ 8월)에 표시됩니다. 예를 들어 공휴일의 상대적 빈도는 월별 유사성 계산 시 온도와 동일한 가중치를 갖는 등 날씨 외적인 요소도 유사성 계산에 포함됨을 유념하세요.

다음으로 습도와 온도가 예측 대여 수에 미치는 2차 효과를 고려해 보겠습니다. 2차 효과는 두 피쳐의 추가적인 상호작용 효과이며, 주효과는 포함하지 않습니다. 즉, 예를 들어 습도가 높으면 평균적으로 예측 대여 수가 줄어든다는 주효과는 2차 ALE plot에서 볼 수 없습니다. 
![](Pasted%20image%2020240102155200.png) Figure 8.15 : 밝은 색은 주 효과를 이미 고려했을 때 평균보다 높은 예측치를, 어두운 색은 평균보다 낮은 예측치를 나타냅니다. 덥고 습한 날씨 예측을 증가시키고, 춥고 습한 날씨는 예측 대여 수에 추가적인 음의 영향을 끼칩니다.


습도와 온도의 주효과는 모두 매우 덥고 습한 날씨에서 예측 대여 수가 감소했다는 것을 유의하세요. 덥고 습한 날씨에 온도와 습도의 (주효과의 합이 아니라 ) 상호작용 효과는 주효과 합보다 큽니다. 순수한 2차 효과(2D ALE plot)와 총 효과의 차이를 강조하기 위해 PDP를 살펴보겠습니다. PDP는 평균 예측치와 두 개의 주 효과 및 2차 (상호작용)효과를 결합한 총 효과를 보여줍니다. 
![](Pasted%20image%2020240102155815.png) Figure 8.16 : 온도와 습도가 예측 대여 수에 미치는 총 효과(total effect)의 PDP입니다. 이 플롯은 상호작용 효과만 보여주는 2D-ALE plot과 달리 각 피쳐의 주효과와 상호작용 효과를 결합합니다. 

상호작용에만 관심이 있는 경우, 총 효과는 주 효과를 혼합해 플롯에 포함하므로 2차효과를 살펴보아야 합니다. 그러나 피쳐의 결합 효과(combined effect)를 알고 싶다면 총 효과(PDP)를 살펴봐야 합니다. 예를 들어, 섭씨 30도, 습도 80%에서 예상되는 자전거 대여 수를 알고 싶다면 2D-PDP에서 직접 읽을 수 있습니다. ALE plot 에서 동일한 정보를 읽으려면 3가지 플롯을 살펴보아야 합니다 : 온도, 습도, 온도 + 습도에 대한 ALE plot & 전체 평균 예측치. 

이제 분류 작업을 위해 자궁경부암 발생 확률을 예측하기 위해 random forest를 학습시키겠습니다. 2가지 피쳐에 대하 ALE plot을 그려보겠습니다 : 
![](Pasted%20image%2020240102160714.png) Figure 8.17 : 자궁경부암 예측 확률에 대한 나이와 호르몬 피임약 복용 기간이 미치는 영향에 대한 ALE plot입니다. 나이의 ALE plot은 예측 암 확률이 평균적으로 40세까지 낮고 그 이후에는 증가함을 보여줍니다. 호르몬 피임약 복용기간이 길수록 8년 후 예측 암 위험이 높아지는 것으로 나타납니다. 

다음으로 임신 횟수와 연령 간 상호작용을 살펴보겠습니다 : 
![](Pasted%20image%2020240102161311.png) Figure 8.18 : 임신 횟수와 연령의 2차 효과에 대한 ALE plot입니다. 이 플롯의 해석은 과적합된 것으로 보입니다. 18~20세의 나이와 3회 이상의 임신횟수에서 이상한 모델 동작(암 확률이 최대 5% 증가)을 보입니다. 데이터에는 이러한 연령과 임신횟수를 가진 여성이 많지 않으므로, 학습 중에 이러한 여성에 대해 실수를 해도 모델이 심각한 불이익을 받지 않습니다. 

## 5. Advantages

- **ALE plot are unbiased** : ALE plot은 비편향이므로 피쳐간 상호 연관된 경우에도 잘 작동합니다. PDP는 비현실적인 외삽을 무시하고 이 조차도 이용하기 때문에 실패하는 케이스입니다. 
- **ALE plots are faster to compute** : ALE plot은 가능한 최대 간격의 수가 인스턴스 당 하나의 간격이 있는 수 이기 때문에 PDP보다 계산 속도가 빠르며 $O(n)$ 으로 확장할 수 있습니다. PDP는 grid point 추정 횟수의 $n$ 배가 필요합니다. Grid points 가 20개인 경우, PDP는 인스턴스 수만큼의 간격이 사용되는 최악의 경우인 ALE plot보다 20배 더 많은 예측치가 필요합니다. 
- **The interpretation of ALE plots is clear** : 주어진 값에 조건부로 피쳐값을 변경하는 것이 예측에 미치는 상대적인 영향을 ALE plot에서 읽을 수 있습니다. ALE plot의 중앙은 0입니다. ALE curve의 각 지점에서의 값이 평균 예측치와의 차이이기 때문에 해석이 용이합니다. 2D-ALE plot은 상호작용만 보여줍니다 : 두 피쳐가 상호작용하지 않으면 플롯에 아무것도 표시되지 않습니다. 
- Functional decomposition 챕터에서 설명할 것 내용으로, 전체 예측 함수는 저차원 ALE 함수의 합으로 분해할 수 있습니다. 
- 대체로 피쳐들 간에는 어느정도의 상관관계가 있기 때문에 PDP보다는 ALE plot을 더 선호합니다. 

## 6. Disadvantages

- 피쳐 간 상관관계가 강할 경우 **구간 사이의 효과를 해석하는 것은 안됩니다**. 피쳐 간 상관관계가 높고 1D-ALE plot의 왼쪽 끝을 보고 있는 경우를 고려해 보겠습니다. ALE plot은 다음과 같은 오해를 부를 수 있습니다 : "ALE curve는 데이터 인스턴스에 대한 각 피쳐 값을 점진적으로 변경하고 다른 피쳐 값을 고정시킬 때 예측치가 평균적으로 어떻게 변하는지를 보여준다." 효과는 구간(local)별로 계산되므로 효과의 해석은 로컬에서만 가능합니다. 
- 피쳐간 상관관계가 있을 때 ALE effect는 선형 회귀 모델에 지정된 계수와 다를 수 있습니다. Grömping (2020)는 2개의 상호 연관된 피쳐와 추가적인 상호 작용 항 ($\hat{f}(x)=\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_3 x_1 x_2$)이 있는 선형 모델에서 1차 ALE plot이 직선을 나타내지 않음을 보였습니다. 대신, 피쳐의 곱셈 상호 작용을 포함하기 때문에 약간 곡선으로 표시됩니다. 여기서 일어나는 일에 대해 자세히 알고 싶다면 Function decomposition 챕터에서 알 수 있습니다. 짧게 말하자면, ALE는 선형 공식이 설명하는 것과는 다르게 1차 효과를 정의합니다. 이는 잘못된 것은 아니지만 확실히 직관에 벗어납니다.
- ALE plot은 간격이 많으면 **약간 흔들릴 수 있습니다 (작은 기복이 많음)**. 이 경우 간격 수를 줄이면 추정치는 안정적이게 되지만, 예측 모델의 실제 복잡성을 낮춰서 보일 수 있습니다. 간격 수를 정하는 데 완벽한 해결책은 없습니다. 간격의 수가 너무 작으면 ALE plot이 정확하지 않을 수 있습니다. 반대로 너무 많으면 곡선이 흔들릴 위험이 있습니다. 
- PDP와 달리 **ALE plot에는 ICE curves 가 동반되지 않습니다**. PDP의 경우 ICE curve는 피쳐 효과의 이질성(heterogeneity), 즉 피쳐 효과가 데이터 하위집합에 따라 다르게 나타나는 것을 표현할 수 있기 때문에 유용합니다. ALE plot은 인스턴스 간 효과가 다른지 여부는 구간별로만 확인할 수 있지, 각 구간마다 다른 인스턴스가 있으므로 ICE curve와 같지 않습니다. 
- **2차 ALE 추정치는 피쳐 공간 전체에 걸쳐 다양한 안정성을 가지며, 이는 어떤 방식으로도 시각화되지 않습니다**. 그 이유는 셀의 로컬 효과에 대한 각 추정치가 서로 다른 수의 인스턴스를 사용하기 때문입니다. 결과적으로 모든 추정치의 정확도가 달리집니다(하지만 여전히 최고의 추정치임). 이 문제는 주효과 ALE plot의 경우 덜합니다. 사분위수 grid를 사용하기 때문에 모든 구간의 인스턴스 수는 동일하지만, 일부 영역에서는 짧은 구간이 많아서 ALE curve가 더 많은 추정치로 구성됩니다. 그러나 전체 곡선의 큰 부분을 차지할 수 있는 긴 각격은 상대적으로 그 수가 적습니다. 예를 들어 고연령에 대한 자궁경부암 예측 ALE plot에서 이런 일이 발생합니다. 
- **2차 효과 플롯은 항상 주 효과를 염두에 두어야 하므로 해석이 다소 번거로울 수 있습니다**. 히트맵은 상호작용의 추가적인 효과일 뿐입니다. 순수한 2차 효과는 상호작용을 발견하고 탐색하는 데는 좋지만, 효과가 어떻게 나타나는지 해석할 때는 주효과 플롯을 동반하는 것이 합리적입니다. 
- ALE plot의 구현은 PDP에 비해 훨씬 복잡하고 직관적이지 않습니다. 
- 상관관계가 있는 피쳐들의 경우 ALE plot은 비편향이지만, 그 상관관계가 강한 경우 해석은 여전히 어렵습니다. 강한 상관관계일 경우 두 피쳐를 따로 분리하지 않고 함께 변경했을 때 효과만 분석하는 것이 합리적이기 때문입니다. 이 단점은 ALE plot 뿐아니라 상관관게가 강한 피쳐의 일반적인 문제입니다. 
- 피쳐 간 상관관계가 없고 계산 시간이 문제되지 않을 경우, 이해가 쉽고 ICE curve와 함께 그릴 수 있는 PDP가 더 바람직합니다. 

## 7. Implementation and Alternatives

대안은 PDP 또는 ICE가 됩니다.

ALE plot은  [`ALEPlot`](https://cran.r-project.org/web/packages/ALEPlot/index.html)패키지에서 가능하며 [`iml`](https://cran.r-project.org/web/packages/iml/index.html) 패키지에서도 가능합니다. Python에서는 [`ALEPython`](https://github.com/blent-ai/ALEPython)과 [`Alibi`](https://docs.seldon.io/projects/alibi/en/stable/index.html)가 있습니다. 

---

# 📕 08-03. Feature Interaction

예측 모델링에서 피쳐끼리 서로 상호작용하는 경우, 하나의 피쳐 효과는 다른 피쳐 값에 따라 달라지기 때문에 예측치를 피쳐 효과의 합으로 표현할 수 없습니다. 

## 1. Feature Interaction ?

ML 모델이 두 가지 피쳐를 기반으로 예측하는 경우, 예측치를 상수항, 1st 항, 2nd 항, 두 피쳐 간 상호작용에 대한 항 이렇게 4가지 항으로 분해할 수 있습니다. 

두 피쳐 간의 상호작용은 개별 피쳐 효과를 고려한 후 피쳐를 변경시킴으로써 발생하는 예측치의 변화를 말합니다. 

예를 들어, 어떤 모델이 집의 크기와 위치를 피쳐로 사용해 주택 가치를 예측하면 가능한 예측은 4가지가 나옵니다:

| Location | Size | Prediction |
| ---: | ---: | ---: |
| good | big | 300,000 |
| good | small | 200,000 |
| bad | big | 250,000 |
| bad | small | 150,000 |
예측치는 다음과 같이 분해됩니다 : 상수 항(150,000), 크기에 대한 효과(+100,000 if big; +0 if small), 위치에 대한 효과(+50,000 if good; +0 if bad). 이러한 분해(decomposition)는 예측치를 완벽하게 설명합니다. 이는 크기와 위치에 대한 단일 피쳐 효과의 합인데, 상호작용 효과가 없기 때문입니다. 

상호작용이 있는 예시를 보겠습니다 :

| Location |  Size | Prediction |
| --------:| -----:| ----------:|
|     good |   big |    400,000 |
|     good | small |    200,000 |
|      bad |   big |    250,000 |
|      bad | small |    150,000 |


상수항(150,000), 크기에 대한 효과(+100,000 if big; +0 if small), 위치에 대한 효과(+50,000 if good; +0 if bad). 이 테이블에는 상호작용에 대한 추가적인 term이 필요합니다 : +100,000 if the house if big and in a good location. 이 경우 큰 집과 작은 집의 예측치 차이가 위치에 따라 달라지기 때문에 이는 크기와 위치 간 상호작용입니다. 

상호작용의 강도를 추정하는 방법은 피쳐의 상호작용에 따라 예측치의 변동이 얼마나 달라지는지를 측정하는 것입니다. 이를 H-statistic이라고 하며 Friedman과 Popescu(2008)가 제안했습니다.

## 2. Theory : Friedman's H-statistic

두 가지 경우를 다뤄 보겠습니다 : 1) 모델에서 두 피쳐가 서로 상호작용하는지 여부와 그 정도를 알려주는 양방향(two-way) 상호작용 측정값, 2) 모델에서 한 피쳐가 다른 모든 피쳐와 상호작용하는지 여부와 그 정도를 알려주는 전체 상호작용 측정값. 이론적으로는 여러 피쳐 간의 임의의 상호작용을 측정할 수 있지만, 이 두 가지가 가장 관심이 가는 것입니다. 

두 피쳐가 상호작용하지 않은 경우 PD 함수를 다음과 같이 분해할 수 있습니다(PD 함수의 중심이 0이라고 가정) : 
$$
P D_{j k}\left(x_j, x_k\right)=P D_j\left(x_j\right)+P D_k\left(x_k\right)
$$

마찬가지로, 하나의 피쳐가 다른 피쳐들과 상호작용하지 않는 경우 예측 함수 $\hat{f}(x)$를 다음과 같이 PD 함수의 합으로 나타낼 수 있습니다. 첫 번째는 $j$에만 종속되고 두 번째는 $j$를 제외한 다른 모든 피쳐들에 종속됩니다 : 
$$
\hat{f}(x)=P D_j\left(x_j\right)+P D_{-j}\left(x_{-j}\right)
$$
여기서 $PD_{-j}(x_{-j})$는 $j$번째 피쳐를 제외한 다른 모든 피쳐들에 대한 PD 함수입니다. 

이러한 분해는 상호작용이 없는 PD 함수를 표현합니다 (j와 k, 또는 각 j와 다른 모든 피쳐 간). 다음 단계는 관찰된 PD 함수와 상호작용 없이 분해된 PD 함수의 차이값을 측정합니다. PD(두 피쳐 간 상호작용 측정) 또는 전체 함수(한 피쳐와 다른 모든 피쳐간 상호작용 측정)의 출력의 분산을 계산합니다. 상호작용에 의해 설명되는 분산량 (관찰된 PD와 상호작용이 없는 PD의 차이)은 상호작용 강도에 대한 통계치로 쓰입니다. 상호작용이 없는 경우 이 값은 0이고, $PD_{jk}$ 또는 $\hat{f}$ 의 모든 분산이 PD 함수의 합으로 설명되는 경우 이 값인 1입니다. 두 피쳐 간 상호작용 통계치가 1이면 각각의 단일 PD 함수가 일정하고 예측치에 미치는 영향은 상호작용을 통해서만 발생함을 의미합니다. H-stat 은 1보다 클 수 있는데, 이는 해석이 더 어렵습니다. 이는 2-way 상호작용의 분산이 2D-PDP 의 분산보다 클 때 발생합니다.

수학적으로 Friedman and Popescu가 피쳐 $j$ 와 $k$ 간 상호작용에 대해 제안한 H-stat은 아래와 같습니다 : 
$$
H_{j k}^2=\frac{\sum_{i=1}^n\left[P D_{j k}\left(x_j^{(i)}, x_k^{(i)}\right)-P D_j\left(x_j^{(i)}\right)-P D_k\left(x_k^{(i)}\right)\right]^2}{\sum_{i=1}^n P D_{j k}^2\left(x_j^{(i)}, x_k^{(i)}\right)}
$$
피쳐 $j$ 가 다른 피쳐와 상호작용하는지 여부를 측정할 때도 동일하게 적용됩니다 : 
$$
H_j^2=\frac{\sum_{i=1}^n\left[\hat{f}\left(x^{(i)}\right)-P D_j\left(x_j^{(i)}\right)-P D_{-j}\left(x_{-j}^{(i)}\right)\right]^2}{\sum_{i=1}^n \hat{f}^2\left(x^{(i)}\right)}
$$
H-stat은 모든 데이터 포인트에 대해 반복하고 각 포인트에서 PD 를 평가하며, 이는 다시 모든 n개의 포인트로 수행되기 때문에 계산 비용이 많이 듭니다. 최악의 경우, 2-way H-stat ($j$ 대 $k$)를 계산하려면 ML 모델 예측 함수를 $2n^2$ 번 호출해야 하고, 전체 H-stat ($j$ 대 전체)를 계산하려면 $3n^2$ 번 호출해야 합니다. 계산 속도를 높이기 위해 n개의 데이터 포인트에서 샘플링을 할 수 있습니다. 이렇게 하면 PD 추정치의 분산이 증가해서 H-stat가 불안정해진다는 단점이 있습니다. 따라서 계산 부담을 줄이기 위해 샘플링을 할 때는 꽤 많은 수를 해야 합니다. 

Friedman and Popescu는 H-stat 이 0과 유의하게 다른지 평가힉 위한 검정 통계량도 제안했습니다. $H_0$은 상호작용이 없다는 가설입니다. $H_0$ 하에서 상호작용 통계량을 구하려면 피쳐 $j$와 $k$ 또는 다른 모든 피쳐 간 상호작용이 없도록 모델을 조정할 수 있어야 합니다. 이는 모든 유형의 모델에서 가능하진 않습니다. 따라서 이 검정은 model-agnostic method가 아니므로 여기서 다루진 않겠습니다.

예측치가 확률인 경우 분류 문제에서도 상호작용의 강도 통계치를 적용할 수 있습니다.

## 3. Examples

이제 피쳐 간 상호작용이 실제로 어떻게 이루어지는지 예제를 통해 살펴보겠습니다. 날씨와 여러 피쳐들을 기반으로 자전거 대여 수를 예측하는 SVM에서 피쳐의 상호작용의 강도를 측정하겠습니다. 아래 그림은 피쳐 간 상호작용의 H-stat 값을 보여줍니다 : 
![](Pasted%20image%2020240102193354.png) Figure 8.19 : 자전거 대여 수를 예측하는 SVM에 대해서 각 피쳐를 나머지 피쳐들과의 상호작용 강도(H-stat)를 측정한 것입니다. 전반적으로 피쳐 간 상호작용 효과는 매우 약합니다 (피쳐당 분산 설명이 10% 미만 이므로).

다음으로는 분류 문제에 대한 상호작용 H-stat 을 계산해 보겠습니다. 몇 가지 위험 요소(features)가 주어졌을 때, 자궁경부암을 예측하는 Random Forest 에서 피쳐 간 상호작용을 분석했습니다 :
![](Pasted%20image%2020240102194432.png) Figure 8.20 : 자궁경부암 확률을 예측하는 Random Forest에서 각 피쳐와 나머지 피쳐 간 상호작용 강도 (H-stat)를 측정한 것입니다. 호르몬 피임약 복용 기간이 다른 모든 피쳐들과의 상대적인 상호작용 효과가 가장 높고, 그 다음은 임신 횟수입니다. 


## 4. Advantages

상호작용 H-stat은 PD 분해(decomposition)를 통한 기저 이론을 갖습니다. 

H-stat은 의미가 있는 해석법입니다 : 상호작용은 설명되는 분산의 점유율로 정의됩니다.

이 통계치는 차원이 없으므로 피쳐 간, 심지어 모델 간에도 비교할 수 있습니다.

이 통계치는 특정한 형태에 관계없이 모든 종류의 상호작용을 감지합니다.

H-stat을 사용하면 3개 이상의 피쳐 간의 상호작용 강도 같은 모든 높은 차수의 상호작용도 분석할 수 있습니다. 

## 5. Disadvantages

상호작용 H-stat은 계산 비용이 많이 듭니다. 

이 계산에는 주변 분포를 추정하는 작업이 포함됩니다. 모든 데이터 포인트를 사용하지 않는 경우 이러한 추정치에도 일정한 분산이 있습니다. 즉, 포인트를 샘플링할 때마다 추정치도 달라지고 결과가 불안정할 수 있습니다. 안정적인 결과를 얻기에 충분한 데이터가 있는지 확인하기 위해 H-stat 계산을 몇 번 반복하는 것이 좋습니다.

상호작용이 0보다 유의하게 큰지는 불분명합니다. 통계적 테스트를 수행해야 하지만 이 테스트는 mode-agnostic version에서는 아직 제공되지 않습니다.

검정 문제에 있어서, H-stat이 언제 상호작용이 "강한" 것으로 판단할 수 있는지 명확히 말하기 어렵습니다.

또한 H-stat이 1보다 클 수 있어서 해석이 어려울 수 있습니다.

두 피쳐의 총 효과는 약하지만 대부분이 상호작용으로 구성된 경우, H-stat이 매우 커질수 있습니다. 이러한 상호작용의 H-stat 은 분모가 작아야 하며, 피쳐가 서로 상관관계가 있을 때 더 심해집니다. 실제로는 두 피쳐가 모델에 별 역할을 하지 않는데도 가짜 상호작용이 강력한 상호작용 효과로 쉽게 과대 해석될 수 있습니다. 이 문제를 해결하는 방법은 정규화되지 않은(unnormalized) H-stat, 즉 H-stat 분자의 제곱근을 시각화하는 것입니다. 이렇게 하면 적어도 회귀 문제의 경우 H-stat을 target과 동일한 수준으로 확장하고 가짜 상호작용을 덜 강조할 수 있습니다. 

$$
H_{j k}^*=\sqrt{\sum_{i=1}^n\left[P D_{j k}\left(x_j^{(i)}, x_k^{(i)}\right)-P D_j\left(x_j^{(i)}\right)-P D_k\left(x_k^{(i)}\right)\right]^2}
$$

H-stat은 상호작용의 강도를 알려주지만, 상호작용이 어떻게 나타나는지는 알려주지 않습니다. 이 점이 바로 PDP의 목적입니다. 우선 상호작용의 강도를 측정한 다음 관심 있는 상호작용에 대한 2D-PDP를 만들면 됩니다. 

입력이 pixels인 경우 H-stat은 제대로 사용할 수 없습니다. 따라서 이 기법은 image classifier에는 유용하지 않습니다. 

상호작용 통계치는 피쳐를 독립적으로 shuffle할 수 있다는 가정 하에 작동합니다. 피쳐 간 상관관계가 강하면 이 가정이 깨지고 실제로는 가능성이 거의 없는 피쳐 조합을 통합하게 됩니다. 이는 PDP에서 발생하는 문제와 동일한 문제입니다. 상호 연관된 피쳐들은 H-stat의 큰값으로 이어집니다.

## 6. Implementations

이 섹션의 예제는 CRAN에서 제공되는 패키지 [`iml`](https://cran.r-project.org/web/packages/iml/)의 dev version을 이용했습니다. 특정한 모델에 초점을 맞춘 것도 있습니다. `RulFit` 패키지와 `pre` 패키지는 H-stat을 구현합니다. 또한 `gbm` 패키지는 부스팅 모델들과 H-stat을 구현합니다. 

## 7. Alternatives

H-stat 말고도 상호작용을 측정할 수 있는 방법은 있습니다 : 

- Hooker (2004)의 Variable Interation Networks; VIN 는 예측 함수를 주 효과와 상호작용 효과로 분해하는 접근법입니다. 그런 다음 피쳐 간 상호작용을 연결망으로 시각화합니다. 안타깝게도 아직 사용할 수 있는 SW는 없습니다.
- Greenwell et al. (2018)의 Partial dependence based feature interaction은 피쳐 간 상호작용을 측정합니다. 이 방식은 다른 피쳐들의 fixed points에 따라 하나의 피쳐의 변수 중요도(PDP 함수의 분산으로 정의)를 측정합니다. 분산이 크면 두 피쳐가 서로 상호작용하는 것이고, 0이면 상호작용하지 않는 것입니다. 이는 `vip` 패키지에서 사용할 수 있습니다. 이 패키지는 PDP와 변수 중요도에 대해서도 다룹니다.
---

# 📕 08-04. Functional Decomposition

Supervised ML 모델은 고차원 피쳐를 input으로 사용하여 예측 또는 분류를 생성하는 함수로 볼 수 있습니다. 함수 분해(Functional decomposition)는 고차원 함수를 분해하여 시각화할 수 있는 개별 피쳐 효과 (individual feature effects)와 상호작용 효과의 합으로 표현하는 해석 기법입니다. 또한 함수 분해는 많은 해석 기법의 기본 원리로, 다른 해석 기법을 보다 더 잘 이해할 수 있도록 도와줍니다.

특정 함수를 살펴보겠습니다. 이 함수는 2개의 피쳐를 input으로 받아 1차원 output을 생성합니다: 
$$
y=\hat{f}\left(x_1, x_2\right)=2+e^{x_1}-x_2+x_1 \cdot x_2
$$
이 함수를 ML 모델이로 생각하면 됩니다. 이 함수를 3D plot이나 등고선(contour)이 있는 히트맵으로 시각화할 수 있습니다 : 
![](Pasted%20image%2020240102201444.png) Figure 8.22 : 두 피쳐 $X_1$, $X_2$ 의 함수의 예측 표면

이 함수는 $X_1$이 크고 $X_2$가 작을 때 큰 값을 취하고, $X_2$가 크고 $X_1$이 작을 때는 작은 값을 취합니다. 예측 함수는 단순히 두 피쳐 간 덧셈 효과가 아니라 두 피쳐 사이의 상호작용입니다. 상호작용의 존재는 위 그림에서 볼 수 있는데, 피쳐 $X_1$의 값 변화의 효과는 $X_2$ 값에 따라 달라집니다. 

이제 우리가 할 일은 이 함수를 피쳐 $X_1$과 $X_2$의 주효과와 상호작용 term으로 분해하는 것입니다. 두 개의 input 피쳐에만 의존하는 2차원 함수 $\hat{f}$ 의 경우 다음과 같이 분해합니다 - 
$\hat{f}(x_1,x_2)$의 경우 각 성분이 주 효과 ($\hat{f}_1$와 $\hat{f}_2$), 상호작용 효과 ($\hat{f}_{1,2}$) 또는 절편 ($\hat{f}_0$) : 
$$
\hat{f}\left(x_1, x_2\right)=\hat{f}_0+\hat{f}_1\left(x_1\right)+\hat{f}_2\left(x_2\right)+\hat{f}_{1,2}\left(x_1, x_2\right)
$$
주효과는 다른 피쳐값과 무관하게 각 피쳐가 예측치에 미치는 영향을 나타냅니다. 상호작용 효과는 피쳐의 공동(joint) 효과를 나타냅니다. 절편은 모든 피쳐 효과가 0으로 설정되었을 때 예측치가 어떻게 되는지 알려줍니다. 

각 구성 요소에 대해 시각화 해보겠습니다. 절편은 $\hat{f}_0 \sim 3.18$ 입니다 : 
![](Pasted%20image%2020240102203347.png) Figure 8.23 : Decomposition of a function

$x_1$ 피쳐는 exponential 인 주효과를 나타내고, $x_2$는 음의 선형 효과를 나타냅니다. 상호작용 term은 프링글스 칩과 비슷하게 생겼습니다. 수학적으로 표현하자면 $x_1 \cdot x_2$ 의 쌍곡선 포물선입니다. 이러한 분해는 뒤에서 설명할 ALE plots 을 기반으로 합니다.

## 1. How to Compute the Components I

왜 이렇게 복잡할까요? 공식을 보면 이미 분해에 대한 답을 알 수 있는데 복잡한 방법이 필요 없겠죠? 피쳐 $x_1$의 경우, $x_1$만 포함된 것들을 모두 합하여 해당 피쳐값의 성분으로 취할 수 있습니다. 즉, $\hat{f}_1(x_1)=e^{x_1}$ 이고, $x_2$의 경우 $\hat{f}_2(x_2)=-x_2$가 됩니다. 그러면 상호작용은 $\hat{f}_{12}(x_1,x_2)=x_1\cdot x_2$ 이 됩니다. 이 예제에서는 이것이 정답이지만 이러한 접근법에는 두 가지 문제점이 있습니다 : 1) 이 예제는 참 공식으로 출발했지만, 실제로는 이렇게 깔끔한 공식으로는 설명할 수 있는 ML 모델은 거의 없습니다. 2) 두 번째 문제점은 더 복잡한데, 상호작용이 무엇인지와 관련 되어 있습니다. 두 피쳐가 모두 0보다 큰 값을 가지며 서로 독립적인 간단한 함수 $\hat{f}(x_1,x_2)=x_1\cdot x_2$ 를 고려해 보겠습니다. 방금 전처럼 look-at-the-formula 방식으로 보면 피쳐 $x_1$과 $x_2$ 간 상호작용이 있지만 개별 피쳐 효과는 없다는 결론이 내려집니다. 다른 피쳐 $x_2$ 값에 관계없이 $x_1$를 증가시키면 예측치는 증가합니다. 예를 들어, $x_2=1$ 일 때 $x_1$의 효과는 $\hat{f}(x_1, 1) = x_1$ 이고, $x_2 = 10$ 일 때는 $\hat{f}(x_1,10)=10\cdot x_1$ 입니다. 따라서 피쳐 $x_1$은 $x_2$와 무관하게 예측치에 양의 영향을 미치며 그 효과가 0이 아님이 분명합니다. 

현실 문제에는 깔끔한 수식/공식이 없다는 문제 1)을 해결하려면 예측 함수 또는 분류 점수만을 사용하는 방법이 필요합니다. 문제 2)의 정의 부족 문제를 해결하려면 구성 요소가 어떻게 생겼는지와 서로가 어떻게 관계되어 있는지를 알려주는 몇 가지 정리/원리가 필요합니다. 우선은 함수 분해가 무엇인지 더 정확하게 정의해봅시다.

## 2. Functional Decomposition

예측 함수는 $p$ 개의 피쳐들을 input으로 받고, $\hat{f}: \mathbb{R}^p \mapsto\mathbb{R}$ output 을 내뱉습니다. 이는 회귀 함수일 수도 있고 클래스 확률이나 클러스터/군집에 대한 점수(비지도 학습)일 수도 있습니다. 완전히 분해하면 예측 함수를 functional components의 합으로 나타낼 수 있습니다 : 
$$
\begin{aligned}
\hat{f}(x)= & \hat{f}_0+\hat{f}_1\left(x_1\right)+\ldots+\hat{f}_p\left(x_p\right) \\
& +\hat{f}_{1,2}\left(x_1, x_2\right)+\ldots+\hat{f}_{1, p}\left(x_1, x_p\right)+\ldots+\hat{f}_{p-1, p}\left(x_{p-1}, x_p\right) \\
& +\ldots \\
& +\hat{f}_{1, \ldots, p}\left(x_1, \ldots, x_p\right)
\end{aligned}
$$
피쳐 조합들의 가능한 모든 부분 집합을 indexing 하여 분해 공식을 좀 더 간단하게 작성할 수 있습니다 : $S\subseteq\{1,\ldots,p\}$. 이 집합에는 절편 ($S=\emptyset$), 주 효과 ($|S|=1$), 모든 상호작용 ($|S| \ge 1$)이 포함됩니다. 이 부분 집합을 정의하면 아래와 같이 분해식을 쓸 수 있습니다 :
$$
\hat{f}(x)=\sum_{S \subseteq\{1, \ldots, p\}} \hat{f}_S\left(x_S\right)
$$
공식에서 $x_S$는 인덱스 집합 $S$의 피쳐 벡터입니다. 그리고 각 부분 집합 $S$는 functional component (예: $S$에 하나의 피쳐만 포함된 경우 주효과, $|S|>1$인 경우 상호작용)를 나타냅니다.

위 공식에는 몇 개의 components가 있을까요? 피쳐 $1,\ldots,p$ 가능한 부분 집합 $S$의 경우의 수인 $\sum_{i=0}^p\left(\begin{array}{c}p \\ i\end{array}\right)=2^p$ 입니다. 예를 들어 한 함수가 10개의 피쳐를 사용하는 경우 함수를 $2^{10}=1024$ 개의 components로 분해할 수 있는 것입니다. 그리고 피쳐가 추가될 때마다 이 컴포넌트의 수는 기존의 2배가 됩니다. 하지만 모든 컴포넌트를 계산하는 것은 불가능하고, 그러지 않는 또 다른 이유는 $|S| >2$인 성분은 시각화와 해석이 어렵기 때문입니다.

## 3. How not to Compute the Components II

여태까지는 컴포넌트/성분들이 어떻게 정의되고 계산되는지에 대해서는 언급하지 않았습니다. 컴포넌트의 수와 차원, 그리고 이들의 합이 원래/기존의 함수를 산출해야 한다는 제약 조건만 말했습니다. 하지만 그 컴포넌트들에 대한 추가적인 제약이 없다면 이들은 unique 하지 않습니다. 즉, 주 효과와 상호작용 또는 저차 상호작용(few features)과 고차 상호작용(more features) 간 효과를 이동시킬 수 있습니다. 이 섹션의 시작 부분에 있는 예제에서는 두 개의 주 효과를 모두 0으로 설정하고 그 효과를 상호작용 효과에 추가할 것입니다. 

다음은 컴포넌트들에 대한 추가 제약 조건의 필요성을 보여주는 더 극단적인 예시입니다. 3차원 함수가 있다고 가정해 보겠습니다. 이 함수의 생김새는 중요하지 않지만, 아래와 같은 분해가 작동합니다 :  $\hat{f}_0=0.12$. $\hat{f}_1(x_1)2\cdot x_1 +\text{shoes}$. $\hat{f}_2,\hat{f}_3, \hat{f}_{1,2},\hat{f}_{2,3}, \hat{f}_{1,3}$는 모두 0입니다. 그리고 이 트릭을 위해 $\hat{f}_{1,2,3}(x_1,x_2,x_3)=\hat{f}(x)-\sum_{S\in\{1,\ldots,p\}}\hat{f}_S(x_S)$로 정의합니다. 따라서 모든 피쳐들을 포함하는 상호작용 항은 나머지 효과를 모두 흡수하며 모든 컴포넌트들의 합이 원래의 예측함수를 제공한다는 의미에서 정의상 항상 작동합니다. 이러한 분해는 모델의 해석으로 제시할 때는 의미가 없습니다.

이러한 모호함(ambiguity)은 추가적인 제약이나 컴포넌트 계산을 위한 구체적인 방법을 지정함으로써 피할 수 있습니다. 여기서는 함수 분해에 대한 3가지 방법을 설명하겠습니다 : 
- (Generalized) Functional ANOVA
- Accumulated Local Effects
- Statistical regression models

## 4. Functional ANOVA

Hooker (2004)는 Functional ANOVA를 제안했습니다. 이 접근법의 전제 조건은 모델 예측 함수 $\hat{f}$ 가 square integrable 해야 한다는 것입니다. functional ANOVA 는 함수를 컴포넌트들로 분해합니다 : 
$$
\hat{f}(x)=\sum_{S \subseteq\{1, \ldots, p\}} \hat{f}_S\left(x_S\right)
$$
Hooker (2004)는 각 컴포넌트를 다음과 같은 공식으로 정의했습니다 : 
$$
\hat{f}_S(x)=\int_{X_{-S}}\left(\hat{f}(x)-\sum_{V \subset S} \hat{f}_V(x)\right) d X_{-S}
$$
이것을 분해해 보겠습니다. 컴포넌트를 다음과 같이 다시 쓸 수 있습니다 : 
$$
\hat{f}_S(x)=\int_{X_{-S}}(\hat{f}(x)) d X_{-S}-\int_{X_{-S}}\left(\sum_{V \subset S}) \hat{f}_V(x)\right) d X_{-S}
$$
첫 번째는 집합 $S$에서 제외된 피쳐($-S$)들에 대한 예측 함수에 대한 적분입니다. 예를 들어서 피쳐 2와 3에 대한 2-way 상호작용 성분을 계산하면 피쳐 1,4,5,... 에 대해 적분하는 것입니다. 적분은 모든 피쳐들이 최소값에서 최대값까지 uniform 분포를 따른다고 가정할 때 $X_{-S}$에 대한 예측 함수의 기댓값으로 볼 수 있습니다. 이 구간에서 $S$의 부분 집합을 갖는 모든 컴포넌트들을 뻅니다. 이 뺄셈은 모든 하위 효과(lower-order effects)를 제거하고 효과를 중심화합니다. $S=\{1,2\}$ 의 경우 두 피쳐 $\hat{f}_1$과 $\hat{f}_2$ 의 주효과와 절편 $\hat{f}_0$을 뺍니다. 이러한 하위 효과의 발생은 수식을 재귀적으로 만듭니다 : 부분 집합의 계층 구조를 거쳐 절편에 도달한 후 이 모든 성분을 계산해야 합니다. 절편 성분 $\hat{f}_0$의 경우 부분 집합은 빈 집합 $S=\{\emptyset\}$이므로 $-S$ 가 모든 피쳐들을 포함합니다.
$$
\hat{f}_0(x)=\int_X \hat{f}(x) d X
$$

이것은 단순히 모든 피쳐들에 대해 통합된 예측 함수입니다. 또한 모든 피쳐들이 uniform 분포를 따른다고 할 때 절편은 예측 함수의 기댓값으로 해석할 수 있습니다. 이제 $\hat{f}_0$ 에 대해 알았으므로 $\hat{f}_1$과 $\hat{f}_2$를 계산할 수 있습니다. 
$$
\hat{f}_1(x)=\int_{X_{-1}}\left(\hat{f}(x)-\hat{f}_0\right) d X_{-S}
$$
컴포넌트 $\hat{f}_{1,2}$에 대한 계산을 완료하려면 모든 것을 합치면 됩니다 :
$$
\begin{aligned}
\hat{f}_{1,2}(x) & =\int_{X_{3,4}}\left(\hat{f}(x)-\left(\hat{f}_0(x)+\hat{f}_1(x)-\hat{f}_0+\hat{f}_2(x)-\hat{f}_0\right)\right) d X_3, X_4 \\
& =\int_{X_{3,4}}\left(\hat{f}(x)-\hat{f}_1(x)-\hat{f}_2(x)+\hat{f}_0\right) d X_3, X_4
\end{aligned}
$$
이 예는 각 고차 효과(high-order effect)가 다른 모든 피쳐들을 통합하고 관심 있는 피쳐 집합의 부분 집합인 모든 하위 효과를 제거하여 정의되는 방법을 보여줍니다.

Hooker (2004)는 이러한 functional components 의 정의가 아래의 공리(axioms)들을 만족한다고 했습니다 : 
- Zero Means :  $\int \hat{f}_S\left(x_S\right) d X_s=0$ for each $S\ne\emptyset$.
- Orthogonality : $\int \hat{f}_S\left(x_S\right) \hat{f}_V\left(x_v\right) d X=0$ for $S\ne V$
- Variance Decomposition : Let $\sigma_{\hat{f}}^2=\int \hat{f}(x)^2 d X$, then $\sigma^2(\hat{f})=\sum_{S \subseteq\{1, \ldots, p\}} \sigma_S^2\left(\hat{f}_S\right)$

Zero Means 공리는 모든 효과 또는 상호작용이 0을 중심으로 구성됨을 의미합니다. 결과적으로 x에서의 해석은 절대적인 예측치가 아니라 중심화된 예측에 대한 상대적인 해석인 것입니다.

Orthogonality 공리는 컴포넌트끼리 정보를 공유하지 않는다는 것을 의미합니다. 예를 들어 피쳐 $X_1$의 1차 효과와 $X_1$, $X_2$ 간 상호작용 하은 서로 상관관계가 없습니다. 직교성 때문에 모든 컴포넌트는 효과를 혼합하지 않는다는 의미에서 "pure" 합니다. 예를 들어, $X_4$의 컴포넌트가 $X_1$과 $X_2$ 간 상호작용과 독립적이어야 한다는 것은 당연합니다. 하나의 컴포넌트가 다른 컴포넌트들의 피쳐, 예를 들어서 $X_1$과 $X_2$ 간 상호작용과 $X_1$의 주효과를 포함하는 계층적 컴포넌트의 직교성에서는 흥미로운 일이 발생합니다. 이와는 대조적으로, $X_1$과 $X_2$에 대한 2D-PDP는 4개의 효과, 즉 절편, $X_1$과 $X_2$의 주효과와 상호작용을 포함합니다. $\hat{f}_{1,2}(x_1,x_2)$에 대한 functional ANOVA는 순수한 상호작용만을 포함합니다. 

Variance decomposition 을 사용하면 함수 $\hat{f}$의 분산을 컴포넌트들로 분해할수 있으며, 최종적으로 함수의 총 분산을 합산할 수 있다는 것을 보장합니다. 이것이 "Functional ANOVA"라고 부르는 이유입니다. 분산분석은 target의 평균 차이를 분석하는 방법을 말합니다. ANOVA는 분산을 나누고 이를 변수가 기여하는 방식으로 작동합니다. 따라서 functional ANOVA 는 이 개념을 모든 함수로 확장한 것으로 볼 수 있습니다. 

하지만 피쳐들이 상호 연관되어 있을 경우 functional ANOVA 는 문제가 생길 수 있습니다. 이에 대한 해결책으로 generalized functional ANOVA 가 제안되어 있습니다.

## 5. Generalized Functional ANOVA for Dependent Features

대부분의 샘플링 데이터 기반 해석 기법(예: PDP)과 마찬가지로, functional ANOVA 는 피쳐들이 상호 연관되어 있을 때 문제가 발생합니다. 균등 분포에 대해 통합할 경우, 실제로는 피쳐들이 종속적인 joint 분포에서 벗어나 가능성이 낮은 조합으로 추정되는 새로운 데이터셋이 만들어집니다. 

Hooker (2007)는 dependent features에 대해 작동하는 분해 기법인 generalized functional anova를 제안했습니다. 컴포넌트들은 $f$ 를 가법적인 함수들의 공간에 사영시킨 것으로 정의됩니다 : 
$$
\hat{f}_S\left(x_S\right)=\operatorname{argmin}_{g_S \in L^2\left(\mathbb{R}^S\right)_{S \in P}} \int\left(\hat{f}(x)-\sum_{S \subset P} g_S\left(x_S\right)\right)^2 w(x) d x .
$$

컴포넌트들은 orthogonality 대신 계층적 직교성 조건(hierarchical orthogonality condition)을 만족합니다 :
$$
\forall \hat{f}_S\left(x_S\right) \mid S \subset U: \int \hat{f}_S\left(x_S\right) \hat{f}_U\left(x_U\right) w(x) d x=0
$$
계층적 직교는 직교와는 다릅니다. 두 개의 집합 $S$와 $U$에 대하여, 어느 쪽도 다른 쪽의 부분집합이 아닌 경우, 분해가 계층적 직교가 되기 위해 성분 $\hat{f}_S$ 및 $\hat{f}_U$가 직교일 필요는 없습니다. 그러나 $S$의 모든 부분집합에 대한 모든 컴포넌트는 $\hat{f}_S$에 직교해야 합니다. 결과적으로 해석은 달라집니다 : ALE 챕터의 M-plot과 유사하게 generalized functional ANOVA는 상관된 피쳐들의 (주변) 효과를 포함할 수 있습니다. 컴포넌트가 주변 효과를 포함할지 여부는 가중치 함수 $w(x)$에 따라 달라집니다. 단위 큐브(unit cube)의  uniform measure 으로 $w$ 을 선택하면 functional ANOVA 를 구할 수 있습니다. $w$에 대한 자연스러운 선택은 joint pdf 입니다. 하지만 joint distribution 은 보통 알 수 없으면 추정이 어렵습니다. 한 가지 요령은 unit cube의 uniform measure 로 시작하여 데이터가 없는 영역을 잘라내는 것입니다. 

이 추정은 피쳐 공간의 grid에서 수행되며 회귀 기법을 사용하여 해결할 수 있는 minimization 문제입니다. 그러나 컴포넌트들은 개별적 또는 계층적으로 계산할 수 없으며, 다른 컴포넌트들과 관련된 복잡한 방정식으로 풀어야 합니다. 따라서 계산이 매우 복잡합니다.

## 6. Accumulated Local Effect Plots

ALE plot 도 함수 분해를 제공하므로 절편, 1D-ALE, 2D-ALE 등의 모든 ALE plot들을 더하면 예측 함수를 얻을 수 있습니다. ALE는 컴포넌트가 직교(orthogonal)하지 않고 유사 직교(pseudo-orthogonal)하기 때문에 (generalized) functional ANOVA와는 다릅니다. pseudo-orthogonality를 이해하려면 함수 $\hat{f}$ 를 취하여 피쳐 부분 집합 $S$ 에 대한 ALE plot에 매핑하는 연산자 $H_S$를 정의해야 합니다. 예를 들어, $H_{1,2}$ 연산자는 ML 모델을 입력(input)으로 받아 피쳐 1과2에 대한 2D-ALE plot을 만듭니다 : $H_{1,2}(\hat{f})=\hat{f}_{A L E, 12}$ . 동일한 연산자를 두 번 적용하면 동일한 ALE plot을 얻을 수 있습니다. 연산자 $H_{1,2}$를 $f$ 에 한 번 적용하면 2D-ALE plot $\hat{f}_{ALE,12}$를 얻습니다. 그런 다음 연산자를 $f$ 가아닌 $\hat{f}_{ALE, 12}$에 다시 적용합니다. 이는 2D-ALE 컴포넌트 자체가 함수기 때문에 가능합니다. 그 결과는 다시 $\hat{f}_{ALE, 12}$이므로 동일한 연산자를 여러 번 적용해도 항상 동일한 ALE plot이 얻어집니다. 이것이 pseudo-orthogonality 의 첫 번째 부분입니다. 하지만 서로 다른 피쳐 집합에 대해 서로 다른 두 개의 연산자를 적용하면 어떤 결과가 나올까요? 예를 들어, $H_{1,2}$와 $H_1$ 또는 $H_{1,2}$와 $H_{3,4,5}$ ? 답은 0입니다. 즉, 동일한 ALE plot을 두 번 적용하지 않는 한 ALE plot의 ALE plot은 0입니다. 다시 말해, 피쳐 집합 $S$에 대한 ALE plot은 다른 ALE plot이 포함되어 있지 않다는 뜻입니다. 수학적인 용어로, ALE 연산자는 함수를 내적 곱(inner product) space의 orthogonal subspaces에 매핑합니다. 

Apley와 Zhu(2020)가 지적했듯이, pseudo-orthogonality 는 피쳐의 주변 효과를 포함하지 않기 때문에 계층적 직교보다 더 바람직할 수 있습니다. 또한 ALE는 joint distribution을 추정할 필요가 없으며, 컴포넌트들을 계층적으로 추정할 수 있으므로 피쳐 1과 2에 대한 2D-ALE를 계산하려면 개별 ALE 와 절편 항을 추가로 계산하기만 하면 됩니다.

## 7. Statistical Regression Models

이 방식은 interpretable models, 특히 GAMs와 관련되어 있습니다. 복잡한 함수를 분해하는 대신 모델링 프로세스에 제약 조건을 구축해 개별 컴포넌트를 쉽게 볼 수 있습니다. 분해는 고차원 함수에서 시작해 분해하는 하향식(top-down) 방식으로 처리할 수 있지만, GAMs는 간단한 컴포넌트에서 모델을 구축하는 상향식(bottom-up) 방식을 제공합니다. 두 방식 모두 개별적이고 해석 가능한 컴포넌트들을 제공하는 것이 목표입니다. 통계 모델에서는 컴포넌트 수를 제한하여 $2^p$ 개의 컴포넌트 모두를 적합할 필요가 없도록 합니다. 가장 간단한 버전은 선형 회귀입니다 :
$$
\hat{f}(x)=\beta_0+\beta_1 x_1+\ldots \beta_p x_p
$$
이 수식은 함수 분해와 매우 유사해 보이지만 두 가지 변형이 있습니다 : 
1) 모든 상호작용 효과는 제외되고 절편과 주효과만 유지됩니다.
2) 주 효과는 피쳐에서 선형효과만 있을 수 있습니다 : $\hat{f}_j(x_j)=\beta_jx_j$.
선형 회귀 모델을 함수 분해 관점에서 보면, 모델 자체가 피쳐에서 target으로 매핑되는 실제 함수의 함수 분해를 나타내지만, 효과가 선형적이고 상호작용이 없다는 강한 조건 하에 있습니다.

GAM은 스플라인을 사용해 보다 복잡한 함수 $\hat{f}_j$를 허용함으로써 두 번째 선형성 가정을 완화합니다. 상호작용을 추가할 수 있긴하지만 이는 다소 수동적입니다. GA2M 과 같은 방식은 양방향 상호작용을 GAM에 추가할 수 있습니다.

선형 회귀 모델이나 GAM을 함수 분해로 간주하면 혼동될 수 있습니다. 이 섹션의 앞부분에서 설명한 분해 방식(generalized functional ANOVA 및 ALE)을 적용하면 GAM에서 보는 것과는 다른 컴포넌트들을 얻게 됩니다. 이것은 상호 연관된 피쳐들의 상호작용 효과가 GAM에서 모델링될 때 발생할 수 있습니다. 이러한 불일치는 다른 함수 분해 방식이 상호작용 효과와 주효과 간 효과를 다르게 분해하기 때문에 발생합니다. 

그렇다면 언제 복잡한 모델의 가법 분해 대신 GAM 을 사용해야 할까요? 대부분의 상호작용이 0인 경우, 특히 3개 이상의 피쳐에 대해서 상호작용이 없는 경우에 GAM을 사용해야 합니다. 상호작용에 관련된 최대 피쳐 수가 두 개($|S| \le 2$), MARS 또는 GA2M과 같은 방식을 사용할 수 있습니다. 궁극적으로 뭐가 더 나은지는 test set에 대한 성능치가 나타내줄 수 있습니다.

## 8. Bonus : PDP

PDP는 함수 분해를 제공하지 않습니다. 피쳐 집합 $S$ 에 대한 PDP는 항상 계층 구조의 모든 효과를 포함하며, $\{1,2\}$에 대한 PDP는 상호작용 뿐아니라 개별 효과도 포함합니다. 결과적으로 모든 부분집합에 대한 모든 PDP를 더해도 원래 함수가 나오지 않으므로 함수 분해가 아닙니다. 

## 9. Advantages

Functional decomposition이 ML interpretation의 핵심 개념입니다.

함수 분해는 고차원적이고 복잡한 모델을 개별 효과와 상호작용으로 분해하는 이론적 근거를 제공하며, 개별 효과를 해석하는 데 필수적인 단계입니다. 함수 분해는 statistical regression models, ALE, (generalized) functional anova, PDP, H-statistic, ICE curve와 같은 기법의 핵심 아이디어 입니다. 

또한 함수 분해는 다른 방법들을 더 잘 이해할 수 있도록 해줍니다. 예를 들어 permutation feature importance는 피쳐와 targer 간 연관성을 분석합니다. 함수 분해 관점으로 보면 순열(permutation)이 피쳐와 관련된 모든 구성 요소(components)의 효과를 '파괴(destroys)' 합니다. 이는 피쳐의 주 효과 뿐 아니라 다른 피쳐들과의 모든 상호 작용에도 영향을 미칩니다. 또 다른 예로, Shapley values는 예측치를 개별 피쳐의 추가/가법적인 효과로 분해합니다. 그러나 함수 분해는 분해에 상호작용 효과도 있어야 함을 알려주는데, 그렇다면 상호작용 효과는 어디에 있을까요? Shapley values는 개별 피쳐에 대한 효과의 공정한 기여도를 제공하므로 모든 상호작용도 피쳐에 대한 공정한 기여도를 가지며, 따라서 Shpley values 로 나누어집니다. 

함수 분해를 도구로 고려할 때, ALE plot을 사용하면 많은 이점을 얻을 수 있습니다. ALE plot은 계산 속도가 빠르고, S/W 구현이 가능하며, 바람직한 pseudo-orthogonality를 가진 함수 분해를 제공합니다. 

## 10. Disadvantages

함수 분해의 개념은 두 피쳐 간 상호작용을 넘어서는 고차원 컴포넌트들에 대해 빠르게 한계점이 옵니다. 피쳐 수가 증가하면 고차 상호작용을 시각화하는 것이 어렵기 때문에 실용성이 제한되고, 모든 상호작용을 계산하려면 계산 시간이 엄청 길어집니다. 

각 함수 분해 방법들에는 단점이 있습니다. 상향식(bottom-up) 방식인 회귀 모델 구축은 상당히 수동적이며 예측 성능에 영향을 줄 수 있는 많은 제약들이 있습니다. Functional ANOVA 는 독립적인 피쳐들이 필요합니다. Genearlized functional ANOVA 는 추정이 매우 어렵습니다. ALE plot 은 분산 분해를 제공하지 않습니다.

함수 분해 방식은 텍스트나 이미지보다 tabular data를 분석하는 데 더 적합합니다.

---
# 📕08-05. Permutataion Feature Importance

순열 변수 중요도(Permutation Feature Importance; PFI)는 피쳐들을 섞어서 true outcome과 피쳐 간 관계(relatioship)을 끊은 후 모델의 예측 오차(prediction error)의 증가값을 측정합니다. 

## 1. Theory

개념은 매우 쉽습니다 : 피쳐들을 섞은 후 모델의 예측 오차의 증가량을 계산해 변수 중요도를 측정합니다. 이 때 피쳐 값을 섞으면 모델 오류가 증가하는 경우는 모델이 예측하는 데에 그 피쳐에 의존했기 때문에 해당 피쳐가 "중요(important)"합니다. 피쳐 값을 섞어도 모델 오류가 변하지 않으면 "중요하지 않음" 으로 간주되며, 이 경우 모델이 예측하는 데 해당 피쳐를 무시했다는 것을 의미합니다. 순열 피쳐 중요도(PFI)의 측정은 Breiman (2001)이 Random Forest에서 도입했습니다. 이 아이디어를 바탕으로 Fisher, Rudin, Dominici (2018)는 보다 발전된 model-agnostic version의 변수 중요도를 소개했습니다. 이 논문의 주요 내용을 소개하겠습니다.

#### The Permutation Feature Importance algorithm based on Fisher, Rudin and Dominici (2018) :

- Input : Trained model $\hat{f}$, feature matrix $X$, target vector $y$, error measure $L(y,\hat{f})$
	1. 기존 모델 오차 $e_{orig} = L(y,\hat{f}(X))$를 추정하세요 : e.g. mean squared error
	2. 각 feature $j\in \{1,\ldots,p\}$에 대해 아래 과정을 진행하세요 : 
		- $X$에 있는 $j$ 번째 feature를 섞어서(permute) featuer matrix $X_{perm}$을 만드세요. 이 동작은 true outcome $y$과 feature $j$ 간 연관성을 끊는 것입니다. 
		- 섞인 데이터를 이용한 예측치로부터 오차 $e_{perm} = L(Y,\hat{f}(X_{perm}))$를 추정하세요.
		- PFI 를 분수 $FI_j = e_{perm}/e_{orig}$ 이나 차이값 $FI_j=e_{perm}-e_{orig}$로 계산하세요.
	3. $p$ 개의 $FI$ 값들을 내림차순으로 정렬하세요.

Fisher, Rudin, and Dominici (2018)는 논문에서 데이터셋을 절반으로 분할하고 피쳐 $j$를 섞는 대신 각 분할의 피쳐값을 교환하는 방법을 제안합니다. 더 정확한 추정을 원한다면, 각 인스턴스를 다른 인스턴스의 피쳐 $j$ 값과 짝 지어(자신은 제외)서 피쳐 $j$ 를 permute 하는 오차를 추정할 수 있습니다. 이렇게 하면 순열 오차 (permutation error)를 추정할 수 있는 크기가 `n(n-1)`의 데이터셋이 필요하며, 계산 시간미 많이 걸립니다. 매우 정확한 추정치를 얻고자 하는 경우에만 n(n-1)-method를 사용하는 것이 좋습니다.

## 2. Should I Compute Importance on Training or Test Data?

**요약 : test data를 사용해야 합니다.**

이 질문에 답하는 것은 변수 중요도가 무엇인지에 대한 근본적인 질문입니다. Train set 기반 변수 중요도와 test set 기반 변수 중요도의 차이를 이해하는 가장 좋은 방법은 '극단적인' 예시를 들어 설명하는 것입니다. 여기서 저는 50개의 무작위 피쳐 (200개의 인스턴스)가 주어졌을 때 연속형 random target outcome을 예측하도록 SVM을 학습시켰습니다. 여기서 "random"이란 target이 50개의 피쳐와는 무관하다는 의미입니다. 모델이 어떤 관계를 "학습"하면 과적합이 발생합니다. 실제로 SVM 은 train data를 과적합 했습니다. Train data의 MAE는 0.29, test data의 MAE는 0.82이며, 이는 평균이 0인 target을 예측하는 best 모델입니다 (MAE : 0.78). 즉, 이 SVM 모델은 형편없습니다. 이러한 과적합된 SVM의 50개 피쳐에 대해 변수 중요도는 의미가 없습니다. Unseen test data의 성능 향상에 기여하는 피쳐가 하나도 없으므로 0으로 해야할까요? 아니면 학습된 관계가 보이지 않는 (unseen) 데이터에 일반화되는지 여부와 관계없이 모델이 각 피쳐에 얼마나 의존하는지를 반영해야 할까요? 우선 train data와 test data의 변수 중요도 분포가 어떻게 다른지 살펴보겠습니다.

![](Pasted%20image%2020240103000212.png) Figure 8.24 : SVM이 데이터를 과적합했습니다. Train data를 기반으로 한 변수 중요도는 중요한 피쳐들을 많이 보여줍니다. Unseen test data 에서 계산된 변수 중요도는 1(중요하지 않음)에 가깝습니다.

두 결과 중 아직 어느 것이 더 바람직한지 알 수 없습니다. 따라서 두 버전 모두에 대한 사례를 만들어 보겠습니다.

#### The case for test data

Train data에 기반한 모델 오류 (추정치)는 쓰레기 (garbage) -> 변수 중요도는 모델 오류 (추정치)에 의존함 -> 학습 데이터에 기반한 변수 중요도는 쓰레기 (garbage)입니다. 모델을 학습시킨 데이터에서 모델 오차/성능을 측정하면 보통 측정값이 너무 낙관적/긍정적이기 때문에 실제보다 훨씬 더 잘 작동하는 것처럼 보입니다. PFI는 모델 오차의 측정값에 의존하기 때문에 unseen test data 를 사용해서 측정해야 합니다. Train data에 기반한 변수 중요도는 실제로는 모델이 그 데이터에 과적합한 것에 불과하고 전혀 중요하지 않은 피쳐인데도 중요하다고 판단할 수 있습니다.

#### The case for training data

Garbage SVM을 다시 살펴봅시다. Train data에 따르면 가장 중요한 피쳐는 `X42` 입니다. 피쳐 `X42`의 PDP를 보면 해당 피쳐의 변화에 따라 모델의 출력인 예측치가 어떻게 변하는지를 보여주며 일반화 오류에 의존하지 않습니다. 즉, PDP는 train 혹은 test data로 계산되었는지가 중요하지 않습니다.

![](Pasted%20image%2020240104182053.png) Figure 8.25 : Train data를 기반으로 한 변수 중요도에서 가장 중요하다고 나온 `X42`의 PDP. 이 플롯은 SVM이 `X42`에 의존하여 예측을 수행하는 방식을 보여줍니다.

이 그림은 SVM이 예측에 피쳐 `X42`에 의존하도록 학습되었지만 test data 기반의 변수 중요도를 보면 `X42`는 중요 변수가 아님을 보여줍니다. Train data에 기반한 중요도는 1.19입니다. Train data 기반 변수 중요도는 예측치 계산에 있어서 그 모델이 어떠한 피쳐에 의존한다는 의미에서 변수들의 중요도를 알려줍니다.

Train data를 사용하는 사례의 일환으로 어떠한 예시를 소개하겠습니다. 실제로는 모든 데이터를 사용해서 모델을 학습시키고 최종적으로 최상의(best) 모델을 얻고자 합니다. 즉, 피쳐 중요드를 계산하는 데 쓰지 않은 test data는 남아 있지 않아야 합니다. 모델의 일반화 오류를 추정할 때도 동일한 문제가 발생합니다. 피쳐 중요도 추정에 (nested) CV를 사용하면 모든 데이터가 포함된 최종 모델에서 피쳐 중요도가 계산되는 것이 아니라 각기 다르게 작동하는 데이터의 부분 집합에 대한 모델에서 변수 중요도가 계산됩니다.

하지만, PFI는 test data에서 사용하는 것이 좋습니다. 모델의 예측치가 피쳐의 영향을 얼마나 받는지에 관심이 있다면 SHAP importance와 같은 측정값을 사용해야 하기 때문입니다.


## 3. Example and Interpretation

여기서는 중요도 계산에 코드의 길이를 줄이기 위해 train data로 중요도를 계산했습니다.

#### Cervical cancer (classification)

자궁경부암 예측을 위해 Random Forest 모델을 적용합니다. 오차의 증감은 1-AUC로 측정합니다. 모델 오차가 1배 증가(=변화 없음)하는 피쳐들은 자궁경부암 예측에 중요하지 않은 것입니다 :

![](Pasted%20image%2020240104184404.png) Figure 8.26 : RF의 자궁경부암 예측에 대한 각 피쳐 중요도. 가장 중요한 피쳐는 호르몬 피임약 복용 년수입니다. 이 변수 값을 섞으면 1-AUC가 6.33배 증가합니다.

#### Bike sharing (regression)

회귀 문제로는 날씨 관련 변수들과 시간 관련 정보들을 이용해 SVM을 적합하여 자전거 대여 수 예측을 해결해 보겠습니다. 오차 측정값으로는 평균 절대 오차 (MAE)를 사용합니다 : 

![](Pasted%20image%2020240104185419.png) Figure 8.27 : 가장 주요 변수는 `temp`, 덜 중요한 변수는 `holiday`입니다.

## 4. Advantages

- **Nice interpretation** : 변수 중요도는 피쳐의 정보가 파괴될 때 모델 오류가 증가함을 의미합니다.
- 변수 중요도는 모델 동작에 대한 **매우 압축된 전반적인 인사이트 (highly compressed, global insight)**를 제공합니다.
- 오차의 차이값 대신 오류 비율 사용 시 장점은 **다른 여러 문제들과 변수 중요도 측정값을 비교할 수 있다**는 것입니다.
- 중요도 값은 다른 피쳐들과의 **모든 상호작용을 자동으로 고려하여** 계산됩니다. 피쳐들을 섞으면 다른 피쳐와의 상호작용 효과는 사라집니다. 즉, PFI는 모델 성능에 대한 주효과와 상효작용 효과를 모두 고려합니다. 이는 두 개의 피쳐 간 상호작용의 중요도가 두 피쳐의 중요도 측정에 포함되기 때문에 단점이기도 합니다. 즉, 변수 중요도가 총 성능 하락에 합산되지는 않지만 총합이 더 크다는 의미입니다. (선형 모델처럼) 피쳐 간 상호작용이 없는 경우에만 중요도가 대략적으로 합산됩니다. 
- PFI는 모델을 **재학습할 필요가 없습니다**. 다른 방법들은 특정 피쳐를 삭제하고 모델을 재학습한 다음 모델 오차를 비교해 중요도를 계산하는 경우가 많습니다. 이러한 재학습에는 시간이 오래 걸리므로, PFI 처럼 피쳐를 오직 섞기만 하면 많은 시간을 절약할 수 있습니다. 또한 데이터의 부분 집합으로 모델을 재학습하는 중요도 방식도 괜찮아보이지만, 줄어든 데이터를 이용한 모델은 변수 중요도에 있어서는 의미가 없습니다. 우리는 고정된 모델의 변수 중요도에 관심이 있는 것이기 때문입니다. 축소된 데이터셋으로 재학습하면 관심 모델과는 다른 모델이 생성됩니다. 

## 5. Disadvantages

- PFI는 **모델의 오류와 관련 있습니다**. 이는 본질적으로는 나쁜 점이 아니지만 불필요한 경우가 많습니다. 어떤 경우에는 성능에 대한 의미는 고려하지 않고 피쳐에 따라 모델 출력이 어떻게 달라지는지만 알고 싶을 수 있습니다. 예를 들어 누군가는 피쳐를 조작했을 때 모델 출력이 얼마나 견고한지(robust) 알고 싶을 수 있습니다. 이 경우 피쳐가 섞일 때 모델 성능이 얼마나 감소하는지가 아니라 각 피쳐가 모델의 출력(예측치)의 분산을 얼마나 설명하는지에 관심이 있는 경우입니다. 모델이 잘 일반화될 때 (즉, 과적합이 나지 않을 때), 모델 분산 (model variance) (피쳐로 설명됨)과 피쳐 중요도는 강한 상관관계가 있습니다. 
- True outcome $y$가 존재해야 이용할 수 있습니다. True $y$가 없을 시 PFI는 이용할 수 없습니다.
- PFI는 피쳐 값에 무작위성(randomness)를 추가하기 때문에 피쳐의 shuffle에 따라 달라집니다. 섞는 것을 반복하면 중요도 결과가 달라질 수 있습니다. Shuffling을 반복하고 이러한 반복에 따른 측정값들의 평균으로 중요도를 구하면 측정치는 안정화되지만, 계산 시간은 늘어납니다.
- 피쳐들이 상화 연관되어 있는 경우, PFI는 **비현실적인 인스턴스 몇 개에 의해 편향될 수 있습니다**. 이 문제는 PDP와 동일합니다 : 2개 이상의 피쳐들이 상관되었을 때 피쳐들을 섞다보면 비현실적인 인스턴스가 만들어집니다. 따라서 피쳐가 강한 상관관계에 놓여 있는지를 먼저 확인하고 해석에 주의를 해야 합니다. 
- 또 까다로운 점은 **상관관계에 놓인 피쳐를 추가하면 두 피쳐간 중요도가 나뉘어서 해당 피쳐들의 중요도가 낮아질 수 있다**는 점입니다. 피쳐 중요도를 '분할'하는 것의 예를 들어보겠습니다: 비가 올 확률을 예측하고 전날 오전 8시 기온을 상관관계가 없는 다른 피쳐들과 함께 하나의 피쳐로 사용하려 합니다. random forest를 학습한 결과 온도가 가장 중요한 피쳐이며 모든 것이 잘 맞았습니다. 이제 오전 8시와 강한 상관관계에 있는 오전 9시 온도를 추가 피쳐로 포함시켠 상황을 가정하겠습니다. 오전 8시의 온도를 이미 알고 있기 때문에 9시의 온도는 별다른 추가 정보를 제공하지 않습니다. Random Forest의 여러 트리 중 일부는 오전 8시의 온도를, 다른 트리는 9시의 온도를, 또 다른 트린느 두 가지 온도를... 포착합니다. 두 개의 온도 피쳐가 합쳐지면 중요도의 총합은 기존보다 조금 더 높아지지만, 각각은 가장 중요한 피쳐로 선정되지 않고 중간 정도의 중요도를 갖습니다. 상관관계에 놓인 피쳐를 추가함으로써 가장 중요한 피쳐를 평번한 피쳐로 끌어내리게 된 것입니다. 이렇게 함으로써 변수 중요도에 대한 해석이 더 어려워집니다. 또 다른 예를 들어보겠습니다: 피쳐에 측정 오류가 있는지 확인하려 한다고 해보겠습니다. 이 점검은 비용이 많이 들기 때문에 가장 중요한 3개의 피쳐만 점검하기로 했습니다. 첫 번째 시나리오는 온도를 확인하는 것이고, 두 번째는 온도 피쳐가 중요도를 공유/분할한다는 이유로 포함하지 않는 것입니다. 중요도 값이 모델 동작 수준에서는 의미가 있을 순 있지만, 상호 연관된 피쳐가 있는 경우에는 혼란을 야기합니다. 

## 6. Alternatives

[`PIMP`](https://academic.oup.com/bioinformatics/article/26/10/1340/193348?login=false)라는 알고리즘은 PFI 알고리즘을 조정하여 중요도에 대한 p-values를 제공합니다. 또 다른 손실 기반 (loss-based) 대안은 train data에서 피쳐를 생략하고 모델을 재학습한 후 손실의 증가를 측정하는 것입니다. 피쳐 값을 섞고 손실의 증가를 측정하는 것이 변수 중요성의 유일한 측정법이 아닙니다. 다양한 중요도 측정 방법은 모델별 (model-specific) 방법과 model-agnostic 방법으로 나뉩니다. Random Forest에 대한 Gini index 또는 회귀 모델에 대한 standardized coefficients는 model-specific 측정의 예입니다.

PFI에 대한 model-agnostic 방법의 대안은 분산 기반 (variance-based) 측정법입니다. Variance-based feature importance (예: Sobol's indices, functional ANOVA)은 예측 함수에서 높은 분산을 유발하는 피쳐에 더 높은 중요도를 부여합니다. SHAP이 이러한 분산 기반 중요도와 유사합니다. 피쳐를 변경하면 output이 크게 변하는 경우 해당 피쳐의 중요도가 높아집니다. 이러한 중요도의 정의는 PFI처럼 loss-based 정의와는 다릅니다. 이는 모델이 과적합된 경우에 분명하게/명백하게 나타납니다. 모델이 과적합되어 output과 관련이 없는 피쳐를 사용할 경우, PFI는 이 피쳐가 올바른 예측치 생성에 기여하지 않기 때문에 0의 중요성을 할당합니다. 반면에 variance-based 중요도 측정법은 피쳐값이 변할 때 예측치가 많이 바뀔 수 있으므로 이러한 피쳐에 높은 중요도를 할당할 수 있습니다. 

다양한 중요도 측정 기법은 Wei (2015)의 논문에서 다루고 있습니다.

## 7. Software

위 예제들에서는 `iml` 패키지가 쓰였습니다. R 에서는 `DALEX`와 `vip` 패키지에서 이러한 기능을 제공하며, Pyhon에서는 `alibi`, `scikit-learn`, `rfpimp`에서 이러한 model-agnostic한 PFI를 구현합니다. 

---

# 📕08-06. Global Surrogate

Global surrogate model은 black box 모델의 예측치를 근사하도록 학습된 interpretable model입니다. Surrogate model을 해석하여 black box 모델에 대한 결론을 냅니다. 

## 1. Theory

Interpretable ML에서 말하는 surrogate model은 기본 black box model이 ML 모델이고, 이 surrogate model이 해석 가능해야 합니다. (Interpretable) surrogate model의 목적은 기본 (black box) 모델의 예측치를 가능한 한 정확히 근사하면서 해석을 제공하는 것입니다. 이 surrogate model은 다양한 이름으로 불립니다 : Approximation model, metamodel, response surface model, emulator, ...

사실 surrogate model의 이해에 필요한 이론은 그리 많지 않습니다. 우리는 black box 예측함수 $f$ 를 surrogate model의 예측 함수 $g$ 를 이용해 최대한 가깝게 근사시키고자 하는데, 이 때 $g$ 는 해석이 가능해야 한다는 제약이 있습니다. 함수 $g$ 로 [CH05. Interpretable Methods](CH05.%20Interpretable%20Methods.md) 에서 다루는 해석 가능한 모델들을 사용할 수 있습니다.

For example a linear model : 
$$
g(x) = \beta_0+\beta_1x_1+\ldots+\beta_px_p
$$
Or a decision tree : 
$$
g(x) = \sum_{m=1}^Mc_mI\{x\in R_m\}
$$
Surrogate model을 학습하는 것은 black box 모델의 내부 작동에 대한 정보가 필요하지 않고 데이터와 예측 함수에 대한 접근성/액세스(access)만 필요하기 때문에 이 방법은 model-agnostic입니다. 기본 ML 모델이 다른 모델로 교체된 경우에도 여전히 surrogate model을 사용할 수 있습니다. Black box 모델 유형과 surrogate model 유형의 선택은 분리되어 있습니다.

아래의 단계들을 수행하여 surrogate model을 얻을 수 있습니다 : 
1. 데이터셋 $X$ 를 선택합니다. Black box 모델을 훈련할 때 사용한 데이터셋, 혹은 그와 동일한 분포의 데이터셋이어야 합니다. 경우에 따라 데이터의 부분 집합 or 포인트 grid 일 수 있습니다. 
2. 선택한 데이터셋 $X$에 대해 black box model의 예측치를 가져옵니다. 
3. Interpretable model type (linear mode, decision tree, ...)을 선택합니다.
4. 데이터셋 $X$와 그 예측치에 대하여 선택한 interpretable model을 학습합니다.
5. 이제 surrogate model이 생겼습니다.
6. 이제 이 surrogate model이 black box model의 예측치를 얼마나 잘 근사/복제하는지를 측정합니다. 
7. 그런 다음 surrogate model을 해석합니다.

Surrogate model 에 대한 접근법들에는 몇몇 추가적인 단계가 있거나 다를 수 있지만 보통은 이러한 단계를 거칩니다.

Surrogate model이 black box model을 얼마나 잘 근사/복제하는지 측정하는 한 가지 방법은 R-squared (결정게수)입니다 : 
$$
R^2=1-\frac{S S E}{S S T}=1-\frac{\sum_{i=1}^n\left(\hat{y}_*^{(i)}-\hat{y}^{(i)}\right)^2}{\sum_{i=1}^n\left(\hat{y}^{(i)}-\overline{\hat{y}}\right)^2}
$$
여기서 $\hat{y}_*^{(i)}$는 $i$ 번째 인스턴스에 대한 surrogate model의 예측치이고, $\hat{y}^{(i)}$는 $i$ 번째 인스턴스에 대한 black box model의 예측치, $\bar{\hat{y}}$는 black box model 예측치들의 평균값입니다. SSE는 오차제곱합, SST는 총 제곱합을 나타냅니다. $R^2$가 1에 가까우면 (=SSE가 낮으면) interpretable model이 black box molde의 동작을 매우 잘 근사한다는 것입니다. 이렇게 매우 근접한 경우 복잡한 모델을 interpretable model로 대체할 수 있습니다. $R^2$가 0에 가까우면 (=SSE가 높으면) 반대의 상황인 것입니다. 

여기서 기저에 깔린 black box model의 성능에 대해서는 다루지 않습니다. Black box model의 성능은 surrogate model을 학습하는 데 영향을 미치지 않기 때문입니다. Surrogate model의 해석은 실제가 아닌 모델의 진술을 위한 것이기 때문에 여전히 유효합니다. 그러나 물론 black box model이 잘못된 것이면 그 자체가 무의미해지기 때문에 이에 따른 surrogate model의 해석 또한 쓸모가 없어지는 것은 당연합니다. 

원본 데이터의 부분 집합을 기반으로 surrogate model을 만들거나 인스턴스들의 가중치를 재조정하여 만들 수도 있습니다. 이런 식으로 surrogate model 의 input의 분포를 변경하면 해석의 초점이 바뀝니다 (그러면 더 이상 전역적/global이지 않게 됩니다). 특정 인스턴스별로 locally 가중치를 부여하면 (선택한 인스턴스에 가까울수록 가중치가 높아짐), 인스턴스 개별 예측을 설명할 수 잇는 local surrogate model을 얻는 것입니다. 다음 섹션에서 이러한 local model에 대해 알아볼 것입니다. 

## 2. Example

우선 날씨와 날짜 정보가 주어졌을 때 일일 자전거 대여 수 예측을 위해 SVM을 학습합니다. SVM은 해석이 어렵기 때문에 이 모델의 동작을 근사화하기 위해 interpretable 한 CART를 사용해 surrogate model을 학습하겠습니다 : 

![](Pasted%20image%2020240104200527.png) Figure 8.28 : 자전거 데이터로 학습된 SVM의 예측치를 근사화하는 surrogate tree의 터미널 노드입니다. 노드의 분포를 보면 surrogate tree는 `temp`가 약 13도 이상일 때와 2년간의 일 수 가 435일보다 클 때 대여 자전거 수가 더 많을 것으로 예측하고 있습니다.

Surrogate model의 $R^2$ (variance explained) 값은 0.77로, 기저에 깔린 black box model의 동작을 상당히 잘 추정하지만 완벽하진 않습니다. 적합도가 완벽하다면 SVM을 버리고 이 트리를 사용할 수 있습니다. 

두 번째 예시로는 random forest를 사용하여 자궁경부암 발생 확률을 예측하는 것입니다. 여기서도 CART를 surrogate model로 삼지만, 이 모델이 사용하는 output은 random forest의 예측치입니다 :
![](Pasted%20image%2020240104202024.png) Figure 8.29 : 자궁경부암 데이터에 대해 학습한 RF의 예측치를 근사하는 surrogate tree의 터미널 노드입니다. 노드 내 counts는 해당 노드에서 black box model의 분류 빈도를 나타냅니다.

Surrogate model의 $R^2$ (variance explained)는 0.19로, RF에 제대로 근사하지 않으므로 이 트리를 과도하게 해석하면 안됩니다.

## 3. Advantages

- Surrogate model method는 **유연(flexible)** 합니다 : [CH05. Interpretable Methods](CH05.%20Interpretable%20Methods.md)의 모든 모델을 사용할 수 있습니다. 이는 interpretable model 뿐 아니라 black box model도 교환이 가능하다는 얘기입니다. 복잡한 모델을 만들어서 회사 팀원들에게 설명하는 상황이라고 하겠습니다. 팀마다 익숙한 모델이 다를 수 있습니다. 원래의 black box model에 대해 각 팀에 익숙한 surrogate models를 학습시키고 2가지 종류의 설명(explanations)를 제공할 수 있습니다. 
- 이 방법은 **매우 직관적이고 간단합니다** : 즉, 구현이 쉽고 ML에 익숙하지 않은 사람들에게도 쉽게 설명할 수 있습니다. 
- $R^2$을 사용하면 surrogate model이 black box model에 얼마나 근접한지를 쉽게 측정할 수 있습니다.

## 4. Disadvantages

- Surrogate model은 실제 outcome을 볼 수 없기 때문에** 데이터가 아닌 모델 (black box)에 대한 결론**을 도출한다는 점을 유의해야 합니다. 
- Surrogate model이 black box model에 충분히 가깝다고 확신할만한 **$R^2$의 최적의 cutoff 값이 무엇인지 명확하지 않습니다**.
- 어떠한 interpretable surrogate model은 데이터셋의 한 부분집합에 대해서는 black box model을 잘 근사하지만 다른 부분집합에 대해서는 크게 다를 수 있습니다. 
- Surrogate model로 선택하는 interpretable model에는 모두 장단점이 있습니다.
- 몇몇 사람은 본질적으로 해석 가능한 모델은 없으며 해석 가능성에 대한 환상을 갖는 것조차 위험하다고 주장합니다.

---

# 📕 08-07. Prototypes and Criticisms

**Prototype**은 모든 데이터를 대표하는 인스턴스를 말합니다. **Criticism**은 prototypes 집합으로 대표되지 않는 데이터 인스턴스들입니다. 이러한 criticism의 목적은 prototypes가 잘 나타내지 못하는 데이터 요소에 대한 인사이트를 찾는 것에 있습니다. Prototypes와 criticisms는 데이터를 설명하기 위해 ML 모델과 독립적으로 쓸 수 있지만, interpretable model을 만들거나 black box model을 해석 가능하게 만들 때도 사용할 수 있습니다. 

이 섹션에서는 단일 인스턴스를 가리키는 '데이터 포인트'라는 표현을 사용하는데, 이는 각 피쳐가 차원인 좌표계의 한 점으로 인스턴스가 존재함을 해석하기를 강조하기 위해서입니다. 아래 그림은 일부 인스턴스들을 prototypes로 일부는 criticisms로 선택한 (시뮬레이션) 데이터 분포를 보여줍니다. 작은 점은 데이터, 큰 점은 criticisms, 큰 네모는 prototypes입니다. Prototypes는데이터 분포의 중심들을 포함하도록 (수동으로) 선택하며, criticisms는 prototypes가 없는 집단 (clusters)의 포인트들입니다. 
![](Pasted%20image%2020240104204141.png) Figure 8.30 : Prototypes and criticisms for a data distribution with two features x1 and x2.

Prototypes를 수동으로 선택하면 결과가 좋지 않을 수 있습니다. 데이터에서 prototypes를 찾는 방법을 몇 가지 소개하겠습니다. K-means 알고리즘과 관련된 clustering 알고리즘인 K-medoids입니다. 데이터 포인트들을 클러스터 중심으로 반환하는 모든 클러스터링 알고리즘은 prototypes를 선택하는 데 적절합니다. 그러나 이 방법 대부분은 prototypes만을 찾을 뿐 criticisms는 제공하지 않습니다. 이 섹션에서는 prototypes와 criticisms을 하나의 프레임워크게 결합한 접근법은 Kim et al. (2016)의 MMD-critic을 소개하겠습니다.

MMD-critic은 데이터의 분포와 선택된 prototypes의 분포를 비교합니다. 이것이 핵심 개념입니다. MMD-critic은 두 분포 간 불일치를 최소화하는 prototypes를 선택합니다. 특히 서로 다른 군집/클러스터에서 포인트를 선택하는 경우 밀도가 높은 영역의 데이터 포인트가 좋은 prototypes가 됩니다. Prototypes로 잘 설명되지 않는 영역의 데이터 요소는 criticisms의 대상으로 선택됩니다. 

이론에 대해 살펴보겠습니다.

## 1. Theory

MMD-critic의 절차는 아래와 같이 간략하게 요약할 수 있습니다 :
1. 찾고자 하는 prototypes와 criticisms의 개수를 고릅니다.
2. Greedy search로 prototypes를 찾습니다. Prototypes의 분포가 데이터 분포에 가까워지도록 prototypes를 선택합니다. 
3. Greedy search로 criticisms을 찾으세요. Prototypes의 분포가 데이터의 분포와 다른 포인트들을 criticisms으로 선택합니다. 

MMD-critic으로 데이터셋에 대한 prototypes와 criticisms을 찾기 위해선 몇 가지 필요한 것이 있습니다. 가장 기본적인 요소는 데이터의 밀도를 추정하기 위한 **커널 함수(kernel function)** 입니다. 커널 함수는 두 데이터 포인트의 근접성(proximity)에 따라 가중치를 부여하는 함수입니다. 밀도 추정을 기반으로 선택한 prototypes의 분포가 데이터 분포에 가까운지 여부를 알려면 두 분포의 차이를 알려주는 측정법이 필요합니다. 이는 **최대 평균 불일치 (maximum mean discrepancy; MMD)** 를 사용해 측정합니다. 또한 커널 함수를 기반으로 특정한 데이터 포인트에서 두 분포가 얼마나 다른지 알려주는 **witness function** 이 필요합니다. Witness function을 사용하면 criticisms, 즉 prototypes와 데이터의 분포가 구분되고 witness function가 절대값이 큰 데이터 포인트들을 선택할 수 있습니다. 마지막 요소는 좋은 prototypes와 criticisms에 대한 검색 전략인데, 이는 간단히 **greedy search**로 해결합니다.

우선 두 분포 간 불일치를 측정하는 **MMD** 를 보겠습니다. Prototypes을 선택하면 prototypes의 밀도 분포가 생성됩니다. 이제 prototypes의 분포가 데이터 분포와 다른지 평가하려 합니다. 커널 밀도 함수를 사용하여 이 두 가지 분포를 모두 추정합니다. MMD는 두 분포 간 차이를 측정하며, 이는 두 분포에 따른 기대값들의 차이값의 함수 공간에 대한 최댓값입니다. 잘 이해가 되지 않을 수 있습니다. 아래 공식은 MMD 측정값의 제곱 ($MMD^2$)을 계산하는 방법입니다 : 
$$
M M D^2=\frac{1}{m^2} \sum_{i, j=1}^m k\left(z_i, z_j\right)-\frac{2}{m n} \sum_{i, j=1}^{m, n} k\left(z_i, x_j\right)+\frac{1}{n^2} \sum_{i, j=1}^n k\left(x_i, x_j\right)
$$
여기서 $k$는 두 개의 포인트의 유사도를 측정하는 커널 함수인데, 이는 추후에 다루겠습니다. $m$은 prototypes $z$의 개수고, $n$ 은 원본 데이터셋의 데이터 포인트 $x$의 수입니다. Prototypes $z$는 데이터 포인트 $x$ 중 선택하는 것입니다. 각 데이터 포인트는 다차원이므로 여러 피쳐를 갖습니다. MMD-critic의 목적은 이 $MMD^2$를 최소화하는 것입니다. 이 값이 0에 가까울수록 prototypes의 분포가 데이터 분포에 가깝다는 것입니다. 이를 0으로 낮추기 위한 핵심은 중간에 있는 term으로, prototypes과 다른 모든 데이터 포인트 사이의 평균 근접도(average proximity)의 2배를 계산합니다. 이 term이 첫 번째 term (prototypes 서로에 대한 평균 근접도)과 마지막 term (데이터 포인트 서로에 대한 평균 근접도)을 더하면 값과 같으면 prototypes이 데이터를 완벽히 나타내는/설명하는 것입니다. 

아래 그림은 $MMD^2$ 측정값을 나타냅니다. 첫 번째 그림은 2가지 피쳐를 갖는 데이터 포인트를 나타내며, 데이터 밀도에 대한 추정치가 음영 처리된 배경으로 표시됩니다. 나머지 그림들은 각각의 MMD2 값과 함께 다양한 prototypes selection을 보여줍니다. prototypes는 큰 점으로 표시되고, 그것의 분포는 등고선으로 표시합니다. 이 시나리오에서 원본 데이터를 가장 잘 커버/대표하는 prototypes selection은 왼쪽 아래 그림으로 MMD2 값이 가장 낮습니다 : 

![](Pasted%20image%2020240104212657.png) Figure 8.31 : The squared maximum mean discrepancy measure (MMD2) for a dataset with two features and different selections of prototypes.

커널은 radial basis function; rbf kernel을 사용했습니다 : 
$$
k\left(x, x^{\prime}\right)=\exp \left(-\gamma\left\|x-x^{\prime}\right\|^2\right)
$$
여기서 $||x-x^\prime||^2$은 두 개의 포인트 간 Euclidean 거리이고, $\gamma$는 척도 모수입니다. 커널값은 두 점 사이의 거리에 따라 감소하며 0~1의 범위입니다. 

Prototypes를 찾기 위한 알고리즘에서 MMD2 값, 커널, greedy search를 결합합니다 : 
- 빈 prototypes 목록으로 시작합니다
- Prototypes의 수가 선택된 수 $m$ 보다 적을 때까지 기다립니다 : 
	- 데이터셋의 각 포인트에 대해 해당 포인트를 prototypes 목록에 추가할 때 $MMD^2$ 가 얼마나 감소하는지 확인합니다. $MMD^2$를 최소화하는 데이터 포인트를 목록에 추가합니다. 
- 목록을 반환합니다.

Criticisms를 찾기 위한 남은 요소는 특정 포인트에서 두 밀도 추정치가 얼마나 다른지를 알려주는 witness function입니다. 이 함수는 아래와 같이 추정합니다 : 
$$
\text { witness }(x)=\frac{1}{n} \sum_{i=1}^n k\left(x, x_i\right)-\frac{1}{m} \sum_{j=1}^m k\left(x, z_j\right)
$$
동일한 피쳐들을 갖는 두 개의 데이터셋이 있을 때 이러한 witness function은 포인트 $x$가 어떤 empirical distribution에 더 잘 맞는지 평가할 수 있습니다. Criticisms를 찾기 위해 음의 방향과 양의 방향 모두에서 witness function의 극단값(extreme values)을 찾습니다. Witness function의 첫 번째 항은 점 $x$와 데이터 포인트 간 평균 근접성이고, 두 번째 항은 점 $x$와 prototypes 간 평균 근접성입니다. 점 $x$에 대한 witness function이 0에 가까우면 데이터와 prototypes의 밀도함수가 서로 가깝다는 것이며, 이는 둘의 분포가 서로 유사함을 의미합니다. 이와 다르게, $x$에서의 witness function이 음수면 prototypes의 분포가 데이터 분포를 과대 평가하는 것이고, 양수면 과소평가한다는 것입니다. 

좀 더 직관적인 이해를 위해, 앞선 그림의 prototypes을 가장 낮은 $MMD^2$로 재사용하고 수동으로 선택한 포인트들에 대한 witness function을 표시해 보겠습니다. 아래 그림에서 삼각형과 레이블은 수동으로 선택한 포인트들에 대한 witness function 값입니다. 가운데 있는 점만 절대값이 높기 때문에 criticisms의 좋은 후보가 됩니다. 

![](Pasted%20image%2020240104214206.png) Figure 8.32 Evaluations of the witness function at different points.

Witness function을 사용하면 prototypes로 잘 대표되지 않는 인스턴스들을 명시적으로 찾을 수 있습니다. Criticisms는 witness function에서 절대값이 높은 포인트들입니다. Prototypes와 마찬가지로 criticisms도 greedy search로 찾습니다. 하지만 전체 $MMD^2$를 줄이는 대신 witness function과 정규화 항(regularizer term)을 포함하는 cost function을 최대화하는 점을 찾습니다. 최적화 함수에 대한 추가적인 term은 포인트들에게 다양성을 주는데, 이는 포인트가 서로 다른 클러스터에서 나오도록 하기 위함입니다.

이 두 번째 스텝은 prototypes 찾는 것과는 무관합니다. 몇 가지 prototypes를 골라서 이 절차를 사용해 criticisms를 학습할 수도 있습니다. 또는 K-medoids 같은 클러스터링을 통해 prototypes을 얻을 수도 있습니다. 

한 가지 의문이 남습니다 : **Interpretable ML에서 MMD-critic은 어떻게 쓰일까요?**

MMD-critic은 3가지 방법으로 사용됩니다 : 1) 데이터 분포를 더 잘 이해하도록 돕거나, 2) interpretable model을 구축하거나, 3) black box model을 해석 가능하게 만드는 것입니다. 

Prototypes와 criticisms를 찾는 데에 MMD-critic을 적용하면, 특히 edge cases가 있는 복잡한 데이터 분포의 경우 데이터에 대한 이해도를 높일 수 있습니다. 뿐만 아니라 더 많은 것을 얻을 수 있습니다.

예를 들어, 해석 가능한 예측 모델을 만들 수 있습니다 : 이를 'nearest prototype model'이라고 부릅니다. 예측함수는 아래와 같이 정의합니다 : 
$$
\hat{f}(x) = \operatorname{argmax}_{i\in S}k(x, x_i)
$$
즉, 커널 함수의 가장 높은 값을 산출한다는 의미에서 prototypes $S$ 집합에서 새로운 데이터 포인트에 가장 가까운 prototype $i$를 선택합니다. Prototype 자체는 예측치에 대한 설명으로 반환됩니다. 이 절차에는 3가지 tuning parameters가 있습니다 : 커널 유형 (type of kernel), 커널 척도 모수 (kernel scaling parameter), prototypes의 수 입니다. 모든 parameters는 CV loop 내에서 최적화할 수 있습니다. 이 방식에는 criticisms가 사용되지 않습니다. 

3번째로, 모델 예측치와 함께 prototypes와 criticisms을 검토하여 모든 ML 모델을 global하게/ 전역적으로 설명 가능하게 만드는 데에 MMD-critic을 쓸 수 있습니다. 절차는 아래와 같습니다 : 
1. MMD-critic 으로 prototypes와 criticisms을 찾습니다. 
2. 평소대로 ML 모델을 학습합니다. 
3. ML 모델로 prototypes와 criticisms에 대한 outcomes을 예측합니다. 
4. 이제 결과를 분석합니다: 어떤 경우에 알고리즘이 잘못되었는지 파악할 수 있습니다. 데이터를 잘 표현하고 ML 모델의 약점을 찾는 데 도움이 되는 점을을 얻을 수 있습니다.

어떻게 도움이 될까요? 모델의 성능만을 확인하는 것은 충분하지 않습니다. 모델이 99% 정확하다면 문제점은 여전히 1%가 남아있습니다. 그리고 라벨도 틀릴 수 있습니다. 모든 train data들을 검토하고 예측에 문제가 있는지 검사하는 것은 문제를 발견할 순 있지만 실현이 불가능합니다. 하지만 수천 개의 prototypes와 criticisms를 선별하는 것은 가능하기 때문에 데이터의 문제를 발견할 수 있습니다. 

## 2. Examples

손을 쓴 숫자 데이터셋을 사용해 MMD-critic의 예시를 다루겠습니다. 

아래 prototypes를 보면 숫자마다 이미지의 수가 다름을 알 수 있습니다. 이는 클래스별로 고정된 개수의 prototypes가 아니라 전체 데이터셋에서 고정된 개수의 prototypes을 찾았기 때문입니다. 예상대로 prototypes는 숫자를 쓰는 방식이 서로 다릅니다. 

![](Pasted%20image%2020240104220511.png) Figure 8.33 : Prototypes for a handwritten digits dataset.

## 3. Advantages

MMD-critic의 저자들은 사용자 연구의 참가자들에게 이미지를 제공했고, 참가자들은 2가지 클래스 (예: 2가지 개 품종) 중 하나를 나타내는 2가지 이미지 중 하나를 시각적으로 일치시켜야 했습니다. 그들은 클래스의 무작위 이미지 대신 **prototypes과 criticisms이 제시되었을 때 가장 좋은 성적을 거두었습니다**.

Prototypes과 criticisms의 개수는 자유롭게 선택할 수 있습니다. 

MMD-critic은 데이터의 밀도 추정치와 함께 작동합니다. 이는 모든 유형의 데이터의 모든 유형의 ML 모델에서 작동합니다. 

알고리즘의 구현이 쉽습니다. 

MMD-critic은 해석 가능성을 높이기 위해 사용되는 방식이 매우 유연합니다. 복잡한 데이터 분포를 이해하는 데 사용할 수 있습니다. 해석 가능한 ML 모델 구축에 사용 가능합니다. 또는 black box model의 의사결정에 도움을 줄 수 있습니다.

Criticisims를 찾는 것은 prototypes의 선별과는 별개입니다. 그러나 이들 모두 prototypes와 데이터의 밀도를 비교하는 동일한 방법으로 생성되기 때문에 MMD-critic에 따라 prototypes을 선택하는 것이 합리적입니다. 


## 4. Disadvantages

수학적으로 prototypes와 criticisms은 서로 다르게 정의되지만, 그 구분은 cutoff 값 (prototypes의 수)을 기준으로 합니다. 데이터 분포를 커버/대표하기 위해 너무 적은 수의 prototypes을 선택했다고 해보겠습니다. Criticisms는 결국 잘 설명되지 않은 영역에 집중될 것입니다. 하지만 prototypes를 더 추가하면 같은 영역에 대한 criticisms이 나타날 것입니다. 어떤 해석을 하든 criticisms는 기존 Prototypes과 그것에 수에 대한 (임의의) cutoff 값에 크게 의존합니다.

Prototypes과 criticisms의 수를 선택해야 합니다. 이는 장점이자 단점입니다. 적절한 prototypes 수에 대한 해결책이 될 수 있는 방법은 prototypes 수를 증가시키면서 MMD2 curve를 측정하여 scree plot method로 결정하는 것입니다. 

Kenel 관련 hyperparameter의 선택이 필요합니다. 

일부 피쳐가 target과 관련이 없을 수 있다는 것을 무시하고 모든 피쳐를 input으로 사용합니다. 한 가지 해결책은 raw pixels 대신 embedding과 같이 관련성 있는 피쳐들만 사용하는 것입니다. 

사용 가능한 코드가 몇 가지 있지만, 패키지화 되지 않았습니다.

---

