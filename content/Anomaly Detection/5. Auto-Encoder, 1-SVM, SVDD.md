---
sticker: emoji//1f4d5
tags:
  - Anomaly_Detection
  - ì´ìƒíƒì§€
  - Auto-encoder
  - 1-SVM
  - SVDD
---
# ğŸ² Auto-Encoder, 1-SVM, SVDD

1-SVMê³¼ SVDDëŠ” íŠ¹ì •í•œ ì¡°ê±´ì´ ë§Œì¡±í•œë‹¤ëŠ” ê°€ì •í•˜ì—ì„œëŠ” ì†”ë£¨ì…˜ì´ ë˜‘ê°™ì€ë°, ì ‘ê·¼ ë°©ë²•ì´ ì•½ê°„ ë‹¤ë¥¸ ë°©ë²•ë¡ ì´ë‹¤. 

## ğŸ¯Auto-Encoder Anomaly Detection

ë¨¼ì € Auto encoderë¥¼ í†µí•œ anomaly detection ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì. 

#### Auto Encoder : Auto-Associative Neural Network

ì—¬ê¸°ì„œëŠ” Feed-forward NNì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ì§€ë§Œ, ì´ë¯¸ì§€ì— ëŒ€í•œ ì´ìƒ íƒì§€ë¥¼ í•  ê²½ìš°ì—ëŠ” Convolution auto encoderë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆê³ , ë˜ëŠ” sequenceë¥¼ ì´ìš©í•´ì„œ ì‹œê³„ì—´ ë°ì´í„°ë‚˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì´ìƒ íƒì§€ì—ì„œëŠ” RNN ê¸°ë°˜ì˜ auto encoderë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 

ì´ ê²½ìš° ëª¨ë‘ ì ìš©ë˜ëŠ” ì›ì¹™ì´ ìˆëŠ”ë°, NNì„ í•™ìŠµí•  ë•Œ inputì„ outputìœ¼ë¡œ ìµœëŒ€í•œ ê°€ê¹ê²Œ reproduction í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. ì´ ë•Œ ì‚¬ìš©ë˜ëŠ” ëª©ì í•¨ìˆ˜ë¥¼ ì‚´í´ë³´ì.

- FFNN trained to <font style="color:skyblue">reproduce</font> its input at the output layer
	- Loss fuunction : $$l\left(f(\mathbf{x})\right) = \frac{1}{2}\sum_{k}(\hat{x}_k-x_k)^2$$
	- ì—¬ê¸°ì„œ $x_k$ ëŠ” input, $\hat{x}_k$ ëŠ” reproductionì´ë‹¤. 

![[Pasted image 20240119182947.png|center]]

ê°€ì¥ ê¸°ë³¸ì ì¸ FFNNì„ ê¸°ì¤€ìœ¼ë¡œ ì‚´í´ë³´ì. Input data $\mathbf{x}$ ê°€ ë“¤ì–´ì™”ì„ ë•Œ ì´ d-ì°¨ì› ë°ì´í„°ê°€ weight $\mathbf{W}$ ë¥¼ í†µí•´ì„œ hidden vector $\mathbf{h}(\mathbf{x})$ ë¡œ ë³€í™˜ì´ ëœë‹¤. ì´ ê³¼ì •ì„ ì •ë³´ë¥¼ ì¶•ì•½í•˜ëŠ” <font style="color:skyblue">Encoding</font> ê³¼ì •ì´ë¼ê³  í•˜ê³  Encoderë¼ê³  ë¶€ë¥¸ë‹¤. í•µì‹¬ì€ $h<d$, ì¦‰ ë°˜ë“œì‹œ Encoding ê³¼ì •ì—ì„œ <font style="color:skyblue">ì •ë³´ì˜ ì¶•ì•½</font> ì´ ì¼ì–´ë‚˜ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ì••ì¶•ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í¼ì¹˜ëŠ” ê²ƒì„ <font style="color:skyblue">Decoding</font> ê³¼ì •ì´ë¼ í•˜ê³  ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë¶€ë¶„ì„ Decoderë¼ê³  ë¶€ë¥¸ë‹¤. ì •ë³´ë¥¼ encodingí•˜ê³  decodingí•˜ëŠ” ê³¼ì •ì—ì„œ ì •ë³´ì˜ ì¶•ì•½ì´ ë°œìƒí–ˆê¸° ë•Œë¬¸ì— ì›ë˜ì˜ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ì˜ ë³´ì¡´í•  ìˆ˜ ìˆëŠ” ê²ƒì„ ì ì¬ ë²¡í„° (ì—¬ê¸°ì„œëŠ” hidden layerì˜ nodes ê°’) latent vectorë¼ê³  ë¶€ë¥¸ë‹¤. ì´ latent vectorëŠ” ì •ìƒ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì¶©ë¶„íˆ í•™ìŠµí–ˆì„ ê²ƒì´ë¯€ë¡œ ì •ìƒ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ reproductionì„ ì˜í•  ê²ƒì´ê³ , ì´ìƒ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ê²Œ ë˜ë©´ reproductionì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì•„ loss ê°’ì´ ì»¤ì§€ê²Œ ë  ê²ƒì´ë‹¤. 

ë”°ë¼ì„œ ì´ loss ê°’ì„ anomaly scoreë¡œ ì‚¬ìš©í•œë‹¤. 

- Overcomplete and Undercomplete hidden layers for AE

![[Pasted image 20240119183647.png|center]]

Hidden layerë¡œ ë¶ˆë¦¬ëŠ” bottleneck layerê°€ ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼ í•˜ëŠ”ë°, ì´ bottleneck layerëŠ” inputì˜ ì •ë³´ëŸ‰ë³´ë‹¤ ì ì–´ì•¼ í•œë‹¤ëŠ” íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤. ì¦‰ ë°˜ë“œì‹œ ì •ë³´ì˜ ì¶•ì•½ì´ ì¼ì–´ë‚˜ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. 

- Example 

![[Pasted image 20240119184223.png|center]]

Inputì´ $32\times32$ ë¼ë©´, $\mathbf{x}\in \mathbb{R}^{32^2}$ì´ ë˜ê³ , hidden layer $\mathbf{h}\in \mathbb{R}^h$ì´ ë˜ëŠ”ë° ì´ë•Œ ì •ë³´ì˜ ì¶•ì•½ì„ ìœ„í•´ $h < 32^2$ ì´ì–´ì•¼ í•œë‹¤. Auto encoderê°€ ì˜ ì‘ë™í•œë‹¤ëŠ” ê°€ì •ì—ì„œ, embedding/latent vector $h$ ëŠ” ì›ë˜ì˜ input ë³´ë‹¤ ì •ë³´ê°€ ì¶•ì•½ë˜ì–´ ìˆìœ¼ë‹ˆê¹Œ ì´ë¥¼ **ì°¨ì› ì¶•ì†Œ ê¸°ë²•**ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. í•˜ì§€ë§Œ ì´ìƒ íƒì§€ì—ì„œëŠ” decoding ê³¼ì •ì„ ì¶”ê°€ë¡œ ìˆ˜í–‰í•œë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œëŠ” '2'ë¼ëŠ” ìˆ«ìë¥¼ ì˜ reproduction í•˜ëŠ” ìƒí™©ì´ë‹¤. ë§Œì•½ ëœ¬ê¸ˆì—†ì´ '4'ë¼ëŠ” ìˆ«ìê°€ reproduct ëœë‹¤ë©´ ê·¸ ë°ì´í„°ì˜ loss ì¸ $(\mathbf{x}-\mathbf{\hat{x}})^2$ ê°€ êµ‰ì¥íˆ ì»¤ì ¸ì„œ í° anomaly scoreë¥¼ ê°–ê²Œëœë‹¤. 

## ğŸ¯ Support Vector-based Novelty Detection

ì•ì„œ ë°€ë„ ê¸°ë°˜ì˜ ë°©ë²•ë¡ ë“¤(LOF, Parzen window, KNN ë“±)ì€ íŠ¹ì •í•œ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ê·¸ ê°ì²´ê°€ ì •ìƒ ë²”ì£¼ì¼ í™•ë¥ ì„ ë‚´ë±‰ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ë°, SV-based ë°©ë²•ë¡ ì€ ê·¸ê²ƒê³¼ëŠ” ë‹¤ë¥´ê²Œ ì–´ë– í•œ í•¨ìˆ˜ë¥¼ ì°¾ëŠ”ë‹¤. ê·¸ í•¨ìˆ˜ëŠ” ì •ìƒê³¼ ë¹„ì •ìƒì„ êµ¬ë¶„í•˜ëŠ” ê²½ê³„ë©´ì„ ì°¾ëŠ” í•¨ìˆ˜ë‹¤. 

#### Support vector-based novelty detection

- ì •ìƒê³¼ ì´ìƒ ë°ì´í„°ë¥¼ êµ¬ë¶„ì§“ëŠ” í•¨ìˆ˜ë¥¼ ì§ì ‘ (directly) ì°¾ìŒìœ¼ë¡œì¨ ì •ìƒ ì˜ì—­ì— ëŒ€í•œ ê²½ê³„ë©´ì„ êµ¬í•œë‹¤. 
	- ë³´í†µì˜ score ê¸°ë°˜ì˜ ì´ìƒ íƒì§€ ë°©ë²•ê³¼ëŠ” ë‹¬ë¦¬ ëª…ì‹œì ìœ¼ë¡œ (explicitly) 'ê²½ê³„ë©´ ì•ˆì— ìˆìœ¼ë©´ ì •ìƒì´ê³ , ì´ë¥¼ ë„˜ìœ¼ë©´ ì´ìƒì¹˜ë‹¤' ë¼ëŠ” íŒë‹¨ì„ ë‚´ë¦¬ëŠ” í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒ
	- ì´ í•¨ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•œ ë‘ ê°€ì§€ ë°©ë²• : 
		1) 1-SVM (One-class SVM)
		2) SVDD (Support vector data description)

![[Pasted image 20240119185408.png|center]]

## ğŸ¯ One-Class Support Vector Machine

#### 1-SVM

- 1-SVMì˜ ëª©ì  : kernelì— ë§ê²Œ ë°ì´í„°ë¥¼ Feature spaceë¡œ mapping í•˜ëŠ”ë°, ì •ìƒ ë°ì´í„°ë¥¼ ì›ì ì—ì„œ ìµœëŒ€í•œ ë©€ì–´ì§€ë„ë¡ ë¶„ë¥˜ ê²½ê³„ë©´ (hyperplane)ì„ ì„¤ì •í•˜ëŠ” ê²ƒ

##### Optimization problem : 

$$
\begin{aligned}
\min_\mathbf{w} &\frac{1}{2}||\mathbf{w}||^2+\frac{1}{\nu l}\sum_{i=1}^l\xi_i-\rho\\
s.t. \;&\mathbf{w}\cdot\Phi(\mathbf{x}_i)\ge \rho - \xi_i\\
i= &1,2,\cdots,l,\quad \xi\ge 0
\end{aligned}
$$
- $-\rho$ì˜ ì˜ë¯¸ : ì›ì ì—ì„œ ìµœëŒ€í•œ ë©€ë¦¬ ë–¨ì–´ì§„ hyperplaneê³¼ì˜ ê±°ë¦¬ë¥¼ ì°¾ê¸° ìœ„í•¨
- ë‘ ë²ˆì§¸ í•­ $\frac{1}{\nu l}\sum_{i=1}^l\xi_i$ ì˜ë¯¸ : $\xi_i$ ê°€ ê°œë³„ì ì¸ ê°ì²´ì˜ ì›ì ê³¼ì˜ ê±°ë¦¬ê°€ $\rho$ ë³´ë‹¤ ê°€ê¹Œìš¸ ë•Œ, ì¦‰ ì •ìƒ ë°ì´í„°ëŠ” ì›ì ìœ¼ë¡œë¶€í„° hyperplane ë³´ë‹¤ ë°”ê¹¥ì— ìˆì–´ì•¼ í•˜ëŠ”ë°, ê·¸ëŸ¬ì§€ ëª»í–ˆì„ ë•Œ ë¶€ì—¬ë˜ëŠ” penalty. 
	- ì¦‰ ê²½ê³„ë©´ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ëª»í•˜ëŠ” ì •ìƒ ë°ì´í„°ì— ë¶€ì—¬ë˜ëŠ” penalty
	- ì´ penaltyê°€ $s.t.$ ë¶€ë¶„ì˜ ì¢Œë³€ì— ëŒ€í•´ ë¶€ì—¬ë˜ëŠ”ë° $\rho$ ë³´ë‹¤ ê°€ê¹Œìš´ ê²½ìš°ì— ëŒ€í•´ì„œ ë„ë‹¬í•˜ì§€ ëª»í•œë§Œí¼ì˜ penaltyë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ - Soft margin SVMê³¼ ë¹„ìŠ·
- ì²« ë²ˆì§¸ í•­ $\frac{1}{2}||\mathbf{w}||^2$ ì˜ë¯¸ : ëª¨ë¸ì˜ ë³€ë™ì„± ê°ì†Œ
- $0\le \nu\le 1$ : C-SVMì—ì„œëŠ” $\frac{1}{\nu l}$ ëŒ€ì‹  $C$ë¥¼ $\xi$ ì˜ summation ê³„ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ”ë°, $C$ ëŠ” ë°ì´í„°ì…‹ì— ë”°ë¼ì„œ ì–¼ë§ˆë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ê°ì´ ì˜ ì¡íˆì§€ ì•Šì§€ë§Œ, 0~1ì˜ ê°’ì„ ê°€ì§€ë¯€ë¡œ, ì–´ëŠ ì •ë„ ê°’ì„ ê°€ì§ˆ ë•Œ decision boundaryê°€ ì–´ë–¤ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§ˆì§€ì— ëŒ€í•œ ê°ì´ ì¡í˜
	- C-SVMì„ ë¨¼ì € ë°°ìš°ëŠ” ì´ìœ ëŠ” ìˆ˜ì‹ì„ í’€ì–´ê°ì— ìˆì–´ì„œ ë” ìˆ˜ì›”í•˜ê¸° ë•Œë¬¸

##### Decision function : 

$$
f(\mathbf{x}_i) = sign(\mathbf{w}\cdot\Phi(\mathbf{x}_i)-\rho)
$$

![[Pasted image 20240119214942.png|center]]

##### Primal Lagrangian problem - Minimize

$$
L = \frac{1}{2}||\mathbf{w}||^2+\frac{1}{\nu l}\sum_{i=1}^l\xi_i-\rho - \sum_{i=1}^l\alpha_i(\mathbf{w}\cdot\Phi(\mathbf{x}_i)-\rho+\xi_i)-\sum_{i=1}^l\beta_i\xi_i
$$
- $L$ ì˜ ì²˜ìŒ 3ê°œì˜ í•­ì´ ì›ë˜ ê°€ì§€ê³  ìˆë˜ ëª©ì í•¨ìˆ˜ì— í•´ë‹¹í•˜ëŠ” ê²ƒ
- Constraint 1 : ê°œë³„ ì •ìƒ ê°ì²´ê°€ ì›ì ìœ¼ë¡œë¶€í„°ì˜ hyperplane ë°”ê¹¥ìª½ì— ìˆì–´ì•¼ í•˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê°ì²´ë“¤ì— ëŒ€í•´ penaltyë¥¼ ë¶€ì—¬ â‡’ $\alpha_i$ ê°€ ìˆëŠ” í•­
- Constraint 2 : $\xi_i$ ë¼ëŠ” penaltyëŠ” 0 ì´ìƒì´ì–´ì•¼ í•œë‹¤ëŠ” ì œì•½ â‡’ $\beta_i$ ê°€ ìˆëŠ” í•­

##### KKT condition

- ì°¾ì•„ì•¼í•  ë¯¸ì§€ìˆ˜ : $\mathbf{w}$, $\xi_i$, $\rho$  â‡’ ì´ë“¤ë¡œ $L$ ì„ í¸ë¯¸ë¶„
- $\nu$ : ì‚¬ìš©ìê°€ ì§€ì •í•´ì•¼í•  hyperparameter
- $l$ : ê°€ì§€ê³  ìˆëŠ” ì •ìƒ ë°ì´í„°ì˜ ìˆ˜

$$
\begin{aligned}
\frac{\partial L}{\partial\mathbf{w}} &= \mathbf{w} - \sum_{i=1}^l\alpha_i\Phi(\mathbf{x}_i) = 0 &&\Rightarrow\quad \mathbf{w} = \sum_{i=1}^l\alpha_i\Phi(\mathbf{x}_i)\\
\frac{\partial L}{\partial\xi_i} &=\frac{1}{\nu l}-\alpha_i-\beta_i = 0 &&\Rightarrow\quad\alpha_i=\frac{1}{\nu l}-\beta_i\\
\frac{\partial L}{\partial\rho} &= -1+\sum_{i=1}^l\alpha_i = 0 &&\Rightarrow\quad \sum_{i=1}^l\alpha_i=1
\end{aligned}
$$

##### Dual Lagrangian problem - Maximize 

KKT conditionì„ í†µí•´ ê³„ì‚°ëœ ê²°ê³¼ë¥¼ primal lagrangian $L$ ì— ëŒ€ì…í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë³€í•´ì„œ dual lagrangian problem ìœ¼ë¡œ í’€ ìˆ˜ ìˆë‹¤ : 
$$
\begin{aligned}
L = &\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)+\frac{1}{\nu l}\sum_{i=1}^l\xi_i-\rho\\
&-\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j) + \rho\sum_{i=1}^l\alpha_i-\sum_{i=1}^l\alpha_i\xi_i-\sum_{i=1}^l\beta_i\xi_i
\end{aligned}
$$

ìœ„ ì‹ì—ì„œ $\xi_i$ ê°€ ìˆëŠ” í•­ë“¤ë§Œ ë¬¶ì–´ì„œ ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤ : 
$$
\sum_{i=1}^l\left(\frac{1}{\nu l}-\alpha_i-\beta_i\right)\xi_i
$$
ì´ëŠ” KKT conditionì— ì˜í•´ 0ì´ ëœë‹¤. 

ê·¸ë¦¬ê³  $\rho$ ì™€ ê´€ë ¨ëœ í•­ë“¤ì„ ë¬¶ìœ¼ë©´ $\rho\left(-1+\sum_{i=1}^l\alpha_i\right)$ ê°€ ë˜ë¯€ë¡œ ì´ ë˜í•œ KKT conditionì— ì˜í•´ 0ì´ ëœë‹¤. ìµœì¢…ì ìœ¼ë¡œ $L$ ì€ ì•„ë˜ì™€ ê°™ì´ ëœë‹¤ : 
$$
L = -\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)
$$
ì´ë¥¼ ë¶€í˜¸ë¥¼ ë°”ê¾¸ì–´ì„œ ì•„ë˜ ì‹ì„ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤ : 
$$
\begin{aligned}
\min\;&L = \frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)\\
s.t.\;&\sum_{i=1}^l\alpha_i,\quad 0\le\alpha_i\le\frac{1}{\nu l}
\end{aligned}
$$
ì´ë ‡ê²Œ dual problemì´ minimization problemìœ¼ë¡œ ë°”ë€Œë©´ì„œ $\alpha$ ì— ëŒ€í•œ ê¹”ë”í•œ Quadratic covex í•¨ìˆ˜ê°€ ëœë‹¤. 

##### Employ Kernel Trick for a non-linear mapping

![[Pasted image 20240119223417.png|center]]
![[Pasted image 20240119223431.png|center]]

ì´ì œ ì´ $\alpha$ ê°’ë“¤ì˜ ì˜ë¯¸ì— ëŒ€í•´ ì‚´í´ë³´ì. ìš°ì„  ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ì¡°ê±´ë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤ : 
$$
\sum_{i=1}^l\alpha_i=1,\quad 0\le\alpha_i\le\frac{1}{\nu l},\quad \alpha_i+\beta_i=\frac{1}{\nu l},\quad \sum_{i=1}^l\left(\frac{1}{\nu l}-\alpha_i-\beta_i\right)\xi_i=0
$$
- Location of point w.r.t. $\alpha_i$ : 
	- Case 1 (í°ìƒ‰ í¬ì¸íŠ¸) : $\alpha_i=0$ â‡’ $\beta_i=\frac{1}{\nu l}\ne 0$ â‡’ $\xi_i=0$ â‡’ a non-support vector
	- Case 2 (ê²€ì€ìƒ‰ í¬ì¸íŠ¸) : $\alpha_i=\frac{1}{\nu l}$ â‡’ $\beta_i=0$ â‡’ $\xi_i>0$ â‡’ Support vector - outside the hyperplane
	- Case 3 (íšŒìƒ‰ í¬ì¸íŠ¸) : $0<\alpha_i<\frac{1}{\nu l}$ â‡’ $\beta_i > 0$ â‡’ $\xi_i=0$ â‡’ Suppor vector - on the hyperplane

![[Pasted image 20240119225414.png|center]]

##### The role of $\nu$ 

$$
0\le\alpha_i\le\frac{1}{\nu l},\quad \sum_{i=1}^l\alpha_i=1
$$
- $\alpha_i$ì˜ ê°€ëŠ¥í•œ ìµœëŒ€ê°’ì€ $\alpha_i=\frac{1}{\nu l}$
- $\alpha_i$ê°€ ìµœëŒ€ê°’ì„ ê°€ì§€ë©´ì„œ í•©ì´ 1ì´ë˜ë ¤ë©´ $\nu l$ ê°œê°€ í•„ìš”í•˜ë‹¤
	- ë§Œì•½ $l=1000$, $\nu=0.1$ì´ë¼ë©´ ê°€ëŠ¥í•œ ìµœëŒ€ê°’ì€ $\alpha_i=\frac{1}{100}$ ì´ ë˜ê³ , ê·¸ í•©ì´ 1ì´ ë˜ë ¤ë©´ ê·¸ ìµœëŒ€ê°’ì¸ $\alpha_i$ê°€ 100ê°œëŠ” ìˆì–´ì•¼ í•œë‹¤
	- ì´ëŠ” $\alpha_i=\frac{1}{100}$ ì¸ ê²ƒì´ 100ê°œ, $\alpha_i=0$ ì¸ ê²ƒì´ 900ê°œê°€ ìˆë‹¤ëŠ” ì˜ë¯¸
	- ì¦‰, $\nu$ ë¥¼ ì •í•¨ì— ë”°ë¼ ì ì–´ë„ $\nu l$ ê°œì˜ suppor vectorë¥¼ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ 
- outside the hyperplane ì´ ìµœëŒ€ $\nu l$ ê°œê¹Œì§€ ê°€ëŠ¥í•˜ë‹¤.  
	- ì˜ˆë¥¼ ë“¤ì–´ ë™ì¼í•œ ìƒí™©ì—ì„œ  $\alpha_i=\frac{1}{200}$ ì¸ ê²ƒì´ 200ê°œê°€ ìˆë‹¤ê³  í•˜ë©´ SVì˜ ìˆ˜ê°€ 200ê°œì¸ë°, $\alpha_i=\frac{1}{\nu l}=\frac{1}{100}$ ì¸ ê²ƒì´ í•˜ë‚˜ë„ ì—†ë‹¤ê³  í•˜ì. ê·¸ë ‡ë‹¤ë©´ $\xi_i>0$ ì¸ SVëŠ” ì—†ëŠ” ê²ƒì´ë‹¤. 
	- ì¦‰ marginì„ ë„˜ì–´ì„œ penaltyë¥¼ ì ìš©ë°›ëŠ” ê°ì²´ (SV)ì˜ ìµœëŒ€ ê°¯ìˆ˜ëŠ” $\nu l$ ê°œê¹Œì§€ ì´ë‹¤. \
- ë”°ë¼ì„œ $\nu$ ì˜ ì—­í• ì€ <font style="color:skyblue">support vectors ë¹„ìœ¨ (fraction)ì˜ í•˜í•œ (lower bound)</font> ì´ë©´ì„œ <font style="color:skyblue">errors ($\xi_i$ë¼ëŠ” penaltyë¥¼ ì ìš©ë°›ëŠ” support vectors)ì˜ ë¹„ìœ¨ì˜ ìƒí•œ (upper bound)</font> ì´ë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´, $\nu=0.1$ ë¡œ ì„¤ì •í•˜ë©´, ì „ì²´ ì •ìƒ ë°ì´í„°ì˜ 10% ì´ìƒì€ support vectors ì„ì„ ì•Œ ìˆ˜ ìˆê³ , penaltyê°€ ë¶€ì—¬ë˜ëŠ” support vectorsì˜ ìµœëŒ€ ë¹„ìœ¨ì€ 10%ê°€ ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, (ë†’ì€ $\nu$ë¡œ ì¸í•´) support vectorsê°€ ë§ì„ìˆ˜ë¡ ê·¸ë¦¬ê³  penaltyë¥¼ ë¶€ì—¬ë°›ëŠ” support vectorsê°€ ë§ì„ìˆ˜ë¡ ì •ìƒ ë°ì´í„°ì˜ ì˜ì—­ì„ ì›ì ìœ¼ë¡œë¶€í„° ë©€ë¦¬ ë–¨ì–´ì§€ê²Œ í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  $\nu$ ê°€ ì‘ì„ìˆ˜ë¡ ì •ìƒ ì˜ì—­ì˜ boundaryê°€ ë„“ì–´ì ¸ generalizationì´ ì˜ë˜ê³ , $\nu$ ê°€ í´ìˆ˜ë¡ specailizationì— íŠ¹í™”ëœë‹¤. ë”°ë¼ì„œ $\nu$ ë¥¼ ì„¤ì •í•˜ë©´ì„œ ê²°ê³¼ë¬¼ì— ëŒ€í•œ ì–´ëŠì •ë„ ì¶”ì •ì´ ê°€ëŠ¥í•˜ë‹¤. 

- $\nu$ ê°€ ë†’ì„ìˆ˜ë¡ ë¶„ë¥˜ ê²½ê³„ë©´ (decision boundary)ëŠ” ë” ë³µì¡í•´ì§„ë‹¤ : 

![[Pasted image 20240119231224.png|center]]

## ğŸ¯ Support Vector Data Description

ì§ê´€ì ì¸ ì´í•´ëŠ” 1-SVM ë³´ë‹¤ SVDDê°€ ë” ì‰½ë‹¤. 

- Feature spaceì—ì„œ ëª¨ë“  ì •ìƒ ë°ì´í„°ë“¤ì„ ê°ì‹¸ëŠ” hypersphereë¥¼ ì°¾ëŠ” ê²ƒ - ê°€ì¥ ì‘ì€ ê²ƒìœ¼ë¡œ!

![[Pasted image 20240119232642.png|center]]

ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ hypersphereì˜ ì¤‘ì‹¬ $a$ ì™€ ë°˜ì§€ë¦„ $R$ ì„ ì°¾ì•„ì„œ ì´ ì•ˆì— ë°ì´í„°ë“¤ì„ ëª¨ì•„ì•¼ í•œë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ë„ˆë¬´ ë™ë–¨ì–´ì§„ ê°ì²´ ($x_i$)ì— ëŒ€í•´ì„œëŠ” $\xi_i$ ë¼ëŠ” penaltyë¥¼ ë¶€ì—¬í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤. SVDDì—ì„œì˜ ë¯¸ì§€ìˆ˜ëŠ” hypersphereì˜ ì¤‘ì‹¬ì¸ $a$, ë°˜ì§€ë¦„ $R$, ê°œë³„ penalty $\xi_i$ ê°€ ëœë‹¤. 

ìœ„ ê·¸ë¦¼ì„ í†µí•´ì„œ 1-SVMê³¼ SVDDì˜ ëª©ì ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ 1-SVMì€ ì •ìƒ ë°ì´í„°ë¥¼ ì›ì ìœ¼ë¡œë¶€í„° ë©€ë¦¬ ìœ„ì¹˜í•˜ê²Œë” í•˜ëŠ” hyperplaneì„ ì°¾ëŠ” ê²ƒì´ê³ , SVDDëŠ” ì •ìƒ ë°ì´í„°ì˜ ì˜ì—­ì„ ìµœëŒ€í•œ ê°ì‹¸ëŠ” hypersphereë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. 

##### Optimization function : 

$$
\begin{aligned}
\min_{R,\,\mathbf{a},\,\xi_i}R^2&+C\sum_{i=1}^l\xi_i\\
s.t.\;||\Phi(\mathbf{x}_i)-\mathbf{a}||^2\le\; &R^2 + \xi_i,\quad \xi_i\ge 0,\quad \forall i.
\end{aligned}
$$
- C-SVMì˜ í˜•íƒœë¡œ ì‚´í´ë³´ì. 
- ë°˜ì§€ë¦„ $R$ë¥¼ ìµœì†Œí™”í•˜ëŠ” compactí•œ êµ¬ë¥¼ ì°¾ë˜ êµ¬ê°€ ê°ì‹¸ì§€ ëª»í•˜ëŠ” ê°ì²´ë“¤ì— ëŒ€í•´ì„œëŠ” penaltyë¥¼ ë¶€ì—¬
- ì œì•½ : ê°œë³„ ê°ì²´ ($\Phi(\mathbf{x}_i)$)ì—ì„œ êµ¬ì˜ ì¤‘ì‹¬ ($\mathbf{a}$) ê¹Œì§€ì˜ ê±°ë¦¬ì˜ ì œê³±ì€ $R^2$ ë³´ë‹¤ ì‘ì•„ì•¼í•˜ëŠ”ë° êµ¬ê°€ ê°ì‹¸ì§€ ëª»í•˜ë©´ penalty ($\xi_i$)ë¥¼ ë¶€ì—¬í•¨

##### Decision function : 
$$
f(\mathbf{x}) = sign(R^2 - ||\Phi(\mathbf{x}_i)-\mathbf{a}||^2)
$$

ì–‘ìˆ˜ë©´ ì •ìƒ, ìŒìˆ˜ë©´ ì´ìƒ ë°ì´í„°

##### Primal Lagrangian problem - Minimization 

$$
\begin{aligned}
L = R^2+C\sum_{i=1}^l\xi_i-\sum_{i=1}^l\alpha_i\Bigg(R^2+&\xi_i-\left(\Phi(\mathbf{x}_i)\cdot\Phi(\mathbf{x}_i)-2\mathbf{a}\cdot\Phi(\mathbf{x}_i)+\mathbf{a}\cdot\mathbf{a}\right)\Bigg)-\sum_{i=1}^l\beta_i\xi_i\\
&\alpha_i\ge0,\quad\beta_i\ge0
\end{aligned}
$$

##### KKT condition 

ìœ„ Primal Lagragian $L$ ì„ ë¯¸ì§€ìˆ˜ $R$, $\mathbf{a}$, $\xi_i$ì— ëŒ€í•´ í¸ë¯¸ë¶„í•˜ì—¬ ê³„ì‚°í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤ : 
$$
\begin{aligned}
\frac{\partial L}{\partial R} &= 2R-2R\sum_{i=1}^l\alpha_i=0 &&\Rightarrow\quad\sum_{i=1}^l\alpha_i=1\\
\frac{\partial L}{\partial \mathbf{a}} &= 2\sum_{i=1}^l\alpha_i\cdot\Phi(\mathbf{x}_i)-2\mathbf{a}\sum_{i=1}^l\alpha_i=0 &&\Rightarrow\quad \mathbf{a}=\sum_{i=1}^l\alpha_i\cdot\Phi(\mathbf{x}_i)\qquad \because\sum_{i=1}^l\alpha_i=1\\
\frac{\partial L}{\partial\xi_i} &= C-\alpha_i-\beta_i=0,\quad \forall i
\end{aligned}
$$

##### Dual Lagrangian problem - Maximization 

![[Pasted image 20240119235146.png|center]]

##### Dual Lagrangian problem - Minimization : 

$$
L = \sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)-\sum_{i=1}^l\alpha_i\Phi(\mathbf{x}_i)\cdot\Phi(\mathbf{x}_i)\quad (0\le\alpha_i\le C)
$$

ì´ì œ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ì œì•½ì€ ì•„ë˜ì™€ ê°™ë‹¤ : 
$$
C-\alpha_i-\beta_i=0,\quad \beta_i\xi_i=0
$$

- Location of a point w.r.t. $\alpha_i$ : 
	- Case 1 (í°ìƒ‰ í¬ì¸íŠ¸) : $\alpha_i=0$ â‡’ $\beta_i=C>0$ â‡’ $\xi_i=0$ â‡’ a non-support vector (êµ¬ì˜ ë‚´ë¶€ì— ìˆìŒ)
	- Case 2 (ê²€ì€ìƒ‰ í¬ì¸íŠ¸) : $\alpha_i=C$ â‡’ $\beta_i=0$ â‡’ $\xi_i>0$ â‡’ Support vector - outside the hypersphere
	- Case 3 (íšŒìƒ‰ í¬ì¸íŠ¸) : $0<\alpha_i<C$ â‡’ $\beta_i>0$ â‡’ $\xi_i=0$ â‡’ Support vector - on the hypersphere

![[Pasted image 20240119235809.png|center]]

- SVDD with Gaussian (rbf) kernels : $$K(\mathbf{x}_i,\mathbf{x}_j)=\exp\left(\frac{-||\mathbf{x}_i-\mathbf{x}_j||^2}{s^2}\right)$$
	- Kernel width $s$ ê°€ ì‘ì„ìˆ˜ë¡ flexibleí•œ decision boundaryê°€ ë§Œë“¤ì–´ì§€ê³ , í´ìˆ˜ë¡ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ì–´ì§„ë‹¤. 

![[Pasted image 20240120000033.png|center]]


- ë§Œì¼ ëª¨ë“  ë°ì´í„°ê°€ unit norm vectorë¡œ normalized ëœë‹¤ë©´ 1-SVMê³¼ SVDDëŠ” ë™ì¼í•˜ë‹¤ : 

![[Pasted image 20240120000133.png|center]]

- 1-SVM ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, $\nu$ -SVDDë„ ê°€ëŠ¥í•˜ë‹¤
- ì•„ë˜ ê·¸ë¦¼ì—ì„œ $D$ ê°€ $\nu$ì˜ ì—­í• ì„ í•œë‹¤ : 

![[Pasted image 20240120000305.png|center]]

ë§ˆì°¬ê°€ì§€ë¡œ $\nu$ ê°€ ì‘ì„ìˆ˜ë¡ generalizationì— íŠ¹í™”ë˜ê³ , í´ìˆ˜ë¡ specializationì— ì§‘ì¤‘ëœ boundaryë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. 