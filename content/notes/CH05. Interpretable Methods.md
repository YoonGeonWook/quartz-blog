---
sticker: emoji//1f525
tags:
  - XAI
  - Interpretability
  - 해석가능성
  - Linear-regression
  - Logistic-regression
  - GLM
  - GAM
  - Decision-Tree
  - Decision-Rules
  - RuleFit
  - KNN
  - Interpretability-Machine-Learning
  - ML
  - bike-rentals
  - cervical-cancer
---
Interpretability 를 확보하는 가장 쉬운 방법은 interpretable models 를 생성하는 알고리즘의 하위 집합만을 사용하는 것입니다. 선형회귀, 로지스틱 회귀, 결정 트리는 일반적으로 사용되는 interpretable models 입니다. 이 챕터에서는 이러한 모델들에 대해 설명합니다. 모델을 해석하는 방법에 중점을 둘 것입니다. 

이 책에서 설명하는 모든 interpretable models 는 KNN 을 제외하고는 모두 modular level 에서 해석할 수 있습니다. 아래 표는 interpretable model의 type과 properties 에 대한 개요를 보여줍니다. Features 와 target 간 연관성이 선형적인 경우 모델은 선형적으로 진행됩니다. 단조 제약(monitinicity constraints) 조건이 있는 모델은 features 와 target 간 관계가 features 의 전체 범위에 걸쳐 항상 같은 방향으로 진행되도록 합니다: 즉 features 값의 증가는 항상 target 의 증가/감소로 이어진다는 것입니다. Monotinicity 는 모델 해석에 유용합니다. 

몇몇 모델은 target 의 예측을 위해 features 간 상호작용을 포함하기도 합니다. 상호작용은 예측 성능을 향상시킬 순 있지만 해석 가능성을 저하시킵니다. 

| Algorithm | Linear | Monotone | Interaction | Task |
| :--- | :--- | :--- | :--- | :--- |
| Linear regression | Yes | Yes | No | regr |
| Logistic regression | No | Yes | No | class |
| Decision trees | No | Some | Yes | class,regr |
| RuleFit | Yes | No | Yes | class,regr |
| Naive Bayes | No | Yes | No | class |
| k-nearest neighbors | No | No | No | class,regr |

로지스틱 회귀와 나이브 베이즈 모두 선형적 설명이 가능하다고 주장할 수 있지만 이는 target의 log 값에만 해당됩니다.

## 05-01. Linear Regression

선형회귀는 feature input의 가중치 합으로 target 을 예측합니다. 선형적인 관계를 학습하기 때문에 해석이 매우 쉽습니다. 선형 모델은 regression target $y$가 몇몇 features $x$에 대한 의존성(dependence)을 모델링하는 데 사용됩니다. 단일 인스턴스 $i$에 대해 아래와 같은 relationship으로 표현할 수 있습니다: 
$$
y=\beta_0+\beta_1 x_1+\ldots+\beta_p x_p+\epsilon
$$

하나의 인스턴스에 대한 예측 결과는 해당 인스턴스가 가진 $p$ 개의 features 가중치 합계입니다. $\beta_j$ 는 학습된 feature weights/coefficients 를 나타냅니다. 최적의 가중치를 추정하기 위해 보통 최소제곱법을 사용해 true outcome 과 estimated outcome 간의 제곱 차이를 최소화하여 가중치를 찾습니다:

$$
\hat{\boldsymbol{\beta}}=\arg \min _{\beta_0, \ldots, \beta_p} \sum_{i=1}^n\left(y^{(i)}-\left(\beta_0+\sum_{j=1}^p \beta_j x_j^{(i)}\right)\right)^2
$$

선형 회귀 모형의 가장 큰 장점은 선형성입니다: 추정 절차가 간단하고 이러한 선형 방정식을 modular level(i.e. 가중치)에서 쉽게 해석할 수 있다는 것입니다. 

선형 회귀 모델이 'correct' model 인지 여부는 데이터의 관계가 선형성, 정규성, 등분산성, 독립성, fixed features, 다중공선성의 부재와 같은 가정들을 충족하는지를 따져보고 알 수 있습니다.

##### Linearity

선형 회귀 모델은 예측이 features 의 선형 조합으로 구성되도록 하는데, 이는 가장 큰 장점이자 한계점입니다. 선형성은 interpretable model 로 이어집니다. Features 의 상호작용 또는 features 와 target 간 비선형적 연관성이 의심될 경우, interaction terms 를 추가하거나 회귀 스플라인을 사용할 수 있습니다.

##### Normality

Features 가 주어진 taraget 은 정규 분포를 따른다고 가정합니다. 이 가정을 위반하면 feature weights 의 추정 신뢰구간이 유효하지 않습니다.

##### Homoscedasticity - constants variance

오차 항의 분산이 전체 features space 에 걸쳐서 일정하다고 가정하는 것입니다. 이 가정은 종종 위반되기 쉬운데, 이런 경우는 heteroscedasticity 와 같은 문제가 발생합니다.

##### Independence

각 인스턴스는 다른 인스턴스와 독립이라고 가정합니다.

##### Fixed features

Input features 는 변수가 아닌 주어진 상수로 취급합니다. 이는 측정 오류가 없음을 의미합니다. 

##### Absence of multicollinearity

상관관계가 강한 features 간에는 가중치 추정이 엉망이 될 수 있습니다. 


#### 1. interpretation

선형 회귀 모델에서 가중치의 해석은 해당되는 features 의 유형에 따라 달라집니다:

- Numeric feature: 
	- 한 단위 증가시키면 가중치만큼 estimated outcome이 변경됩니다. 
- Binary feature:
	- reference category 를 사용합니다.
- Categorical feature with multiple levels:
	- One-hot encoding or dummy encoding etc
- Intercept $\beta_0$ 


##### **Interpretation of a Numerical feature**
 
- 다른 모든 feature 값들이 고정되어 있을 때, $x_k$ 의 한 단위 증가는 예측치가 $\beta_k$ 만큼 증가하게 합니다.

##### **Interpretation of a Categorical Feature**

 - 다른 모든 feature 값들이 고정되어 있을 때, $x_k$ 를 참조 레벨로부터 다른 level 로 변경하면 예측치는 $\beta_k$ 만큼 변합니다.

##### $R^2$ 결정계수

- 결정계수는 전체 분산에 비해 모델에 의해서 설명되는 분산의 양을 측정합니다. $$R^2=1-SSE/SST$$
- SSE 는 잔차의 제곱합입니다: $$SSE = \sum_{i=1}^n(y_i-\hat{y}_i)^2$$
- SST : $$SST = \sum_{i=1}^n(y_i-\bar{y})^2$$
- SSE 는 선형 모델을 적합한 후 얼마나 많은 분산이 남았는지 알려주며, SST는 target 의 총 분산입니다. 결정계수는 선형 모델로 분산을 얼마나 설명할 수 있는지를 알려줍니다. 
- 결정계수는 features 의 수가 증가하면 따라서 증가하는 경향이 있습니다. 따라서 이를 보완하기 위해서는 Adjusted R-sq를 사용하면 됩니다: $$ AR^2 = 1-(1-R^2)\frac{n-1}{n-p-1}$$

##### Feature importance

선형 회귀 모형에서의 변수 중요도는 각 feature weight 에 대한 $t$-통계량의 절대값으로 나타냅니다: 
$$
t_{\hat{\beta}_j}=\frac{\hat{\beta}_j}{S E\left(\hat{\beta}_j\right)}
$$

#### 2. Example

이 예제에서는 선형 회귀 모델을 사용하여 날씨 및 달력 정보가 주어지면 특정 날짜에 대여된 자전거 수를 예측합니다. 해석을 위해 추정된 회귀 가중치를 살펴보겠습니다. 

> [!note]- code fold
> ```r
> X <- bike %>% select(all_of(bike.features.of.interest))
> y <- bike %>% pull(cnt)
> dat <- cbind(X,y)
> 
> mod <- lm(y ~ ., data = dat, x = T)
> lm_summary <- summary(mod)$coefficients
> 
> tidy(mod) %>% 
>   select(` ` = term, Weight = estimate, SE = std.error, `|t|` = statistic) %>% 
>   mutate(`|t|` = abs(`|t|`))
> ```

|                             |  Weight   |  SE     | \|t\|   |    |    |
|:----------------------------|:----------|:--------|:--------|:---|:---|
|  (Intercept)                |   2399.4  |  238.3  |   10.1  |    |    |
|  seasonSPRING               |    899.3  |  122.3  |    7.4  |    |    |
|  seasonSUMMER               |    138.2  |  161.7  |    0.9  |    |    |
|  seasonFALL                 |    425.6  |  110.8  |    3.8  |    |    |
|  holidayHOLIDAY             |   -686.1  |  203.3  |    3.4  |    |    |
|  workingdayWORKING DAY      |    124.9  |   73.3  |    1.7  |    |    |
|  weathersitMISTY            |   -379.4  |   87.6  |    4.3  |    |    |
|  weathersitRAIN/SNOW/STORM  |  -1901.5  |  223.6  |    8.5  |    |    |
|  temp                       |    110.7  |      7  |   15.7  |    |    |
|  hum                        |    -17.4  |    3.2  |    5.5  |    |    |
|  windspeed                  |    -42.5  |    6.9  |    6.2  |    |    |
|  days_since_2011            |      4.9  |    0.2  |   28.5  |    |    |  

- `temp` 해석: 다른 모든 features 가 고정돼 있을 때 `temp`가 섭씨 1도 상승하면 추정 자전거 수가 110.7대 증가합니다.
- `weathersit` 해석: 다른 모든 features가 변하지 않는다고 가정할 때 날씨가 좋을 때와 비교하여 비/눈/폭풍우가 내릴 때 예상되는 자전거 수는 -1901.5대 감소합니다. 안개 낀날에는 다른 모든 조건이 동일할 때 좋은 날씨에 비해 예상 자전거 수가 -379.4대 감소합니다. 

이처럼 모든 해석에 항상 '다른 모든 features 가 동일하게 유지된다' 라는 조건이 붙습니다. 이는 선형 회귀 모델의 특성 때문입니다. Estimated target 은 가중치가 적용된 features 의 선형조합입니다. 장점은 가중치가 개별 feature effect 의 해석을 다른 features 와 분리한다는 것입니다. 단점은 이 해석이 features 의 joint distribution 을 무시한다는 것입니다. 

#### 3. Visual Interpretation

다양한 시각화를 통해 선형 회귀 모델을 쉽고 빠르게 파악할 수 있습니다.

##### Weight plot

가중치 테이블의 정보(Weights & variance estimates)는 가중치 플롯으로 시각화할 수 있습니다:

> [!note]- code fold
> ```r
> #### 1. Weight Plot
> coef_plot(mod) +
>   scale_y_discrete("")
> ```

![[Pasted image 20231227172743.png|center]] Figure 5.1 : Weights are displayed as pts and the 95% CIs as lines.

Weights Plot 은 비/눈/폭풍우 날씨가 예측 자전거 수에 강한 음의 영향을 미치고 있음을 보여줍니다. Working day의 가중치는 0에 가깝고 95% CI에 0이 포함되어 있어 그 효과가 통계적으로 유의하지 않음을 알 수 있습니다. 몇몇 features 의 가중치가 0에 가깝고 CI가 매우 짧지만 효과는 유의한 것으로 나타납니다. `temp` 온도가 그러합니다. Weights Plot 의 문제는 features 가 서로 다른 척도로 측정된다는 것입니다. 따라서 선형 모델을 적합하기 전에 scaling(mean:0, sd: 1)로 조정하여 추정 가중치 간 비교를 용이하게 하는 것이 좋습니다.

##### Effect plot

선형 회귀 모델의 가중치는 실제 features 값을 곱할 때 더 의미 있는 분석이 가능합니다. 가중치는 features 의 scale 에 다라 달라집니다. 또한 분산이 낮으면 모든 인스턴스에서 해당 feature 의 기여도가 비슷하다는 의미가 되기 때문에 데이터에서 feature 의 분포를 아는 것도 중요합니다. Effect Plot 은 가중치와 feature 의 조합이 데이터 예측에 얼마나 기여하는지를 이해하는 데 도움이 됩니다. 우선 feature 별로 가중치와 인스턴스 값을 곱한 effect 를 계산합니다:
$$ 
\text{effect}_j^{(i)} = w_jx_j^{(i)}
$$

이 효과는 boxplot 으로 시각화 합니다. 범주형 피처의 effect 는 각 레벨에 고유한 row 가 있는 weights plot 과 비교해서 단일 boxplot 으로 요약할 수 있습니다

> [!note]- code fold
> ```r
> #### 2. Effect Plot
> effect_plot(mod, dat) +
>   scale_x_discrete("")
> ```

![[Pasted image 20231227175542.png|center]] Figure 5.2: The feature effect plot shows the distribution of effects (=feature value times weight) across the data per feature.

예상 대여 자전거 수에 가장 큰 기여를 하는 것은 시간 경과에 따른 자전거 대여 추세(trend)를 파악하는 `temp`와 `days_since_2011` 입니다. `temp` 는 예측에 기여하는 정도의 범위가 넓습니다. `days_since_2011` 의 가중치가 4.3이기 때문에 첫 번째 날(2011년 1월 1일)은 trend effect 가 매우 작고, 일 수가 지남에 따라 feature 는 큰 양의 기여도를 갖습니다. 즉, 이 효과는 매일 증가하며 데이터셋의 마지막 날(2012년 12월 31일)에 가장 높습니다. `windspeed` 의 음의 효과가 큰 날은 풍속이 높은 날입니다.

#### 4. Explain Individual Predictions

인스턴스 개개인에 대해서 각 feature 는 예측에 얼마나 기여했을까요? 인스턴스별 effects 에 대한 해석은 각 feature 에 대한 effect 분포와 비교할 때만 의미를 갖습니다. 자전거 데이터셋의 6번째 인스턴스에 대한 선형 모델의 예측을 보겠습니다. 

> [!note]- code fold
> ```r
> df <- data.frame(feature = colnames(bike), value = t(bike[6, ]))
> colnames(df) <- c("feature", "value")
> df
> ```

| **Feature**     | **Value**   |
|:--------------- |:----------- |
| season          | WINTER      |
| yr              | 2011        |
| mnth            | JAN         |
| holiday         | NO HOLIDAY  |
| weekday         | THU         |
| workingday      | WORKING DAY |
| weathersit      | GOOD        |
| temp            | 1.604356    |
| atemp           | 23.92911    |
| hum             | 51.8261     |
| windspeed       | 6.000868    |
| cnt             | 1606        |
| days_since_2011 | 5           |

이 6번째 인스턴스의 feature effects 를 얻으려면 해당 피처 값에 가중치를 곱해야 합니다. `workingday` 의 `'WORKING DAY'` 에 대한 effect 는 124.9 입니다. `temp` 가 섭씨 1.6도 인 경우 effect 는 1.6 * 110.7 = 177.6 입니다. 이러한 개별 효과를 Effects plot 에서  crosses 로 표기하면 데이터에서 개별 효과의 분포를 볼 수 있습니다. 이를 통해 개별 효과를 데이터의 효과 분포와 비교할 수 있습니다:

> [!note]- code fold
> ```r
> effect_plot(mod, dat) +
>   geom_point(aes(x = key, y = value), color = 'red', data = effects_6, shape = 4, size = 4) +
>   scale_x_discrete("") +
>   ggtitle(sprintf("Predicted value for instance: %.0f\nAverage predicted value: %.0f\nActual value: %.0f",
>                   pred_6, predictions_mean, y[6]))
> ```

![[Pasted image 20231227182345.png|center]] Figure 5.3 : The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest.

학습 데이터 인스턴스에 대한 예측의 평균을 구하면 4504가 나옵니다. 이에 비해 6번째 인스턴스의 예측은 1571건에 불과하므로 작은 예측값을 갖습니다. Effect Plot을 보면 그 이유를 알 수 있습니다. Box plot 은 데이터셋의 모든 인스턴스에 대한 effect 분포를 나타내고, X 표시는 6번째 인스턴스에 대한 effects 를 나타냅니다. 6번째 인스턴스의 `temp` effect 가 낮은 이유는 이날의 기온이 섭씨 1.6도로 낮았기 때문입니다( `temp` 피처의 가중치가 양수임을 기억하자). 또한 6번째 인스턴스는 시작 날짜로부터 얼마 떨어져 있지 않아서 trend 피처인 `days_since_2011`의 effect가 다른 인스턴스들에 비해 작습니다.

#### 6. Do Linear Models Create Good Explanations?

[[CH03. Interpretability#03-06. Human-friendly Explanations|Human-friendly Explanations]] 에서 제시한 대로 good explantion 의 속성으로 판단할 때, 선형 모델은 Best 인 설명은 아닙니다. 대조적이기는 하지만 숫자 피처는 0과 비교하고, 범주형 피쳐는 참조 범주에 있는 데이터 포인트로 비현실적인 무의미한 인스턴스입니다. 또 다른 속성으로는 선택성(selectivity)입니다. 기본적으로 선형 모델은 선택적 설명을 하지 않습니다. 


#### 7. Sparse Linear Models

수백, 수천 개의 피쳐가 존재할 경우 선형 회귀 모델은 해석 가능성이 떨어집니다. 심지어 인스턴스보다 피쳐가 더 많은 경우에는 적합을 할 수조차 없을 수 있습니다. 다행인 점은 sparsity 를 도입해 이를 해결할 수 있다는 것입니다.

##### Lasso

라쏘는 선형 회귀 모델에 sparsity 를 도입한 방법입니다. 이를 선형 회귀 모델에 적용하면 피쳐 선택(feature selection)과 선택된 피쳐 가중치에 대해 정규화(regularization)를 수행합니다. 아래의 목적함수를 가지고 라쏘는 최적화 합니다:
$$
\min _{\boldsymbol{\beta}}\left(\frac{1}{n} \sum_{i=1}^n\left(y^{(i)}-x_i^T \boldsymbol{\beta}\right)^2+\lambda\|\boldsymbol{\beta}\|_1\right)
$$
여기서 $||\boldsymbol{\beta}||_1$은 피쳐의 L1-norm 으로 큰 가중치에 페널티를 부여합니다. 이로 인해 몇몇 피쳐들의 가중치는 0의 값을 갖게 되고 나머지는 가중치 값이 줄어드는 효과가 생깁니다. Parameter $\lambda$ 는 정규화의 강도를 조절하는 값으로 CV 를 통해 tuning 됩니다.  $\lambda$ 가 클수록 더 많은 가중치가 0이 됩니다.  

> [!note]- code fold
> ```r
> library("glmnet")
> X.d = model.matrix(y ~ . -1, data = X)
> l.mod = glmnet(X.d, y)
> plot(l.mod,  xvar = "lambda", ylab="Weights")
> ```

![[Pasted image 20231227184855.png|center]] Figure 5.4 : With increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths. The number above the plot is the number of non-zero weights.

페널티 항을 tuning parameter 로 간주해서 교차 검증을 통해 모델 오류를 최소화 하는 $\lambda$ 를 찾을 수 있습니다. 또한 $\lambda$ 는 모델의 해석 가능성을 제어하는 parameter 로 간주할 수도 있습니다. 페널티가 클수록 모델에 존재하는 피쳐의 수가 줄어들고 모델을 더 잘 해석할 수 있게 됩니다.

##### Example with Lasso

Lasso 를 사용해 자전거 대여를 예측해 보겠습니다. 모델에 포함할 피쳐의 수를 미리 지정합니
다. 먼저 2개로 해보겠습니다: 

> [!note]- code fold
> ```r
> ### Example with Lasso
> extract.glmnet.effects <- function(betas, best.index){
>   data.frame(beta = betas[, best.index])
> }
> 
> n.features <- apply(l.mod$beta, 2, function(x){sum(x!=0)})
> 
> extract.glmnet.effects(l.mod$beta, max(which(n.features==2))) %>% 
>   rownames_to_column()
> ```

|                           | Weight |
|:--------------------------|:-------|
| seasonWINTER              |      0 |
| seasonSPRING              |      0 |
| seasonSUMMER              |      0 |
| seasonFALL                |      0 |
| holidayHOLIDAY            |      0 |
| workingdayWORKING DAY     |      0 |
| weathersitMISTY           |      0 |
| weathersitRAIN/SNOW/STORM |      0 |
| temp                      |  52.33 |
| hum                       |      0 |
| windspeed                 |      0 |
| days_since_2011           |   2.15 |  

5개로 해보겠습니다: 

> [!note]- code fold
> ```r
> extract.glmnet.effects(l.mod$beta, max(which(n.features==5))) %>% 
>   rownames_to_column()
> ```

|                           | Weight  |
|:--------------------------|:--------|
| seasonWINTER              | -389.99 |
| seasonSPRING              |       0 |
| seasonSUMMER              |       0 |
| seasonFALL                |       0 |
| holidayHOLIDAY            |       0 |
| workingdayWORKING DAY     |       0 |
| weathersitMISTY           |       0 |
| weathersitRAIN/SNOW/STORM | -862.27 |
| temp                      |   85.58 |
| hum                       |   -3.04 |
| windspeed                 |       0 |
| days_since_2011           |    3.82 |  

`temp`와 `days_since2011`의 가중치는 2개일 때와 5개일 때가 다릅니다. 그 이유는 $\lambda$ 를 낮추면 이미 포함되어 있는 피쳐가 받는 페널티가 줄어들게 되어 가중치의 절대값이 커질 수 있기 때문입니다. LASSO 가중치의 해석은 선형 회귀 모형과 같습니다. `glmnet()` 을 이용하면 default 로 표준화를 진행해주지만 가중치는 다시 자동으로 변환되어 내뱉습니다.


##### Other methods for sparsity in linear models

선형 모델에서 피쳐의 수를 줄이는 다양한 방법이 존재합니다.

- Pre-processing methods :
	- Manually selected features : 전문 지식을 사용해 일부 피쳐를 선택하거나 버릴 수 있습니다. 단점은 자동화가 불가능하고 도메인 지식이 있는 사람의 도움이 필요합니다.
	- Univariate selection : 상관 계수를 이용해 특정 임계값을 초과하는 피쳐만을 고려할 수 있습니다. 단점은 피쳐들을 개별적으로 고려한다는 것입니다. 
- Step-wise methods : 
	- Forward selection
	- Backward elimination

#### 8. Advantages

예측을 가중치의 합으로 모델링하면 예측이 어떻게 생성되는지 투명하게(transparent) 알 수 있습니다. 또한 Lasso 를 사용하면 피쳐의 수도 적게 할 수 있습니다. 

수학적으로 가중치 추정이 간단하고 선형 모델의 모든 가정이 충족된다는 전제하에 최적의 가중치를 찾을 수 있다는 보장이 있습니다. 

또한 가중치와 함께 신뢰구간, 검정 및 통계 이론을 얻을 수 있습니다. 또 다른 확장 버전은 GLM, GAM 등이 있습니다. 

#### 9. Disadvantages

비선형 관계 또는 상호작용이 의심될 경우 이를 수작업으로 만들어야 하며 이를 모델에 명시적으로 지정해야 합니다. 

선형 모델이 학습할 수 있는 관계는 매우 제한적이고 현실의 복잡성을 너무 단순화하기 때문에 예측 성능이 그다지 좋지 못합니다. 

가중치 해석이 다른 피쳐들에 따라 달라지기 때문에 직관적이지 않을 수 있습니다. target y 와 양의 상관관계가 높은 피쳐가 다른 피쳐들이 포함되어 있는 선형 모델에서 음의 가중치를 가질 수 있는데, 이는 다른 상관관계가 있는 피쳐가 high-dim space 에서 y 와 음의 상관관계가 있기 때문입니다. 


---
## 05-02. Logistic Regression

로지스틱 회귀는 binary target 에 대한 분류 문제인 경우 확률을 모델링 합니다. 이는 분류 문제에 대한 선형 회귀 모델의 확장입니다. 

#### 1. What is Wrong with Linear Regression for Classfication?

선형 회귀 모형은 regression 에서는 잘 작동하지만 classification 에는 실패할 확률이 높습니다. Binary class 를 갖는 target 변수를 고려하겠습니다. 기술적으로는 이 문제를 선형 회귀 모형에서 작동이 가능하며 가중치를 내뱉습니다. 하지만 이 방식에는 몇 가지 문제점이 있습니다: 
- 분류 문제에 있어서 선형 모형은 클래스 확률을 내뱉지 않고 target 의 클래스를 숫자로 인식하여 점과 hyperplane 간 거리를 최소화하는 최적의 hyperplane 을 찾아냅니다. 이는 점과 점 사이를 보간(interpolate)하는 것이므로 클래스 확률이 아닙니다. 
- 클래스 예측 값으로 0 미만 또는 1 초과의 값을 내뱉기도 합니다.
- 예측 결과는 클래스 확률이 아닌 점 사이의 선형 보간 (linear interpolation)이기 때문에 클래스를 구분할 수 있는 유의미한 임계값 (threshold)이 존재하지 않습니다. 
- 또한 선형 모델을 multi-class 에 대한 분류 문제로 확장할 수 없습니다. 

> [!note]- code fold
> ```r
> df <- data.frame(x = c(1,2,3,8,9,10,11,9),
>                  y = c(0,0,0,1,1,1,1,0),
>                  case = '1) 0.5 threshold ok')
> df_extra <- data.frame(x = c(df$x, 7,7,7,20,19,5,5,4,4.5),
>                        y = c(df$y, 1,1,1,1,1,1,1,1,1),
>                        case = '2) 0.5 threshold not ok')
> df.lin.log <- rbind(df, df_extra)
> 
> df.lin.log %>% 
>   ggplot(aes(x=x, y=y)) +
>   # position_jitter(): 겹치는 데이터를 시각적으로 구분하기 위해 사용
>   geom_point(position = position_jitter(width = 0, height = 0.02)) + 
>   geom_smooth(method = 'lm', se=F) +
>   my_theme() +
>   scale_y_continuous('', breaks = c(0, 0.5, 1), labels = c('benign tumor', '0.5', 'malignant tumor'), limits = c(-0.1, 1.3)) +
>   scale_x_continuous('Tumor size') +
>   facet_grid(. ~ case) +
>   geom_hline(yintercept = 0.5, linetype = 3)
> ```

![[Pasted image 20231230125820.png|center]] Figure 5.5 : 선형 모델은 종양(tumor)의 크기에 따라 악성(malignant: 1) 또는 양성(benign: 0)으로 분류합니다. 파란색 선은 모델의 예측치를 나타냅니다. 왼쪽의 경우 0.5를 분류 임계값으로 사용하는 것이 가능합니다. 이후 악성 종양 사례를 몇 개 더 추가하여 회귀선이 바뀌면 (오른쪽처럼) 임계값 0.5으로는 더 이상 분류 해내지 못합니다. 

#### 2. Theory

로지스틱 회귀를 통해 분류 문제를 해결할 수 있습니다. 로지스틱 함수는 아래와 같이 정의 됩니다: 
$$
\operatorname{logistic}(\eta)=\frac{1}{1+\exp (-\eta)}
$$

> [!note]- code fold
> ```r
> logistic <- function(x){1 / (1+exp(-x))}
> x <- seq(from = -6, to = 6, length.out = 100)
> df <- data.frame(x = x,
>                  y = logistic(x))
> df %>% 
>   ggplot(aes(x=x, y=y)) +
>   geom_line() + 
>   my_theme()
> ```

![[Pasted image 20231230130556.png|center]] Figure 5.6 : 로지스틱 함수는 0과 1 사이의 숫자를 출력합니다. 0의 input에서는 0.5를 출력합니다.

선형 회귀 모델에서는 ouput 과 features 간 관계를 아래와 같이 선형 방정식으로 모델링 했습니다: 
$$
\hat{y}^{(i)}=\beta_0+\beta_1 x_1^{(i)}+\ldots+\beta_p x_p^{(i)}
$$
분류 문제의 경우 0과 1 사이의 클래스 확률을 더 선호하기 때문에 위 방정식의 우변을 로지스틱 함수의 input 으로 취급합니다: 
$$
P\left(y^{(i)}=1\right)=\frac{1}{1+\exp \left(-\left(\beta_0+\beta_1 x_1^{(i)}+\ldots+\beta_p x_p^{(i)}\right)\right)}
$$

종양(tumor) 크기 예제를 다시 살펴보겠습니다:

> [!note]- code fold
> ```r
> logistic1 <- glm(y ~ x, family = binomial(), data = df.lin.log %>% filter(case == '1) 0.5 threshold ok'))
> logistic2 <- glm(y ~ x, family = binomial(), data = df.lin.log %>% filter(case != '1) 0.5 threshold ok'))
> 
> lgrid <- data.frame(x = seq(from = 0, to = 20, length.out = 100))
> lgrid$y1_pred <- predict(logistic1, newdata = lgrid, type = 'response')
> lgrid$y2_pred <- predict(logistic2, newdata = lgrid, type = 'response')
> 
> lgrid.m <- data.frame(lgrid %>% pivot_longer(cols = c(y1_pred, y2_pred), names_to = 'variable', cols_vary = "slowest"))
> colnames(lgrid.m) <- c("x", "case", "value")
> 
> lgrid.m <- lgrid.m %>% 
>   mutate(case = ifelse(case == 'y1_pred', '1) 0.5 threshold ok', '2) 0.5 threshold ok as well'))
> df.lin.log <- df.lin.log %>% 
>   mutate(case = ifelse(case == '2) 0.5 threshold not ok', '2) 0.5 threshold ok as well', case))
> 
> df.lin.log %>% 
>   ggplot(aes(x=x, y=y)) +
>   geom_line(aes(x=x, y=value), data = lgrid.m, color = 'blue', lwd = 1) +
>   geom_point(position = position_jitter(width = 0, height = 0.02)) +
>   my_theme() +
>   scale_y_continuous('Tumor class', breaks = c(0, 0.5, 1), labels = c('benign tumor', '0.5',  'malignant tumor'), limits = c(-0.1,1.3)) +
>   scale_x_continuous('Tumor size') +
>   facet_grid(. ~ case) +
>   geom_hline(yintercept=0.5, linetype = 3)
> ```

![[Pasted image 20231230132159.png|center]] Figure 5.7 : 로지스틱 회귀 모델은 tumor size에 따라 악성과 양성 간 정확한 결정 경계(decision boundary)를 찾아냅니다. 

#### 3. Interpretation

로지스틱 회귀가 내뱉는 값은 0과 1 사이의 클래스 확률이기 때문에 가중치의 해석은 선형 회귀와는 달라집니다. 가중치 (weights)는 클래스 확률에 선형적인 영향을 미치지 않습니다. 가중합 값은 로지스틱 함수를 통해 확률로 변환됩니다. 따라서 해석을 위해서는 방정식을 변형시켜서 선형식만을 수식의 우변에 위치시켜야 합니다: 
$$
\ln \left(\frac{P(y=1)}{1-P(y=1)}\right)=\log \left(\frac{P(y=1)}{P(y=0)}\right)=\beta_0+\beta_1 x_1+\ldots+\beta_p x_p
$$
$ln$ 함수 내부에 있는 항을 Odds 라고 부르며, 이를 log 로 감싼 것을 Log odds 라고 부릅니다. 위 수식은 로지스틱 회귀 모델이 Log Odds 에 대한 선형 모델임을 보여줍니다. 
$$
\frac{P(y=1)}{1-P(y=1)}=o d d s=\exp \left(\beta_0+\beta_1 x_1+\ldots+\beta_p x_p\right)
$$
$$
\frac{o d d s_{x_j+1}}{o d d s x_j}=\frac{\exp \left(\beta_0+\beta_1 x_1+\ldots+\beta_j\left(x_j+1\right)+\ldots+\beta_p x_p\right)}{\exp \left(\beta_0+\beta_1 x_1+\ldots+\beta_j x_j+\ldots+\beta_p x_p\right)}
$$
$$
\frac{o d d s_{x_j+1}}{o d d s x_j}=\exp \left(\beta_j\left(x_j+1\right)-\beta_j x_j\right)=\exp \left(\beta_j\right)
$$


$j$ 번째 피쳐 값을 한 단위 변경하면 Odds ratio 가 $e^{\beta_j}$ 배만큼 바뀝니다. 또는 $x_j$ 가 한 단위 변하면 해당 가중치 만큼 Log Odds의 비율이 증가한다고 해석할 수도 있습니다. 

아래는 여러 피쳐 유형에 따른 로지스틱 회귀 모델에 대한 해석 방법입니다:
- Numerical feature: $x_j$ 를 한 단위 증가시키면 추정 Odds 가 $e^{\beta_j}$ 만큼 변합니다.
- Binary categorical feature: 피쳐 값 중 하나는 참조(reference) 수준입니다. $x_j$를 참조 수준에서 다른 수준으로 바꾸면 추정 Odds 가 $e^{\beta_j}$ 만큼 변합니다.
- Multi-class categorical feature: one-hot encoding
- Intercept $\beta_0$ : 모든 숫자형 피쳐가 0이고, 범주형 피쳐가 참조 수준인 경우의 추정 Odds 는 $e^{\beta_0}$ 입니다.

#### 4. Example

로지스틱 회귀 모델을 사용하여 몇 가지 위험 요소(risk factors)를 기반으로 자궁경부암 발병 여부를 예측할 수 있습니다. 아래 표에는 추정된 가중치, Odds ratio, 추정치의 표준오차 값이 나와 있습니다: 

> [!note]- code fold
> ```r
> neat_curvical_names <- c('Intercept', 'Hormonal contraceptives y/n',
>                          'Smokes y/n', 'Num. of pregnancies',
>                          'Num. of diagnosed STDs',
>                          'Intrauterine device y/n')
> mod <- glm(Biopsy ~ Hormonal.Contraceptives + Smokes + Num.of.pregnancies +STDs..Number.of.diagnosis + IUD,
>            data = cervical, family = binomial())
> coef.table <- tidy(mod) %>% 
>   select(` ` = term, Estimate = estimate, `Std. Error` = std.error) %>% 
>   mutate(`Odds ratio` = exp(round(Estimate, 2)), 
>          ` ` = neat_curvical_names)
> 
> 
> coef.table %>% 
>   mutate(across(where(is.numeric), ~round(.x, 2))) %>% 
>   relocate(`Odds ratio`, .after = Estimate)
> ```

|                             | Estimate | Odds ratio | Std. Error |
|:----------------------------|:---------|:-----------|:-----------|
| Intercept                   |    -2.91 |       0.05 |       0.32 |
| Hormonal contraceptives y/n |    -0.12 |       0.89 |        0.3 |
| Smokes y/n                  |     0.26 |        1.3 |       0.37 |
| Num. of pregnancies         |     0.04 |       1.04 |        0.1 |
| Num. of diagnosed STDs      |     0.82 |       2.27 |       0.33 |
| Intrauterine device y/n     |     0.62 |       1.86 |        0.4 |  


- Numeric features : 
	- 성병 진단 횟수(`STDs..Number.of.diagnosis`)가 증가하면 다른 모든 피쳐가 동일할 때 암에 걸리는 것에 대한 오즈비가 2.27배 입니다. 이는 상관관계일 뿐이지 인과성을 의미하지 않습니다.
- Categrical features :
	- 호르몬 피임약 복용 여부(`Hormonal.Contraceptives`)의 경우, 호르몬 피임약을 복용하는 여성의 경우 다른 모든 피쳐들이 동일할 때 복용하지 않는 여성에 비해 암에 걸리는 것에 대한 오즈비가 0.89배 입니다. 

로지스틱 회귀 모델은 선형 모델과 마찬가지로 해석시에 '다른 모든 피쳐들이 동일할 때'라는 가저이 붙습니다.

#### 5. Advantages & Disadvantages

선형 회귀 모형의 여러 장단점들이 로지스틱 회귀 모델에서도 적용됩니다. 널리 사용되고 있지만 상호작용 항을 수동으로 추가해야 한다는 표현력의 제한점이 있으며, 예측 성능이 떨어진다는 단점이 있습니다. 

또한 가중치의 해석이 가법적이지 않고 곱셈이기 때문에 좀 더 까다롭습니다.

장점으로는 로지스틱 모델은 클래스 확률을 제공한다는 것입니다. 또한 Multi-class 로 확장이 가능합니다.


---

## 05-03. GLM, GAM and more

선형 회귀 모델의 가장 큰 장점이자 약점은 모델의 예측이 피쳐들의 가중치 합으로 모델링 된다는 것입니다. 또한 선형 모델에는 여러 가정들이 수반됩니다. 현실 문제의 대부분 이 가정들이 위반되기 때문에 선형 회귀 모델은 성능이 낮습니다. 

선형회귀 모형의 수식이 아래와 같았습니다:
$$
y=\beta_0+\beta_1 x_1+\ldots+\beta_p x_p+\epsilon
$$
이는 개별 인스턴스의 output 인 $y$ 를 오차가 있는 $p$ 개의 피쳐들의 가중치 합으로 표현할 수 있음을 가정하는 것입니다. 그렇기 때문에 피쳐가 한 단위 증가하면 예측 결과의 증/감을 하나의 숫자로 압축할 수 있는 feature effect 를 통한 해석을 얻을 수 있습니다. 

하지만 단순한 가중합은 현실의 예측 문제에 너무나도 제한적입니다. 이 섹션에서는 고전적인 선형 회귀 모델의 3가지 문제와 이를 해결하는 방법에 대해 소개할 것입니다. 더 다양한 문제점이 있지만 아래 3가지 그림에 한하여 다루도록 하겠습니다:

> [!note]- code fold
> ```r
> ###### For the GLM
> n <- 10000
> df <- data.frame(x = c(rnorm(n), rexp(n, rate = 0.5)),
>                  dist = rep(c("Gaussian", "Definitely Not Gaussian"), each = n))
> df <- df %>% 
>   mutate(dist = relevel(factor(dist), ref = "Gaussian"))
> p.glm <- df %>% 
>   ggplot() +
>   geom_density(aes(x = x)) +
>   facet_grid(. ~ dist, scales = 'free') +
>   theme_blank
> ###### For the interaction
> df <- data.frame(x1 = seq(-3, 3, length.out = n),
>                  x2 = sample(c(1,2), size = n, replace = T))
> df <- df %>% 
>   mutate(y = 3+5*x1+(2-8*x1)*(x2==2),
>          interaction = "Interaction")
> df2 <- df %>% 
>   mutate(y = 3+5*x1+0.5*(-8*x1)+2*(x2==2),
>          interaction = "No Interaction")
> 
> df <- rbind(df, df2)
> df <- df %>% 
>   mutate(interaction = relevel(factor(interaction), ref = "No Interaction"),
>          x2 = factor(x2))
> 
> p.interaction <- df %>% ggplot() +
>   geom_line(aes(x=x1, y=y, group=x2, lty=x2)) +
>   facet_grid(.~interaction) +
>   theme_blank
> 
> ###### For the gam
> df <- data.frame(x = seq(0, 10, length.out=200))
> df <- df %>% 
>   mutate(y = 5+2*x,
>          type = "Linear")
> df2 <- df %>% 
>   mutate(y = 3+2*x+3*sin(x),
>          type = "Nonlinear")
> df <- rbind(df, df2)
> 
> p.gam <- df %>% ggplot() +
>   geom_line(aes(x=x, y=y)) +
>   facet_grid(.~type) +
>   theme_blank
> 
> gridExtra::grid.arrange(p.glm, p.interaction, p.gam)
> ```

![[Pasted image 20231230141757.png|center]] Figure 5.8 : 왼쪽(선형 모형의 3가지 가정): features 가 주어졌을 때 Outcome의 가우시안 분포, 가산성(=상호작용 없음), 선형성. 현실(오른쪽)은 이러한 가정을 따르지 않습니다. 가우시안 분포가 아닐 수 있으며, 상호작용이 존재하며, 비선형적인 관계를 가질 수 있습니다.

이러한 문제들의 해결책은 아래와 같습니다.
- **Problem** : 피쳐들이 주어졌을 때 target $y$ 의 분포가 가우시안 분포가 아닐 경우
- **Example** : 특정한 날에 자전거를 몇 분 동안 탈것인지 예측하려는 경우를 고려해보겠습니다. 요일, 날씨 등을 피쳐로 사용하겠습니다. 선형 모델을 사용하려는 경우 0분 아래인 음수 minute을 갖는 가우시안 분포를 가졍하므로 이를 이용해 예측하는 경우 음수이거나 확률 예측치가 1보다 큰 경우가 발생할 수 있습니다.
- **Solution** : Genearlized Linear Models; GLMs

- **Problem** : The features interact.
- **Example** : 가벼운 비는 자전거를 타려는 욕구에 부정적인 영향을 미칩니다. 하지만 여름철 출퇴근 시간대에 비가 오면 남들은 모두 집에 있을거라는 생각에 홀로 자전거를 탈 수 있다는 생각에 반가울 수 있습니다. 이것은 시간과 날씨 사이의 상호작용으로 단순한 덧셈 모델로는 포착할 수 없습니다.
- **Solution** : Adding interactions Manually

- **Problem** : The true relationship b/w the features & $y$ is not linear.
- **Example** : 섭씨 0 ~ 25도 사이에서는 기온(`temp`)이 자전거를 타고 싶은 욕구에 선형적인 영향을 미칠 수 있지만, 이는 0도에서 1도로 기온이 상승하는 것이 20도에서 21도로 상승하는 것과 동일한 증감을 나타냅니다. 하지만 기온이 높아지면 자전거를 타려는 의욕이 떨어지고 감소할 수도 있습니다. 이는 비선형 관계를 의미합니다.
- **Solution** : Generalized Additive Models; GAMs - transformation of features


#### 1. Non-Gaussian Outcome: GLMs

선형 회귀 모델에서는 피쳐들이 주어졌을 때의 outcome 이 가우시안 분포를 따른다는 가정을 필수로 합니다. 하지만 현실 문제에서는 이 가정이 위배되는 경우가 매우 많습니다. 이럴 경우에는 일반화 선형 회귀 모형(GLM)을 사용하여 확장할 수 있습니다. 모든 GLM의 핵심 개념은 다음과 같습니다: 피쳐들의 가중치 합계는 유지하되, Gaussian 이외의 outcome 분포를 허용하고 이 분포의 expected mean 과 가중치 합계를 비선형 함수를 통해 연결하는 것입니다. 예를 들자면, 로지스틱 회귀 모델은 outcome 에 대해서 Bernoulli 분포를 가정하고 로지스틱 함수를 사용해서 기대값과 가중치 합계를 연결합니다. 

GLM 은 outcome 의 유형에 따라 유연하게 선택할 수 있는 link function $g$ 를 사용해서 피쳐의 가중치 합을 가정된 분포의 기댓값과 수학적으로 연결합니다: 
$$
g\left(E_Y(y \mid x)\right)=\beta_0+\beta_1 x_1+\ldots \beta_p x_p
$$
이러한 GLM은 3가지 구성요소로 이루어져 있습니다: 1) link function $g$, 2) 가중치 합 $X^T\beta$ (때로는 선형 예측치; linear predictor라고 부름), 3) 기대값 $E_Y$를 정의하는 지수족으로 부터의 확률 분포.

위키백과에는 지수족에 포함되는 분포에 대한 목록이 작성되어 있습니다: [지수족 in wikipedia](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions). 이 리스트에 있는 모든 분포를 GLM 에서 사용 가능합니다. Outcome 이 무엇인가의 count 를 나타내면 Poisson 분포가 적절할 것이고, outcome 이 항상 양수인 경우 Exponential 분포가 적절할 것입니다. 

GLM 에서 특수한 경우로 고전적인 선형 모델을 고려해 보겠습니다. 고전적인 선형 모델에서 Gaussian 분포의 link function 은 identity 함수입니다. 

GLM 프레임워크는 해당 분포의 평균과 가중치 합계가 link function으로 연결되는 개념으로 일반화 됩니다. 특정한 날에 마신 커시의 수 같은 $y$ 는 어떤 것의 count 이므로 Poisson 분포와 자연 로그라는 link function 으로 GLM 모델링을 할 수 있습니다: 
$$
\ln \left(E_Y(y \mid x)\right)=x^T \beta
$$
로지스틱 회귀 모델은 Bernoulli 분포를 가정하고 logit 함수를 link function으로 사용하는 GLM 모델입니다. 로지스틱 회귀에서 사용하는 이항분포의 평균은 $y$ 가 1일 확률입니다: 
$$
x^T \beta=\ln \left(\frac{E_Y(y \mid x)}{1-E_Y(y \mid x)}\right)=\ln \left(\frac{P(y=1 \mid x)}{1-P(y=1 \mid x)}\right)
$$
가정한 분포의 기대값을 좌변에 두고 방정식으로 풀면 로지스틱 회귀 공식과 같아집니다: 
$$
P(y=1)=\frac{1}{1+\exp \left(-x^T \beta\right)}
$$
지수족의 각 분포에는 수학적으로 유도가 가능한 canonical link function이 있습니다. GLM 프레임워크에서는 분포와 link function의 선택은 독립적으로 이루어지기 때문에 항상 옳은 link function 은 존재하지 않습니다. 어떤 분포에서는 canonical link function 이 유효하지 않은 값을 도출하기도 합니다. 따라서 link function 의 선택은 분포의 영역을 고려해서 진행하면 됩니다. 

##### Example

커피를 마시는 행동에 대한 데이터 셋을 시뮬레이션을 통해 생성해보겠습니다. 매일 커피를 마시는 행동에 대한 데이터를 수집했다고 가정해 보겠습니다. 커피를 좋아하지 않으면 차를 마신다고 하겠습니다. 커피 잔 수와 함께 스트레스 수준을 1 ~ 10 까지, 전날 잠을 얼마나 잘 잤는지를 1 ~ 10 까지, 그날 일을 해야 했는지 여부를 기록합니다. 목표는 스트레스, 수면, 업무 라는 피쳐들을 고려해서 커피 소비량을 예측하는 것입니다. 200일 동안의 데이터를 생성했습니다. 스트레스와 수면은 1 ~ 10 사이에서 균일하게, 업무 여부는 50:50 확률로 생성했습니다. 그런 다음 매일의 커피 소비량을 Poisson 분포에서 추출하여 $\lambda$ 를 수면, 스트레스, 업무 라는 피쳐들로 모델링 했습니다. 

우선 target 의 분포, 즉 주어진 날의 커피 소비량을 살펴 보겠습니다:

> [!note]- code fold
> ```r
> n <- 200
> df <- data.frame(stress = runif(n=n, min = 1, max = 10),
>                  sleep = runif(n = n, min = 1, max = 10),
>                  work = sample(c("YES", "NO"), size = n, replace = T))
> df <- df %>% 
>   mutate(lambda = exp(1*stress/10 - 2 * (sleep-5)/10 - 1*(work=="NO")))
> 
> df$y <- rpois(lambda = df$lambda, n = n)
> df %>% str()
> df %>% 
>   count(y) %>% 
>   ggplot() +
>   geom_col(aes(x = y, y = n), fill = default_color, width = 0.3) +
>   scale_x_discrete("Number of coffees on a given day") +
>   scale_y_continuous("Number of days")
> ```

![[Pasted image 20231230152101.png|center]] Figure 5.9 : 200일 일일 커비 판매량 분포를 시뮬레이션으로 생성했습니다.

200일 중 80일은 커피를 전혀 마시지 않았고, 가장 심한 날에는 7잔을 마셨습니다. 순진하게 선형 모델을 사용해 수면 수준, 스트레스 수준 및 업무 여부를 피쳐로하여 커피 마신 횟수를 예측해 보겠습니다. 함부로 가우시안 분포를 가정하며 어떤 문제가 발생할까요? 잘못된 분포 갖어은 추정치, 특히 가중치의 신뢰구간을 쓸모 없게 만듭니다. 특히 아래와 같이 예측이 실제 outcome의 domain과 일치하지 않게 된다는 것입니다. 

> [!note]- code fold
> ```r
> df <- df %>% select(-lambda)
> mod.gaus <- glm(y ~ ., data = df, x = T)
> pred.gauss <- data.frame(pred = predict(mod.gaus), actual = df$y)
> pred.gauss %>% 
>   ggplot() +
>   geom_histogram(aes(x = pred), fill = default_color, color = 'black') +
>   scale_x_continuous("Predicted number of coffees") + 
>   scale_y_continuous("Frequency")
> ```

![[Pasted image 20231230152739.png|center]]Figure 5.10 : 스트레스 정도, 수면 시간, 업무에 따른 커피 소비량에 대한 예측입니다. 선형 모델이 음수값을 예측해 버렸습니다. 

선형 모델은 쓸모가 없는 음수 값을 예측했습니다. 이 문제를 GLM으로 해결할 수 있습니다. 첫 번째로 가능한 것은 가정 분포를 가우시안 분포로 유지하고, Identity 대신 log-link function을 사용하여 예측 값을 항상 양수로 유도하는 것입니다. 더 좋은 방법은 데이터 생성시 사용했던 것과 마찬가지로 Poisson 분포를 가정하고 log-link function을 사용하는 것입니다. Outcome이 count이므로 이에 대한 canonical link가 자동으로 선택됩니다. 

> [!note]- code fold
> ```r
> mod.pois <- glm(y ~ ., data = df, x = T, family = poisson(link = 'log'))
> pred.pois <- data.frame(pred = predict(mod.pois, type = 'response'),
>                         actual = df$y)
> pred.pois %>% 
>   ggplot() +
>   geom_histogram(aes(x = pred), fill = default_color, color = 'black') + 
>   scale_x_continuous("Predicted number of coffees") + 
>   scale_y_continuous("Frequency")
> ```

![[Pasted image 20231230153633.png|center]] Figure 5.11 : Poisson 가정과 log-link를 사용한 GLM이 이 데이터셋에 적합해 보입니다.

커피 소비량에 대해 음의 예측값이 없으니 더 좋아 보입니다. 

##### Interpretation of GLM weights

Link function 과 함께 가정된 분포에 따라 추정 가중치에 대한 해석 방식이 달라집니다. 위 예제에서는 expected outcome 과 features 간 아래와 같은 관계가 있습니다: 
$$
\ln (E(\operatorname{coffee} \mid \operatorname{str}, \mathrm{slp}, \mathrm{wrk}))=\beta_0+\beta_{\mathrm{str}} x_{\mathrm{str}}+\beta_{\mathrm{slp}} x_{\mathrm{slp}}+\beta_{\mathrm{wrk}} x_{\mathrm{wrk}}
$$
가중치를 해석하기 위해서 아래와 같이 변환해야 합니다: 
$$
E(\text { coffee } \mid \operatorname{str}, \mathrm{slp}, \mathrm{wrk})=\exp \left(\beta_0+\beta_{\mathrm{str}} x_{\mathrm{str}}+\beta_{\mathrm{slp}} x_{\mathrm{slp}}+\beta_{\mathrm{wrk}} x_{\mathrm{wrk}}\right)
$$
모든 가중치에 지수 함수가 포함되어 있으므로 효과에 대한 해석은 곱셈이 됩니다. 아래 표는 위 예제에서의 가중치와 가중치의 95% 신뢰구간입니다: 

> [!note]- code fold
> ```r
> cc <- tidy(mod.pois) %>% select(term, beta = estimate, var.beta = std.error) %>% 
>   mutate(exp.beta = exp(beta)) %>% 
>   select(beta, exp.beta)
> cc
> cc <- cc %>% cbind(exp(confint(mod.pois)))
> cc %>% 
>   rename(weight = beta) %>% 
>   mutate(across(where(is.numeric), ~round(.x, 2))) %>% 
>   mutate(`exp(weight) [2.5%, 97.5%]` = paste0(exp.beta, " [", `2.5 %`,", ", `97.5 %`, "]")) %>% 
>   select(c(1, 5)) 
> ```

|             | weight | exp(weight) [2.5%, 97.5%] |
|:------------|:-------|:--------------------------|
| (Intercept) |  -0.33 |         0.72 [0.46, 1.12] |
| stress      |   0.12 |         1.13 [1.08, 1.19] |
| sleep       |  -0.18 |         0.83 [0.79, 0.88] |
| workYES     |   1.14 |         3.13 [2.38, 4.17] |  

스트레스 수준이 1점 증가하면 예상 커피 수에 1.13이 곱해집니다. 수면의 질이 1점 증가하면 예상 커피 수에 0.83의 계수가 곱해집니다. 근무일의 예상 커피 소비량은 휴일 대비 평균적으로 3.13 배입니다. 요약하자면, 스트레스가 많고 수면이 적으며 업무량이 많을수록 커피를 더 많이 마신다는 것입니다. 


#### 2. Interactions

피쳐간 상호작용이 있는 상황을 고려해 보겠습니다. 자전거 대여 수를 예측하려는 상황에서는 기온과 근무일 여부 간 상호작용이 있을 수 있습니다. 어떤 사람들은 일하는 날에는 무슨 일이 있어도 자전거를 대여해서 출근하기 때문에 기온이 자전거 대여 수에 큰 영향을 미치지 않을 수 있습니다. 쉬는 날에는 많은 사람들이 자전거를 타지만 날씨가 충분히 따뜻할 때만 그럴 수도 있습니다. 따라서 자전거 대여 수 예측에서는 기온과 근무일 간 상호작용이 예상됩니다. 

선형 모델에 상호작용을 포함시키기 위해서는 fitting 하기 전에 feature matrix 에 피쳐 간 상호작용을 나태내는 칼럼을 추가하고 fitting 하면 됩니다. 근무일 변수는 근무일이 참조 수준이라고 가정하고 쉬는 날에는 0이고, 근물 일에는 기온 값을 갖는 새로운 피쳐를 추가해 보겠습니다: 

- 근무 여부와 기온 변수의 일부만을 우선 살펴 보겠습니다: 

```r
x = data.frame(work = c("Y", "N", "N", "Y"), temp = c(25, 12, 30, 5))
x
```

| work | temp |
|:-----|:-----|
| Y    |   25 |
| N    |   12 |
| N    |   30 |
| Y    |    5 |  

- 선형 모델에서 사용하는 model matrix 형태로 보겠습니다: 

> [!note]- code fold
> ```r
> #### data-frame-lm-no-interaction
> mod <- lm(1:4 ~ ., data=x)
> model.tab <- model.matrix(mod) %>% data.frame()
> colnames(model.tab)[1] <- "Intercept"
> model.tab
> ```

| Intercept | workY | temp |
|:----------|:------|:-----|
|         1 |     1 |   25 |
|         1 |     0 |   12 |
|         1 |     0 |   30 |
|         1 |     1 |    5 |  

첫 번째 칼럼은 절편 항이고, 두 번째는 범주형 피쳐를 인코딩하고 있는데 참조 수준은 0입니다. 세 번째 칼럼은 온도를 나타냅니다.  이제 기온과 근무일 간 상호작용을 고려한 항을 추가해 보겠습니다: 

> [!note]- code fold
> ```r
> #### data-frame-lm
> mod <- lm(1:4 ~ work * temp, data = x)
> model.tab <- data.frame(model.matrix(mod))
> colnames(model.tab)[1] <- "Intercept"
> model.tab
> ```

| Intercept | workY | temp | workY.temp |
|:----------|:------|:-----|:-----------|
|         1 |     1 |   25 |         25 |
|         1 |     0 |   12 |          0 |
|         1 |     0 |   30 |          0 |
|         1 |     1 |    5 |          5 |  
새로운 칼럼 `workY.temp` 는 근무일과 온도 피쳐 간 상호작용을 포착합니다. 상호작용 항을 포함하면 numeric feature 의 effect/slope 가 각 범주에서 다른 값을 갖도록 허용합니다. 

범주형 피쳐 간의 상호작용도 비슷하게 작동합니다. 아래는 근무일과 날씨 특징을 포함한 데이터입니다: 

```r
#### data-frame-lm-cat
x = data.frame(work = c("Y", "N", "N", "Y"), wthr = c("2", "0", "1", "2"))
x
```

| work | temp |
|:-----|:-----|
| Y    |   25 |
| N    |   12 |
| N    |   30 |
| Y    |    5 |  

상호작용 항을 추가하면 아래와 같이 됩니다: 

> [!note]- code fold
> ```r
> #### data-frame-lm-cat2
> mod = lm(1:4 ~ work * wthr, data = x)
> model.tab = data.frame(model.matrix(mod))
> colnames(model.tab)[1] = c("Intercept")
> model.tab
> ```

| Intercept | workY | wthr1 | wthr2 | workY.wthr1 | workY.wthr2 |
|:----------|:------|:------|:------|:------------|:------------|
|         1 |     1 |     0 |     1 |           0 |           1 |
|         1 |     0 |     0 |     0 |           0 |           0 |
|         1 |     0 |     1 |     0 |           0 |           0 |
|         1 |     1 |     0 |     1 |           0 |           1 |  

첫 번째 칼럼은 절편을 추정하고, 두 번째 칼럼은 근무일 변수에 대한 인코딩입니다. 세 번째와 네 번째 칼럼은 날씨 특징을 위한 것으로 3가지 수준에 대해서 dummy encoding 된 것입니다. 나머지 칼럼 두 개는 상호작용을 포착합니다. 

상호작용 항을 자동으로 감지하고 추가하는 방법도 있습니다. 이 방법은 추후에 RuleFit 챕터에서 다룰 것입니다. RuleFit 알고리즘은 먼저 상호작용 조건을 찾아낸 다음 상호작용을 포함한 선형 회귀 모델을 추정합니다.

##### Example

자전거 대여 수 예측 데이터셋을 활용하겠습니다. 여기서는 `temp`와 `workkingday` 간 상호작용을 고려하겠습니다 : 

> [!note]- code fold
> ```r
> #### example-lm-interaction
> X <- bike %>% select(all_of(bike.features.of.interest))
> y <- bike %>% pull(cnt)
> dat <- cbind(X, y)
> mod <- lm(y ~ . + temp * workingday, data = dat, x = T)
> lm_summary <- tidy(mod) %>% 
>   rename(Estimate = estimate, `Std. Error`=std.error, `t value` = statistic, `Pr(>|t|)` = p.value) %>% 
>   column_to_rownames(var = 'term')
> lm_summary_print <- lm_summary
> rownames(lm_summary_print) <- pretty_rownames(rownames(lm_summary_print))
> 
> rownames(lm_summary_print)[rownames(lm_summary_print) == "weathersitRAIN/SNOW/STORM"] = "weathersitRAIN/..."
> lm_summary_print %>% 
>   select(Weight = Estimate, `Std. Error`) %>% 
>   bind_cols(confint(mod)) %>% 
>   mutate(across(where(is.numeric), ~round(.x, 1)))
> ```

|                            | Weight  | Std. Error |   2.5 % | 97.5 % |
|:---------------------------|:--------|:-----------|:--------|:-------|
| (Intercept)                |  2185.8 |      250.2 |  1694.6 | 2677.1 |
| seasonSPRING               |   893.8 |      121.8 |   654.7 | 1132.9 |
| seasonSUMMER               |   137.1 |        161 |    -179 |  453.2 |
| seasonFALL                 |   426.5 |      110.3 |   209.9 |  643.2 |
| holidayHOLIDAY             |  -674.4 |      202.5 | -1071.9 | -276.9 |
| workingdayWORKING DAY      |   451.9 |      141.7 |   173.7 |  730.1 |
| weathersitMISTY            |  -382.1 |       87.2 |  -553.3 |   -211 |
| weathersitRAIN/...         | -1898.2 |      222.7 | -2335.4 |  -1461 |
| temp                       |   125.4 |        8.9 |     108 |  142.9 |
| hum                        |   -17.5 |        3.2 |   -23.7 |  -11.3 |
| windspeed                  |   -42.1 |        6.9 |   -55.5 |  -28.6 |
| days_since_2011            |     4.9 |        0.2 |     4.6 |    5.3 |
| workingdayWORKING DAY:temp |   -21.8 |        8.1 |   -37.7 |   -5.9 |  

추가된 상호작용 효과는 음수(-21.8)이며 신뢰구간에 0을 포함하지 않아 유의한 것으로 보입니다. 다만 서로 가까운 날짜 간에는 서로 독립이 아닐 가능성이 크기 때문에 해당 데이터는 i.i.d 가 아닙니다. 상호작용 항은 관련된 피쳐 가중치에 대한 해석을 변경합니다. `temp`와 `workingdayWORKING DAY:temp`의 가중치를 합산해서 예측치가 얼마나 변하는지를 확인해야 합니다. 

상호작용은 시각적으로 살펴보는 게 이해하기에 더 쉽습니다. 범주형 피쳐와 숫자형 피쳐 간 상호작용 항을 추가하면 온도에 대해 2개의 slope 를 살펴볼 수 있습니다. 일하지 않는날(`'NO WORKING DAY'`)에 대한 온도 기울기는 125.4 이고, `'WORKING DAY'`에 대한 기울기는 125.4-21.8=103.6 입니다. `temp=0`에서 `'WORKING DAY'` 라인의 절편은 절편항 + 근무일 효과인 2185.5 + 451.9 = 2637.7 가 됩니다. 

```r
interactions::interact_plot(mod, pred = "temp", modx = "workingday")
```

![[Pasted image 20231230164503.png|center]] Figure 5.12 : 선형 모델에서 온도와 근무일이 예측 자전거 수에 미치는 영향(상호작용 포함).


#### 3. Nonlinear Effects - GAMs

선형 모델에서 선형성이란 특정한 피쳐가 한 단위 증가되면 예측 결과에 대해서 항상 동일한 영향을 미친다는 것입니다. 온도라는 피쳐는 자전거 대여 수에 선형적인 영향을 미치지만, 어느 순간 평평해지고 심지어 고온에서는 부정적인 영향을 미칠 것입니다. 선형 모형은 이를 고려하지 않으며 단지 Euclidean distance를 최소화하는 최적의 선형 평면을 찾을 뿐입니다. 

아래와 같은 기법들을 활용해서 비선형적인 관계를 모델링할 수 있습니다: 

- 피쳐에 대한 단순 변환: e.g. logarithm
- 숫자형 피쳐의 범주화
- Generalized Additive Models; GAMs

3가지 각각에 대해서 자세히 다루기 전에, 이를 모두 한꺼번에 보여줄 수 있는 예제부터 다루겠습니다. 자전거 대여 데이터셋에서 `temp` 만 있는 선형 모델을 학습해서 자전저 대여 수를 예측해 보겠습니다. 아래 그림은 standard linear model, `temp`를 log 변환한 선형 모델, `temp` 를 이산화한 선형 모델, 회귀 스플라인을 사용한 선형 모델을 사용한 다음 추정된 기울기를 나타내고 있습니다:

> [!note]- code fold
> ```r
> mod.simple <- lm(cnt ~ temp, data = bike)
> bike.plt <- bike %>% 
>   mutate(pred.lm = predict(mod.simple),
>          log.temp = log(temp + 10))
> mod.log = lm(cnt ~ log.temp, data = bike.plt)
> bike.plt <- bike.plt %>% 
>   mutate(pred.log = predict(mod.log),
>          cat.temp = cut(temp, breaks = seq(min(temp), max(temp), length.out = 10), include.lowest = T))
> mod.cat <- lm(cnt ~ cat.temp, data = bike.plt)
> bike.plt <- bike.plt %>% 
>   mutate(pred.cat = predict(mod.cat))
> 
> mod.gam <- gam(cnt ~ s(temp), data = bike)
> bike.plt <- bike.plt %>% 
>   mutate(pred.gam = predict(mod.gam))
> bike.plt <- bike.plt %>% 
>   select(starts_with("pred."), temp, cnt) %>% 
>   pivot_longer(cols = starts_with("pred."), names_to = "variable", values_to = "value", cols_vary = "slowest")
> 
> model.type <- c(pred.lm = "Linear model",
>                 pred.log = "Linear model with log(temp + 10)",
>                 pred.cat = "Linear model with categorized temp",
>                 pred.gam = "GAM")
> 
> bike.plt %>% 
>   mutate(variable = factor(variable, levels = c("pred.lm", "pred.log", "pred.cat", "pred.gam"), 
>                            labels = model.type)) %>% 
>   ggplot() +
>   geom_point(aes(x = temp, y = cnt), size = 1, alpha = 0.3) +
>   geom_line(aes(x = temp, y = value), lwd = 1.2, color = 'blue') +
>   facet_wrap(~variable) +
>   scale_x_continuous("Temperature (temp)") + 
>   scale_y_continuous("(Predicted) Number of rented bikes")
> ```

![[Pasted image 20231230185638.png|center]] Figure 5.13 : `temp` 만을 사용해서 자전거 대여 수를 예측했습니다. (왼쪽 위) 선형 모델은 데이터에 잘 맞지 않습니다. log 변환을 적용할 수 있고, `temp` 를 10개로 이산화하거나, GAM를 사용하여 이를 해결할 수 있습니다. 

##### Feature transformation

자연 로그(log10)를 사용하여 `temp`를 변환하면 섭씨 온도가 10도 증가할 때 마다 선형 효과를 얻을 수 있습니다. 1도에서 10도로 변경하는 것은 0.1에서 1로 변형되는 것과 동일한 효과가 나타납니다. 다른 변환으로는 제곱근, 제곱 함수, 지수 변환 등이 있습니다. GLM에서 feature transformation을 사용할 경우 link function 에 맞게 해석을 진행해야 하므로 해석이 더 복잡해질 수 있습니다. 

##### Feature categorization

Nonliear effect 를 얻을 수 있는 또 다른 방법으로는 피쳐를 이산화하여 범주형으로 바꾸는 것입니다. 이 방식의 문제는 필요한 데이터의 양이 커야하며 과적합의 가능성이 높고 피쳐를 이산화하는 방법에 따라 결과가 달라질 수 있다는 점입니다. 

##### Generalized Additive Models ; GAMs

GAM 은 relationship 이 단순 가중치의 합계이어야 한다는 제한을 완화하고 대신 각 피쳐에 대한 임의의 함수들의 합으로 결과를 모델링하는 것입니다. 수학적인 GAM 의 관계는 아래와 같습니다: 
$$
g\left(E_Y(y \mid x)\right)=\beta_0+f_1\left(x_1\right)+f_2\left(x_2\right)+\ldots+f_p\left(x_p\right)
$$

GAM 의 핵심은 여전히 feature effects 의 합이지만 비선형 관계를 허용할 수 있습니다. 

가장 중요한 문제는 비선형 함수를 어떻게 학습하느냐 입니다. 이에 대한 해결책은 spline 입니다. 스플라인은 복잡한 함수의 근사치를 구하는 데 사용됩니다. 이러한 spline basis function 을 정의하는 방법은 여러 가지가 있습니다. 스플라인을 쉽게 이해하기 위해서 개별 basis function 을 시각화하고 데이터 행렬이 수정되는 방식을 살펴 보겠습니다. 예를 들어, 스플라인으로 `temp` 를 모델링하려면 데이터에서 `temp`를 제거하고 spline basis 를 나타내는 4개의 칼럼으로 대체합니다. 보통 더 많은 spline basis 가 필요하지만 설명을 위해 적은 수만 택했습니다. 이 새로운 spline basis 의 각 인스턴스 값은 해당 인스턴스의 `temp` 값에 따라 달라집니다. 그런 다음 GAM 은 모든 선형 효과와 함께 이 스플라인에 대한 가중치도 추정합니다. 또한 GAM 은 sparsity 를 도입할 수 있어서 가중치에 대한 페널티 항도 제공합니다. 그리고 곡선의 유연성/복접성을 제어할 수 있는 smoothness parameter 를 CV 를 통해 tuning  할 수 있습니다. 

`temp` 만을 사용하여 GAM 으로 자전거 수를 예측하는 예제에서 model matrix 는 아래와 같습니다: 

> [!note]- code fold
> ```r
> mod.gam <- gam(cnt ~ s(temp, k = 5), data = bike)
> model.matrix(mod.gam) %>% head() %>% 
>   round(2)
> ```

| (Intercept) | s(temp).1 | s(temp).2 | s(temp).3 | s(temp).4 |
|:------------|:----------|:----------|:----------|:----------|
|           1 |     -0.93 |     -0.14 |      0.21 |     -0.83 |
|           1 |     -0.83 |     -0.27 |      0.27 |     -0.72 |
|           1 |     -1.32 |      0.71 |     -0.39 |     -1.63 |
|           1 |     -1.32 |       0.7 |     -0.38 |     -1.61 |
|           1 |     -1.29 |      0.58 |     -0.26 |     -1.47 |
|           1 |     -1.32 |      0.68 |     -0.36 |     -1.59 |  

각 행은 개별 인스턴스(하루)를 나타냅니다. 각 spline 칼럼에는 피쳐인 `temp` 값에 대한 spline basis 함수 값이 들어갑니다. 아래 그림은 이러한 spline basis function 이 `temp`에 따라서 어떻게 구성되는지를 나타냅니다:

> [!note]- code fold
> ```r
> mm <- model.matrix(mod.gam) %>% data.frame()
> colnames(mm) <- dimnames(model.matrix(mod.gam))[[2]]
> mm <- mm %>% 
>   mutate(temp = bike$temp)
> mm2 <- mm %>% 
>   pivot_longer(cols = c(everything(), -temp), names_to = "variable", values_to = "value") %>% 
>   mutate(variable = factor(variable))
> 
> mm2 %>% 
>   filter(variable != "(Intercept)") %>% 
>   ggplot() +
>   geom_line(aes(x = temp, y = value)) +
>   facet_wrap(~variable) +
>   scale_x_continuous("Temperature") +
>   scale_y_continuous("Value of spline basis feature")
> ```

![[Pasted image 20231230192811.png|center]] Figure 5.14 : 온도 효과를 매끄럽게 모델링하기 위해 4개의 스플라인 기저를 사용했습니다. 인스턴스의 온도가 30도인 경우 1st 스플라인의 값은 1, 2nd 스플라인은 0.8, 3rd 는 -0.8, 4th 는 1.7 입니다. 

GAM 으로 구한 각 spline basis function에 대한 가중치는 아래와 같습니다: 

> [!note]- code fold
> ```r
> data.frame(weight = coef(mod.gam) %>% round(2)) %>% 
>   rownames_to_column()
> ```

|             | weight  |
|:------------|:--------|
| (Intercept) | 4504.35 |
| s(temp).1   |  989.34 |
| s(temp).2   |  740.08 |
| s(temp).3   | 2309.84 |
| s(temp).4   |  558.27 |  

추정된 가중치를 부여한 spline basis functions 의 합으로 인한 곡선은 아래와 같습니다 : 

```r
plot(mod.gam)
```

![[Pasted image 20231230193637.png|center]] Figure 5.15 : 자전거 대여 수 예측을 위해 `temp` 만을 feature로 활용했을 때의 GAM feature effects.

평활 효과의 해석을 위해서는 적합 곡선을 시각적으로 확인해야 합니다. spline 은 보통 평균 예측값을 중심으로 형성되므로 곡선의 한 점이 평균 예측값과의 차이입니다. 예를 들어, 섭씨 0도에서 예측된 자전거 수는 평균 예측보다 3000대 적습니다. 

#### 4. Advantages

많은 ML 모델의 (black box로 인한) 불투명성은 1) 너무나 많은 피쳐의 사용으로 인한 sparsity의 부족, 2) 비선형 관계를 나타내는 피쳐들, 3) 피쳐 간 상호작용 등을 모델링하는 데서 비롯됩니다. 선형 모델이 해석 가능성은 높지만 현실에 적합하지 않은 경우가 많은데, 이 섹션에서 설명하는 확장들은 해석 가능성을 일부 유지하면서 보다 flexible 한 모델을 적합할 수 있습니다. 

#### 5. Disadvantages

기본적인 선형 모델에서 확장/수정이 가해지면 해석 가능성이 떨어집니다. Identity 가 아닌 link function 은 해석을 복잡하게 만들고, 상호작용도 마찬가지 입니다. 또한 nonlinear feature effect 는 직관적이지 않고 더 이상 단일 수치로 요약할 수 없습니다. 

또한 GLM, GAM 과 같은 것은 데이터의 생성 프로세스에 대한 가정에 의존합니다. 이러한 가정이 위반되면 가중치 해석이 유효하지 않습니다. 

RF 나 GBM 과 같은 tree-based 앙상블의 성능은 많은 경우 선형 모델보다 낫습니다. 

#### 7. Further Extensions

- 데이터가 i.i.d 를 위반할 경우: 
	- **mixed models** 
	- **generalized estimating equations**
- 모델에 이분산의 문제가 발생 : 
	- **robust regression**
- 심각한 이상치 문제 발생 : 
	- **robust regression**
- 이벤트 발생까지의 시간을 예측하고 싶은 경우: 
	- **parametric survival models**
	- **cox regression**
	- **survival analysis**
- Outcome 이 범주형인 경우: 
	- **multinomial regressions**
- Ordered categories 를 예측하려는 경우 : 
	- **proportional odds models**
- Outcome 이 count인 경우 : 
	- **poisson regression**
	- 0 값을 포함하는 경우 : 
		- **zero-inflated poisson regression**
		- hurdle model
- 올바른 인과 관계 도출을 위해 모델에 어떤 피쳐들이 포함되어야 하는지 알고 싶을 경우: 
	- **causal inference**
	- **mediation analysis**
- 결측치가 존재할 경우 : 
	- **multiple imputataion**
- 모델에 대한 사전 지식을 통합하고 싶을 경우 : 
	- **bayesian inference**

---

## 05-04. Decision Tree

선형 회귀 및 로지스틱 회귀 모델은 피쳐와 outcome 간 비선형적인 관계가 존재하거나 피쳐간 상호작용이 있는 상황에서는 실패합니다. Tree-based 모델은 피쳐의 cutoff 값에 따라 데이터를 여러 번 분할(split) 합니다. 분할을 통해 데이터셋을 여러 subset 으로 만들고, 각 인스턴스는 하나의 subset에 속하게 됩니다. 최종 하위 집합을 terminal / leaf node 라고 부르고, 중간 하위 집합을 internal nodes / split nodes 라고 부릅니다. 각 leaf node 의 outcome 을 예측하기 위해 해당 노드의 학습 데이터의 평균 결과(average outcome)가 사용됩니다. 

다양한 알고리즘이 Tree 구조를 사용하고 있습니다.  노드 당 분할 수 , 분할 컷오프를 찾는 기준, 분할 중지 시기, leaf node 내 모델 추정법 등 다양한 tree 구조가 있습니다. 분류 및 회귀 트리(CART) 알고리즘을 다뤄 보겠습니다.

> [!note]- code fold
> ```r
> set.seed(42)
> n <- 100
> dat_sim <- data.frame(feature_x1 = rep(c(3,3,4,4), times = n), 
>                       feature_x2 = rep(c(1,2,2,2), times = n),
>                       y = rep(c(1,2,3,4), times = n))
> dat_sim <- dat_sim[sample(1:nrow(dat_sim), size = 0.9 * nrow(dat_sim)), ]
> dat_sim$y <- dat_sim$y + rnorm(nrow(dat_sim), sd = 0.2)
> ct <- ctree(y ~ feature_x1 + feature_x2, data = dat_sim)
> ct %>% 
>   plot(inner_panel = node_inner(ct, pval = F, id = F),
>        terminal_panel = node_boxplot(ct, id = F))
> ```

![[Pasted image 20231230202534.png|center]] Figure 5.16 : 인위적인 데이터를 사용해 적합한 DT 입니다. x1에 대해서 3보다 큰 값을 갖는 인스턴스는 5번 node 에 배치됩니다. 다른 모든 인스턴스는 x2의 값이 1을 초과하는지 여부에 따라 3번 node 또는 4번 node 에 배치됩니다. 

아래 수식은 outcome $y$ 와 피쳐 $x$ 간 관계를 설명합니다: 
$$
\hat{y}=\hat{f}(x)=\sum_{m=1}^M c_m I\left\{x \in R_m\right\}
$$
각 인스턴스는 정확히 하나의 leaf node(=subset $R_m$)에 속합니다. $I_{\{x\in R_m\}}$ 는 $x$ 가 subset $R_m$ 에 속하면 1, 그렇지 않으면 0을 갖는 지시함수입니다. 인스턴스가 leaf node $R_l$ 에 속하면 예측 결과(predicted outcome)는 $\hat{y}=c_l$ 이며, 여기서 $c_l$ 은 leaf node $R_l$ 에 있는 모든 training instances 의 평균입니다. 

그렇다면 하위집합(subset)은 어떻게 생성되는 것일까요? CART는 regression 의 경우 피쳐를 이용해 $y$의 분산을 최소화하는 cutoff 지점을, classification 의 경우 $y$ 클래스 분포의 gini index 최소화하는 cutoff 를 결정합니다. 분산은 node 내 $y$ 값이 평균을 중심으로 얼마나 퍼져있는지를 나타내고, gini index 는 node 가 얼마나 "불순한지(impure)"를 나타냅니다. 모든 클래스의 빈도가 동일하면 해당 노드는 불순한 것이고, 단일 클래스로 구성되어 있으면 순수한 것입니다. 분산과 gini index 모두 노드 내 인스턴스 $y$ 들이 비슷한 값을 가질 때 작아집니다. 결과적으로 최적의 cutoff 지점은 target 과 관련해 subset 을 가능한 한 다르도록 만듭니다. 범주형 피쳐의 경우, 알고리즘은 각 수준에 대해 subsetting 을 시도합니다. 각 피쳐별로 최적의 cutoff 가 결정되면 분산 또는 gini index 측면에서 가장 좋은 분할을 생성하는 피쳐를 선택하고 해당 분할을 tree 에 추가합니다. 중지 기준(stop criterion)에 도달할 때 까지 이러한 노드의 분할 및 서칭을 계속합니다. 중지 기준은 다음과 같습니다: 분할 전에 노드 내 최소 인스턴스 수, 또는 터미널 노드 내 최소 인스턴스 수

#### 1. Interpretation

##### Feature importance

DT 에서 feature importance 계산 방법은 아래와 같습니다. 각 피쳐가 사용된 모든 분할을 살펴보고 상위 노드에 비해 분산 또는 gini index 를 얼마나 줄였는지 측정합니다. 모든 중요도의 합계는 100으로 척도화됩니다. 즉, 각 중요도는 전체 모델 중요도의 점유율로 해석됩니다. 

##### Tree decomposition

DT의 개별 예측치는 tree path 를 피쳐당 하나의 구성 요소로 분해해서 설명할 수 있습니다. DT를 통해서 모델의 결정을 추적하고 각 노드에 추가된 기여도를 통해 해당 예측치를 설명할 수 있습니다. 

DT 에서 root node 를 출발점으로 하여 시작합니다. 우선 root node 내의 학습 데이터의 예측치들을 평균낸 후, 다음 분할에서는 경로(path)의 다음 node 의 예측치를 더합니다. 최종 예측치는 설명하고자 하는 인스턴스의 경로를 따라 수식에 계속 추가합니다: 

$$
\hat{f}(x)=\bar{y}+\sum_{d=1}^D \text { split.contrib }(\mathrm{d}, \mathrm{x})=\bar{y}+\sum_{j=1}^p \text { feat.contrib }(\mathrm{j}, \mathrm{x})
$$
개별 인스턴스의 예측치는 target 의 평균에 root node 와 terminal node 간 모든 분할 기여도를 합한 값입니다. 하지만 저희는 지금 분할 기여도가 아니라 feature contribution 에 관심이 있습니다. 각 피쳐에 대한 기여도를 모두 더하면 각 피쳐가 예측에 얼마나 기여했는지에 대한 해석을 얻을 수 있습니다. 

#### 2. Example

자전거 대여 데이터를 DT 로 적합하여 특정 날짜에 대여된 자전거 수를 예측하려고 합니다. 학습된 트리는 아래와 같습니다: 

> [!note]- code fold
> ```r
> X <- bike %>% select(all_of(bike.features.of.interest))
> y <- bike %>% pull(cnt)
> dat <- cbind(X, y)
> 
> ###### increases readability of tree
> x <- rpart(y ~ ., data = na.omit(dat), 
>            method = 'anova',
>            control = rpart.control(cp = 0, maxdepth = 2))
> xp <- as.party(x)
> plot(xp, digits = 9, id = F, terminal_panel = node_boxplot(xp, id = F),
>      inner_panel = node_inner(xp, id = F, pval = F))
> ```

![[Pasted image 20231230205344.png|center]] Figure 5.17 : 자전거 대여 데이터에 회귀 트리를 적용했습니다. Tree 에 허용되는 max depth 는 2 로 설정하였고, 분할에 사용된 피쳐는 `day_since_2011`과 `temp` 입니다. Boxplot 은 leaf node 에서의 `cnt` 의 분포입니다. 

1st 분할과 2nd 분할은 `day_since_2011` 을 이용했는데, 시간이 지남에 따라 자전거 대여 서비스의 인기가 높아지는 추세를 다루는 피쳐를 사용했습니다. 105일 이전 며칠 동안의 에측 대여 수는 약 1800대, 160일에서 434일 사이에는 약 3900대 입니다. 430일 이후 며칠 동안은 (기온이 섭씨 12도 미만일 경우) 5600대 이고, 기온이 12도 이상이면 약 6600대로 예측됩니다. 

Feature importance 는 피쳐가 모든 노드의 순도(purity)를 개선하는 데 얼마나 기여를 했는지를 알려줍니다. 여기서는 자전거 대여 수 예측이 regression 이기 때문에 분산이 사용됩니다. 

아래는 `temp`와 추세인 `day_since_2011` 이 모두 분할에 사용되었음을 보여주지만, 어떤 피쳐가 더 중요한지를 정량화하지는 않습니다. 

> [!note]- code fold
> ```r
> imp <- round(100 * x$variable.importance / sum(x$variable.importance), 0)
> imp.df <- data.frame(feature = names(imp),
>                      importance = imp)
> imp.df <- imp.df %>% 
>   mutate(feature = reorder(factor(feature), importance))
> imp.df %>% 
>   ggplot() + 
>   geom_point(aes(x = importance, y = feature)) +
>   scale_y_discrete("")
> ```

![[Pasted image 20231230210843.png|center]] Figure 5.18 : Feature importance 는 node purity 를 평균적으로 얼마나 개선했는지를 측정합니다. 

#### 3. Advantages

트리 구조는 데이터의 피쳐 간 상호작용을 포착하는 데 이상적입니다. 

데이터는 선형 회귀처럼 다차원 hyperplance 위의 점보다 더 이해하기 쉽도록 별도의 그룹으로 분할됩니다. 해석이 매우 간단합니다. 

트리는 [[CH03. Interpretability#03-06. Human-friendly Explanations|Human-friendly Explanations]]에서 정의한 기준으로 좋은 설명(explanations) 를 만들어냅니다. 트리 구조는 개별 인스턴스에 대한 예측치를 반사실적(counterfactual)으로 생각되는 것을 자동으로 유도합니다: "만약 어떤 피쳐가 분할 지점보다 크거나 작았다면, 예측치는 y2가 아니라 y1이 되었을 것이다." 라는식으로 말이죠. 트리의 다른 leaf node에 속하는 "만약" 시나리오를 통해 인스턴스의 예측치를 비교할 수 있기 때문에 tree 기반 설명은 contrastive 합니다. 이외에도 다른 기준들도 충분히 만족합니다. 

Tree 기반 모델은 피쳐를 변환할 필요가 없습니다. 선형 모델에서는 때때로 피쳐에 로그변환을 취해야 할 경우가 있습니다. 하지만 DT는 그러지 않더라도 잘 작동합니다. 

#### 4. Disadvantages

- 트리는 선형 관계에 대한 처리를 진행하지 못합니다: 
	- 피쳐들과 outcome 간 선형 관계는 분할을 통해 근사하여 step function 을 세세하게 만들어야 하는데 이것은 매우 비효율적입니다. 
	- 이것은 평활(smoothness)의 부족으로 이어집니다. 
- 상당히 불안정(unstable) 합니다: 
	- 학습 데이터셋이 약간만 변해도 완전히 다른 트리가 생성될 수 있습니다. 
	- 이는 DT가 과적합의 위험이 높음을 의미합니다.

---

## 05-05. Decision Rules

결정 규칙(DR) 은 조건(condition / antecedent)과 예측으로 구성된 IF-THEN 문 입니다. 예를 들어 오늘 비가 내리고 4월이면(조건), 내일 비가 올 것이다(예측). 단일 DR 또는 여러 규칙의 조합을 사용해 예측을 수행할 수 있습니다. 

DR의 일반적인 구조 : 조건이 충족되면 특정한 예측을 수행합니다. 

집의 가격/가치(`low`, `medium`, `high`)를 예측하기 위해서 결정 규칙(DR) 모델을 학습한다고 해보겠습니다. 이 모델이 학습하는 DR은 다음과 같다고 하겠습니다 : 집이 100㎡ 보다 크고 정원이 있으면 그 집의 가치는 높을 것입니다. 
- IF `size > 100` AND `garden = 1` THEN `value = high`

이 규칙을 세분화 해보겠습니다: 
- `size > 100` 는 IF-파트의 첫 번째 조건입니다.
- `garden = 1` 는 IF-파트의 두 번째 조건입니다.
- 두 조건을 AND로 연결해서 새로운 조건을 만듭니다. 규칙이 적용되기 위해선 두 조건을 모두 만족시켜야 합니다. 
- 예측 결과(predicted outcome)인 THEN-파트는 `value = high` 입니다.

결정 규칙(DR)은 조건에 한 개 이상의 `feature = value` 문을 사용하며, AND로 조건을 추가할 수 있고 조건 개수의 제한은 없습니다. 명시적으로 작성하지 않고 default 로 작동하는 기본 규칙은 예외이지만 이에 대해서는 추후에 자세히 다룰 것입니다. 

- Support or coverage of a rule : 
	- 규칙의 조건이 적용되는 인스턴스의 비율을 support 라고 부릅니다. 예를 들어 집값 예측을 위해 `size = big` AND `location = good` THEN `value = high` 라는 규칙을 사용한다고 해보겠습니다. 1000채의 집 중 100채가 크고 좋은 위치에 있다고 한다면 이 규칙의 support(지지도)는 10% 입니다. 예측치(THEN-파트)는 support 계산에 사용되지 않습니다. '
- Accuracy or confidence of a rule : 
	- 규칙의 정확도(accuracy)는 조건이 적용되는 인스턴스에 대해 규칙이 얼마나 정확히 클래스를 예측하는지에 대한 척도입니다. 예를 들어 100채의 주택 중 `size = big` AND `location = good` THEN `value = high`이라는 규칙이 적용되고 실제 데이터는 85개가 `value = high`이고, 14개가 `value = medium`이며, 1개가 `value = low` 인 경우, 해당 규칙의 정확도(accuracy)는 85%가 되는 것입니다. 

일반적으로 accuracy 와 support 사이에는 trade-off 관계까 존재합니다 : 
- 조건에 피쳐가 많아질수록 accuracy 는 높아지지만 support 는 감소합니다. 

주택 가치 예측 문제에 대해서 좋은 분류기(classifier)를 만드려면 하나가 아닌 여러 개의 규칙을 학습해야 할 것입니다. 그렇게 되면 모델은 복잡해져 아래와 같은 문제가 발생할 것입니다: 
- **Rules can overlap** : 주택 가치 예측에 있어서 2개 이상의 규칙이 적용되어 상반된/모순된(contradictory) 예측치가 나온다면 어떻게 해야 하나?
- **No rule applies** : 주택 가치 예측에 있어서 어떠한 규칙도 적용되지 않는다면 어떻게 해야 하나?

다수의 규칙을 결합하는 데는 2가지 전략이 필요합니다: 
- **Decision lists (ordered)** : 
	- 결정 목록(decision lists)은 DR에 순서를 도입하는 것입니다. 어떤 인스턴스에 대해 1st 조건이 참이면 1st 규칙의 예측을 사용하는 것입니다. 그렇지 않은 경우 다음 규칙으로 넘어가서 해당 규칙의 적용 여부를 확인하는 식으로 진행됩니다. 결정 목록은 적용되는 목록에서 가장 첫 번째 규칙의 예측치만을 반환함으로써 **규칙이 겹치는 문제를 해결**합니다. 
- **Decision sets (unordered)** : 
	- 결정 집합(decision sets)은 일부 규칙이 더 높은 투표권을 가질 수 있다는 점을 제외하면 규칙들 간 민주주의와 같습니다. 하나의 집합에서의 규칙들은 상호 배타적(mutually exclusive)이거나 다수결 투표와 같은 충돌을 해결할 수 있는 전략이 있으며, 개별 규칙의 정확도 또는 기타 규칙 품질 척도에 따라 가중치가 부여될 수 있습니다. 하지만 여러 개의 규칙이 적용되면 해석 가능성이 저하될 수 있습니다. 



결정 목록과 결정 집합은 모두 규칙이 없는 문제가 발생할 수 있습니다. 이 문제는 기본 규칙(default rule)을 도입하여 해결할 수 있습니다. 기본 규칙은 다른 규칙이 적용되지 못할 때 적용되는 규칙입니다. 기본 규칙이 내뱉는 예측치는 다른 규칙이 적용되지 않는 데이터 포인트 중 가장 빈번한 클래스로 하는 것이 보통입니다. 규칙의 집합 또는 목록이 전체 피쳐 공간(entire feature space)을 포함하는 경우 이를 포괄적(exhaustive)이라고 부릅니다. 기본 규칙을 추가하면 결정 집합 또는 목록이 자동적으로 exhaustive가 됩니다. 

데이터에서 규칙을 학습하는 방법에는 여러 가지가 있으며, 여기서는 모든 것을 다루지는 않습니다. 이 중 3가지 방법을 소개하겠습니다. 알고리즘은 규칙 학습에 대한 일반적인 아이디어를 포괄적으로 다루기 위해 선택한 것들이므로 3가지가 모두 상이한 방식입니다. 

1. **OneR** 는 단일 피쳐에서 규칙을 학습합니다 : 
	- OneR 의 특징은 단순성(simplicity), 해석 가능성(interpretability), 벤치마킹으로서의 활용 가능함 입니다. 
2. **Sequential covering** 은 규칙을 반복적으로 학습하고 새로운 규칙에 포함되는 데이터 포인트를 제거하는 프로시져입니다. 
	- 이 프로시저는 많은 rule learning alogorithms 에서 사용됩니다. 
3. **Bayesian Rule Lists** 는 베이지안 통계를 사용해 사전에 학습한 드문 패턴들을 결정 목록(decision lists)으로 결합하는 것입니다. 
	- Pre-mined patterns 를 사용하는 것은 많은 규칙 학습 알고리즘에서 사용되는 방식입니다. 

#### 1. Learn Rules from a Single Feature; OneR

Holte (1993) 이 제안한 OneR 알고리즘은 가장 간단한 규칙 유도 알고리즘 중 하나 입니다. OneR 은 모든 피쳐 중 outcome 에 대한 정보를 가장 많이 담고 있는 피쳐를 선택하고 해당 피쳐에서 결정 규칙(DR)을 생성하는 것입니다. 

"하나의 규칙(One Rule)" 을 의미하는 이름과는 다르게 해당 알고리즘은 한 개 이상의 규칙을 만들어냅니다. 실제로는 선택한 best feature 의 unique vale 당 한 개의 규칙입니다. 따라서 OneFeatureRules 라고 부르는 것이 더 좋을 수도 있습니다. 

이 알고리즘은 간단하며 빠릅니다 : 
1. 적당한 간격(intervals)을 선택하여 연속형 피쳐를 이산화(discretize) 합니다.
2. 각 피쳐에 대하여 :
	- 피쳐 값과 범주형 outcome 간 교차 표를 만듭니다.
	- 피쳐의 각 값에 대해 이 피쳐 값을 갖는 인스턴스 중 가장 빈번한 클래스를 예측하는 규칙을 만듭니다(교차 표에서 읽을 수 있음).
	- 피쳐에 대한 해당 규칙의 총 오차를 계산합니다.
3. 총 오차가 가장 작은 피쳐를 선택합니다. 

OneR 은 선택한 피쳐의 모든 수준을 사용하기 때문에 항상 데이터셋의 모든 인스턴스를 포함합니다. 결측치는 해당 피쳐의 추가적인 값으로 처리하거나 미리 대체할 수 있습니다. 

OneR 모델은 분할이 한 개만 있는 결정 트리입니다. 분할은 CART 처럼 반드시 binary 일 필요는 없지만 피쳐의 고유한 값의 개수에 따라 달라집니다. 

OneR 이 best feature 를 선택하는 방법을 살펴보겠습니다. 아래 표는 주택의 가치(value), 위치(location), 크기(size), 반려동물 허용 여부에 대한 정보가 포함된 인위적인 데이터셋을 보여줍니다. 이제 우리는 주택의 가치를 예측하는 간단한 모델을 학습하고 싶다고 가정하겠습니다 : 

> [!note]- code fold
> ```r
> value <-  factor(c("high", "high", "high", "medium", "medium", "medium", "medium", "low", "low", "low"), levels = c("low", "medium", "high"))
> 
> df <- data.frame(
>   location = c("good", "good", "good", "bad", "good", "good", "bad", "bad", "bad", "bad"),
>   size = c("small", "big", "big", "medium", "medium", "small", "medium", "small", "medium", "small"), 
>   pets = c("yes", "no", "no", "no", "only cats", "only cats", "yes", "yes", "yes", "no"),
>   value = value
> )
> ```

| **location** | **size**   | **pets**      | **value**  |
|:---------|:-------|:----------|:-------|
| good     | small  | yes       | high   |
| good     | big    | no        | high   |
| good     | big    | no        | high   |
| bad      | medium | no        | medium |
| good     | medium | only cats | medium |
| good     | small  | only cats | medium |
| bad      | medium | yes       | medium |
| bad      | small  | yes       | low    |
| bad      | medium | yes       | low    |
| bad      | small  | no        | low    |  

OneR 은 각 피쳐와 outcome 간 교차 표 (cross table)를 만듭니다 : 

```r
table(paste0("location=", df[,"location"]), value.f)
```

|               | value=low | value=medium | value=high |
|:--------------|:----------|:-------------|:-----------|
| location=bad  |         3 |            2 |          0 |
| location=good |         0 |            2 |          3 |  

```r
table(paste0("size=", df[,"size"]), value.f)
```

|             | value=low | value=medium | value=high |
|:------------|:----------|:-------------|:-----------|
| size=big    |         0 |            0 |          2 |
| size=medium |         1 |            3 |          0 |
| size=small  |         2 |            1 |          1 |  

```r
table(paste0("pets=", df[,"pets"]), value.f)
```

|                | value=low | value=medium | value=high |
|:---------------|:----------|:-------------|:-----------|
| pets=no        |         1 |            1 |          2 |
| pets=only cats |         0 |            2 |          0 |
| pets=yes       |         2 |            1 |          1 |  

이 교차 테이블은 각 피쳐에 대해 살펴보자면, 각 피쳐의 값은 규칙에서 조건인 IF 파트입니다. 해당 피쳐 값을 가지는 인스턴스에 대한 가장 일반적인/빈번한 클래스는 규칙에서 THEN 파트인 예측치 입니다. 예를 들어, `small`, `medium`, `big` 수준의 `size` 피쳐는 3가지 규칙을 생성합니다. 각 피쳐에서 생성된 규칙의 총 오류율(error rate), 즉 오류의 합을 계산합니다. location 피쳐는 `bad`와 `good` 이라는 값을 갖습니다. `bad` location에 있는 주택에서 가장 빈번한 값은 `low` 이며 이를 예측치로 사용하면 2개의 오류를 범합니다. `good` location에 있는 주택의 예측치는 `high`인데 마찬가지로 이 규칙도 2개의 오류를 범합니다. Location 피쳐를 사용했을 시 발생하는 오류는 4/10, size 피쳐는 (0+1+2)/10 = 3/10, pets 피쳐는 (2+0+2)/10=4/10 입니다. size 피쳐가 가장 낮은 오차를 가지는 규칙을 생성하므로 최종 OneR 모델에 사용됩니다:

IF `size = small` THEN `value = low`
IF `size = medium` THEN `value = medium`
IF `size = big` THEN `value = high`

OneR 모델은 가능한 레벨/수준이 많은 피쳐를 선호하는데, 그 이유는 그러한 피쳐가 target 에 더 과적합되기 쉽게 때문입니다. Noise 만 있고 신호/패턴이 없는 데이터셋, 즉 모든 피쳐가 random한 값을 취하고 target 에 대한 예측력 있는 값이 없는 데이터셋을 상상해 보세요. 특정한 피쳐가 유독 많은 레벨을 가질 때, 이 피쳐를 이용한 OneR 은 과적합의 위험이 큽니다. 데이터의 각 인스턴스에 대해서 별도의 레벨을 가지는 피쳐는 전체 학습 데이터셋을 완벽히 예측할 수 있습니다. Train set 과 validation set으로 나누고, 학습 데이터에 대한 규칙을 학습하고, 검증 데이터에서 피쳐를 선택할 때 총 오류를 평가하는 것이 이에 대한 해결책이 됩니다. 

동점(Ties), 즉 두 개 이상의 피쳐가 동일한 총 오류를 갖는 경우 OneR 은 가장 낮은 오류를 가지는 맨 첫 번째 피쳐를 사용하거나, Chi-squared test 의 p-value 가 가장 낮은 피쳐를 사용해 이러한 동점 문제를 해결합니다. 

##### Example

실제 데이터를 통해 OneR 을 수행해 보겠습니다. 자궁경부암 분류 문제를 통해 OneR 알고리즘을 테스트하겠습니다. 모든 연속형 피쳐를 5개의 사분위수로 이산화시켜서 아래와 같은 규칙을 만들었습니다 : 

> [!note]- code fold
> ```r
> rule <- OneR::OneR(Biopsy ~ ., data = cervical)
> 
> rule.to.table <- function(rule){
>   dt <- data.frame(x = names(rule$rules), 
>                    prediction = unlist(rule$rules))
>   colnames(dt) <- c(rule$feature, "prediction")
>   return(dt)
> }
> 
> rule.to.table(rule)
> ```

| Age         | prediction |
|:------------|:-----------|
| (12.9,27.2] | Healthy    |
| (27.2,41.4] | Healthy    |
| (41.4,55.6] | Healthy    |
| (55.6,69.8] | Healthy    |
| (69.8,84.1] | Healthy    |  

`Age` 피쳐가 OneR 에서 best feature 로 선택되었습니다. 암은 드문 사건이므로 각 규칙에 대해 과반 수 클래스, 즉 예측 레이블이 항상 Healthy 로 설정되기 때문에 도움이 되지 않는 모델로 보입니다. 이렇게 불균형한 경우에서는 클래스 예측을 하는 것은 의미가 없습니다. `Age`의 간격과 Cancer / Healthy 사이의 교차 테이블을 암에 걸린 여성의 비율과 함께 작성하면 더 많은 정보를 얻을 수 있습니다. 

> [!note]- code fold
> ```r
> #### oner-cervical-confusion
> tt <- table(paste0("Age=", bin(cervical$Age)), cervical$Biopsy)
> cn <- colnames(tt)
> tt <- data.frame(matrix(tt, ncol = 2), row.names = rownames(tt))
> colnames(tt) <- cn
> tt %>%
>   mutate(`P(Cancer)` = round(Cancer / (Cancer+Healthy), 2)) %>% 
>   rename(`# Cancer` = Cancer, `# Healthy` = Healthy)
> ```

|                 | # Cancer | # Healthy | P(Cancer) |
|:----------------|:---------|:----------|:----------|
| Age=(12.9,27.2] |       26 |       477 |      0.05 |
| Age=(27.2,41.4] |       25 |       290 |      0.08 |
| Age=(41.4,55.6] |        4 |        31 |      0.11 |
| Age=(55.6,69.8] |        0 |         1 |         0 |
| Age=(69.8,84.1] |        0 |         4 |         0 |  

하지만 해석을 하기도 전에 모든 피쳐와 값에 대한 예측치가 'Healthy'이기 때문에 총 오류율은 동일합니다. 총 오류의 동점은 기본적으로 오류율이 가장 낮은 피쳐에서 맨 첫 번째의 피쳐를 사용하기 때문에 `Age`가 선택된 것입니다. 

OneR 은 regression 을 지원하지 않습니다. 하지만 연속형 outcome 을 구간으로 잘라서 classification 문제로 전환할 수 있습니다. 이를 사용하면 자전거 대여 수를 4 사분위수로 잘라 OneR 을 통해 자전거 수를 예측할 수 있습니다. 아래는 이를 적용한 것입니다 : 

> [!note]- code fold
> ```r
> #### oner-bike
> bike2 <- bike
> bike2 <- bike2 %>% 
>   mutate(days_since_2011 = max(0, days_since_2011),
>          cnt = cut(cnt, breaks = quantile(cnt), dig.lab = 10, include.lowest = T))
> rule <- OneR::OneR(cnt ~ ., data = bike2)
> rule
> rule.to.table(rule)
> ```

| mnth | prediction  |
|:-----|:------------|
| JAN  | [22,3152]   |
| FEB  | [22,3152]   |
| MAR  | [22,3152]   |
| APR  | (3152,4548] |
| MAY  | (5956,8714] |
| JUN  | (4548,5956] |
| JUL  | (5956,8714] |
| AUG  | (5956,8714] |
| SEP  | (5956,8714] |
| OCT  | (5956,8714] |
| NOV  | (3152,4548] |
| DEC  | [22,3152]   |  

선택된 피쳐는 월을 나타내는 `mnth` 입니다. 12개의 레벨이 있는데, 다른 피쳐들보다 많은 수의 레벨을 가집니다. 따라서 과적합이될 위험이 있습니다. 다행인 것은 `mnth` 피쳐는 계절적인 trend 를 다룰 수 있으며 어느정도 예측이 합리적으로 보입니다. 


#### 2. Sequential Covering

Sequential Covering ; SC 는 하나의 규칙을 반복적으로 학습해서 전체 데이터셋의 규칙을 규칙별로 커버하는 결정 목록(lists) 또는 집합(sets)을 만드는 프로시저입니다. 여러 규칙 학습 알고리즘들이 이 SC 의 변형입니다. 이 섹션에서는 주요 recipe 를 소개하고 예제에서는 SC 의 변형인 RIPPER 를 사용해 보겠습니다. 

아이디어는 간단합니다 : 먼저 데이터 포인트 일부에 적용되는 좋은 규칙을 찾습니다. 이 규칙은 적용되는 모든 데이터 포인트를 제거합니다. 데이터 포인트가 올바르게 분류되었는지는 관계없이 조건이 적용되면 데이터 포인트가 커버됩니다. 더 이상 데이터 포인트가 남지 않거나 다른 중지 조건이 충족될 때까지 남은 데이터 포인트로 규칙을 학습하고 규칙이 적용되는 데이터 포인트의 제거를 반복합니다. 그 결과는 규칙들의 결정 목록(lists)입니다. 규칙 학습과 커버된 데이터 포인트의 제거를 반복하는 이러한 방식을 "separate-and-conquer"이라고 부릅니다. 

데이터 일부를 커버하는 단일 규칙을 생성하는 알고리즘을 가정하겠습니다. 2개의 클래스(+1 or -1)에 대한 SC 알고리즘은 아래와 같이 작동합니다 : 
- 빈 목록으로 시작합니다 (rlist).
- 규칙 r 을 학습합니다. 
- 규칙들의 목록이 특정한 임계값 미만일 경우 : 
	- 규칙 r 을 목록 rlist 에 추가합니다.
	- 규칙 r 이 적용되는 모든 데이터 포인트를 제거합니다. 
	- 나머지 데이터에 대해 다른 규칙을 학습합니다.
- 결정 목록을 반환합니다.

> [!note]- code fold
> ```r
> set.seed(42)
> n <- 100
> dat <- data.frame(x1 = rnorm(n), x2 = rnorm(n))
> dat$class <- rbinom(n=100, size=1, p = exp(dat$x1 + dat$x2)/(1+exp(dat$x1+dat$x2)))
> dat$class <- factor(dat$class)
> 
> min.x1 <- min(dat$x1)
> min.x2 <- min(dat$x2)
> 
> p1 <- dat %>% 
>   ggplot() +
>   geom_point(aes(x=x1, y=x2, color = class, shape = class)) +
>   scale_color_viridis(guide = "none", discrete = T, option = "D", end = 0.9) +
>   scale_shape_discrete(guide = "none") +
>   ggtitle("Data")
> p2 <- dat %>% 
>   ggplot() +
>   geom_rect(xmin = -3, xmax = 0, ymin = -2, ymax = -0.5, color = "black", fill = NA) +
>   geom_point(aes(x = x1, y = x2, color = class, shape = class)) +
>   scale_color_viridis(guide = "none", discrete = T, option = "D", end = 0.9) +
>   scale_shape_discrete(guide = "none") +
>   ggtitle("Step 1 : Find rule")
> 
> dat.reduced <- dat %>% 
>   filter(!(x1 <=0 & x2 <= -0.5))
> 
> p3 <- dat.reduced %>% 
>   ggplot() +
>   geom_point(aes(x = x1, y = x2, color = class, shape = class)) +
>   geom_rect(xmin = -3, xmax = 0, ymin = -2, ymax = -0.5, color = "black", fill = NA) +
>   scale_x_continuous(limits = c(min.x1, NA)) +
>   scale_y_continuous(limits = c(min.x2, NA)) +
>   scale_color_viridis(guide = "none", discrete = T, option = "D", end = 0.9) +
>   scale_shape_discrete(guide = "none") +
>   ggtitle("Step 2 : Remove covered instances")
> 
> p4 <- p3 +
>   geom_rect(xmin = 0.8, xmax = 2.5, ymin = -1.5, ymax = 1.5, color = 'black', fill = NA) +
>   ggtitle("Step 3 : Find next rule")
> 
> gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2) 
> ```

![[Pasted image 20231231174836.png|center]] Figure 5.19 : SC 알고리즘은 단일 규칙으로 피쳐 공간을 순차적으로 커버하고 규칙이 적용된 데이터를 제거하는 방식으로 작동됩니다. 시각화를 위해서 이 예시에서는 두 개의 피쳐가 연속형이지만, 보통의 규칙 학습 알고리즘에서는 범주형 피쳐에서 많이 쓰입니다.

예를 들어 주택의 `size`, `location`, `pets` 에 따라 주택의 가치 `value` 를 예측하는 작업과 해당되는 데이터셋이 있다고 하겠습니다. 첫 번째 규칙을 우선 학습합니다 :  IF `size = big` AND `location = good` THEN `value = high`. 그런 다음 데이터셋에서 조건에 부합하는 데이터 포인트를 모두 제거합니다. 남은 데이터로 다음 규칙을 학습합니다 : IF `location = good` THEN `value = medium`. 이 규칙은 `location = good`인 주택 중에 더 이상 `size = big` 인 주택이 없으므로 `size = medium` 과 `size = small` 인 집만 학습하게 됩니다. 

다중 클래스의 경우 약간의 수정을 거친 접근방식으로 진행합니다. 먼저 선호도가 높은 순서대로 클래스를 정렬합니다. SC 알고리즘은 가장 빈도가 낮은 클래스부터 시작해서 해당 클래스에 대한 규칙을 학습하고, 커버된 인스턴스를 모두 제거한 다음 두 번째 빈도로 낮은 클래스로 이동하는 식입니다. 현재 반복에서의 클래스는 항상 positive class 로 취급되며, 더 높은 빈도를 갖는 클래스들은 모두 negative class 로 간주합니다. 마지막 (최빈의) 클래스가 default rule 이 됩니다. 이를 분류 문제에서 one-versus-all 전략이라고 부릅니다. 

단일 규칙 학습 방식을 알아봅시다. OneR 알고리즘은 항상 전체 피쳐 공간을 다루기 때문에 여기선 쓸모가 없습니다. 다른 방법으로 beam search 를 통해 결정 트리에서 단일 규칙을 학습하는 것입니다 : 
- 먼저 결정 트리(CART or 다른 tree based 알고리즘)를 학습합니다.
- root node 에서 시작하여 가장 순도가 높은 node (예 : 오분류율이 가장 낮은 node)를 재귀적으로 선택합니다.
- Terminal node 에서의 다수 클래스가 해당 규칙의 예측치로 사용되며, 각 해당 노드로 이어지는 경로(path)가 규칙의 조건(condition)이 됩니다.

아래 그림은 결정 트리에서 beam search 를 나타냅니다 :

![[Pasted image 20231231180247.png|center]] Figure 5.20 : 결정 트리를 통해 경로를 검색하여 규칙을 학습합니다. Root node 에서 시작하여 가장 순도가 높은 subset 을 (국지적으로) 생성하는 경로를 greedy & iterative 방식으로 찾아가며 분할의 모든 값을 규칙의 조건으로 추가합니다. : If `location=good` and `size=big`, then `value=high`.

단일 규칙을 학습하는 것은 검색의 문제이고, 여기서 검색 공간(search space)는 모든 가능한 규칙의 공간입니다. 검색(seach)의 목적은 몇몇 규칙/기준에 맞는 최적의 규칙을 찾아내는 것입니다. 검색 전략에는 hill-climbing, beam search, exhaustive search (완전 탐색), best-first search, ordered search, stochastic search, top-down search, bottom-up search 등 다양하게 있습니다. 

Cohen (1995) 의 RIPPER (Repeated Incremental Pruning to Produce Error Reduction) 는 SC 알고리즘의 변형입니다. RIPPER 는 좀 더 정교하고 사후 처리 단계 (rule pruning; 규칙 가지치기)를 사용하여 결정 목록 또는 집합을 최적화합니다. RIPPER 는 ordered 또는 unordered mode 로 실행할 수 있으며 결정 목록 또는 집합을 생성합니다.

##### Example

RIPPER 알고리즘을 자궁경부암 분류 문제에 사용했을 시 어떠한 규칙도 찾아내지 못했습니다.

> [!note]- code fold
> ```r
> extract.rules.jrip <- function(rule){
>   rules <- scan(text = .jcall(rule$classifier, "S", "toString"), sep = "\n", what = "")
>   # removes text
>   rules <- rules[-c(1, 2, length(rules))]
>   rules <- gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
>   rules <- as.matrix(rules)[-c(1:2, 6), , drop = F]
>   rules <- data.frame(rules)
>   if(nrow(rules) == 0){
>     return(NULL)
>   } else {
>     return(rules)
>   }
> }
> rule <- RWeka::JRip(Biopsy ~ ., data = cervical)
> > extract.rules.jrip(rule)
> NULL
> ```



자전거 대여 수 예측 문제에 RIPPER 사용 시 몇 가지 규칙이 발견됩니다. RIPPER 는 분류 문제에만 작동하기 때문에 대여 수 (`cnt`)를 범주화/이산화 해야 합니다. 여기서는 `cnt` 를 사분위수로 binning 했습니다. 아래 표는 학습된 규칙의 결정 목록(lists)를 보여줍니다 :

> [!note]- code fold
> ```r
> bike2 <- bike %>% 
>   mutate(cnt = round(cnt)) %>% 
>   mutate(cnt = cut(cnt, breaks = quantile(cnt), dig.lab = 10, include.lowest = T),
>          temp = round(temp),
>          windspeed = round(windspeed), 
>          hum = round(hum))
> 
> rule <- JRip(cnt ~ ., data = bike2)
> tab <- extract.rules.jrip(rule)
> tab 
> ```

|rules|
|---|
|(days_since_2011 >= 431) and (days_since_2011 <= 467) and (windspeed >= 19) => cnt=(4548,5956]|
|(days_since_2011 <= 98) => cnt=[22,3152]|
|(atemp <= 31.539564) and (hum >= 81) and (windspeed >= 9) => cnt=[22,3152]|
|(temp <= 11) and (atemp <= 24.822014) and (workingday = NO WORKING DAY) => cnt=[22,3152]|
|(days_since_2011 <= 415) and (atemp <= 33.75225) and (hum >= 64) and (windspeed >= 14) => cnt=[22,3152]|
|(atemp <= 29.092958) and (days_since_2011 <= 367) and (season = WINTER) => cnt=[22,3152]|
|(hum >= 92) => cnt=[22,3152]|
|(days_since_2011 <= 430) => cnt=(3152,4548]|
|(windspeed >= 24) => cnt=(3152,4548]|
|=> cnt=(5956,8714]|

해석은 간답하니다 : 조건이 적용되면 `cnt` 에 대한 우변의 interval (사분위수)를 예측합니다. 마지막 규칙은 다른 규칙이 인스턴스에 적용되지 않을 때 적용되는 default rule 입니다. 새 인스턴스에 대한 예측은 결정 목록의 맨 위부터 시작하여 규칙의 적용을 확인합니다. Default rule 은 항상 예측치를 보장합니다. 

#### 3. Bayesian Rule Lists

이번에는 결정 목록을 학습하는 또 다른 방식을 소개하겠습니다 : 
1. 데이터에서 결정 규칙 (DR)의 조건(condition)으로 사용할 수 있는 빈번한 패턴을 미리 학습/포착 합니다. 
2. 사전에 찾아낸 (pre-mined) 규칙들 중에서 결정 목록을 학습합니다.

이 방식(recipe)을 사용하는 것은 Bayesian Rule Lists (Lethan et al., 2015) 이며 줄여서 BRL 로 부릅니다. BRL 은 베이지안 통계를 사용하고 FP-tree 알고리즘으로 pre-mined 된 빈번한 패턴으로부터 결정 목록을 학습합니다.

BRL 의 첫 번째 단계부터 시작하겠습니다.

##### Pre-mining of frequent patterns

여기서 말하는 frequent patterns 라 함은 데이터에서 자주 나타나는 피쳐 값들의 조합을 의미합니다. BRL 알고리즘의 전처리 단계로써, 피쳐들을 사용해 이렇게 자주 발생하는 패턴을 추출합니다. 

패턴의 빈도는 데이터셋 내에서 해당 패턴의 support 값으로 측정합니다 :
$$
\operatorname{Support}\left(x_j=A\right)=\frac{1}{n} \sum_{i=1}^n I\left(x_j^{(i)}=A\right)
$$
주택 가격 데이터셋에서 20% 의 주택에는 발코니가 없고 80% 의 주택에는 발코니가 하나 이상 있어서 `balcony = 0` 이라는 패턴의 support 값은 20%가 됩니다. 또한 `balcony = 0 AND pets = allowed` 와 같은 패턴의 support 값도 계산할 수 있습니다. 

이러한 패턴을 찾는 알고리즘에는 Apriori 또는 FP-Growth 같은 것들을 사용합니다. 어떤 것을 사용하든 속도만 다를 뿐 결과 패턴은 동일하기 때문에 크게 중요하지 않습니다. 

Apriori 가 이러한 패턴을 찾는 작동 방식에 대해 간단하게 소개하겠습니다. 이 알고리즘은 실제로 2개 파트로 구성돼 있는데, 하나는 패턴을 찾고 나머지는 이를 통해서 연관 규칙 (association rules)을 구축합니다. BRL 알고리즘의 경우, Aprioir 가 찾은 패턴에만 관심이 있습니다. 

첫 번째 단계에서 Aprior 는 모든 피쳐를 가지고 시작하는데 이는 사용자가 지정한 최소 support 값보다 큰 값을 갖게 하기 위함입니다. 사용자가 최소 서포트를 10%로 지정했는데 주택의 5%만 `size=big`이라면, 해당 피쳐값을 제거하고 `size=medium`과 `size=small`만 패턴으로 유지합니다. 이는 데이터셋에서 인스턴스를 제거하는 것이 아니라 `size=big`이 FP로 선정되지 않음을 의미합니다. Apriori 는 하나의 피쳐값에서의 FP를 시작으로 점차 차수를 높여서 반복적으로 진행합니다. 패턴은 `feature=value` 문과 AND 를 결합하여 구성됩니다. 최소 서포트보다 낮은 서포트를 내뱉는 패턴은 제거됩니다. FP 의 subset 은 다시 FP가 되는데, 이를 Apriori property 라고 합니다. 이는 직관적인데, 패턴에서 조건을 하나 제거하면 축소된 패턴은 오히려 더 큰 데이터를 포함할 뿐이기 때문입니다. 예를 들어 주택의 20%가 `size=medium and location=good` 인 경우 `size=medium` 만 있는 주택의 support 는 20% 이상입니다. Apriori property 는 검사할 패턴의 수를 줄여줍니다. 

이제 BRL 의 pre-mined 조건은 끝난 것입니다. 그 다음 단계로 넘어가기 전에 pre-mined patterns 를 기반으로 규칙을 학습하는 또 다른 방법에 대해 살펴보겠습니다. 다른 방법으로는, FP mining processing 에 outcome 을 포함시키는 IF-THEN 규칙을 구축하는 Apriori 알고리즘의 두 번째 파트를 수행하는 것입니다. 이것은 비지도 방식이기 때문에 THEN 부분에는 관심 없는 피쳐값도 포함됩니다. 하지만 THEN 부분에서 outcome 만 있는 규칙으로 필터링할 수 있습니다. 이러한 규칙은 이미 결정 집합(set)을 형성하지만 규칙을 정렬(arrange), 정리(prune), 삭제(delete), 재결합(recombine) 하는 기능도 가능합니다. 

BRL 방식에서는 FP 로 pre-mining 하고 베이지안 통계를 사용하여 THEN 부분과 패턴을 결정 목록으로 정렬(arrange)하는 방법을 채택합니다.

##### Learning Bayesian Rule Lists

BRL의 목표는 pre-mined conditions 를 사용해 정확한 결정 목록(list)을 학습하는 것입니다. 그러면서 적은 규칙과 짧은 조건들을 갖는 목록들의 우선순위를 정합니다. BRL은 조건의 길이 (가급적 짧은 규칙)와 규칙의 수 (가급적 짧은 목록)에 대한 사전 분포로 결정 목록의 분포를 정의함으로써 이를 수행합니다. 

목표는 결정 목록의 사후 확률을 최대화하는 목록을 찾는 것입니다. 최적의 목록을 찾는 방법은 아래와 같습니다:
1. 사전 분포에서 무작위로 추출한 초기 목록을 우선 생성합니다.
2. 규칙들을 추가, 변경, 제거하여 목록을 반복적으로 수정해 결과 목록이 사후 분포를 따르도록 합니다. 
3. 추출된 목록 중 사후 분포에서 가장 높은 확률을 갖는 목록을 선택합니다.

이 알고리즘은 FP-Growth 입니다: 우선 피쳐값 패턴을 pre-mining 하는 것으로 시작합니다. BRL은 target 의 분포와 이 분포를 정의하는 모수(parameter)의 분포에 대해 여러 가정을 합니다 (베이지안 통계). 사전 분포에 대한 가정에 의해 짧은 규칙들이 만든 짧은 결정 목록이 생성되므로 합리적입니다. 

목적은 사후 분포에서 결정 목록 $d$ 를 추출/샘플링하는 것입니다 : 
$$
\underbrace{p(d \mid x, y, A, \alpha, \lambda, \eta)}_{\text {posteriori }} \propto \underbrace{p(y \mid x, d, \alpha)}_{\text {likelihood }} \cdot \underbrace{p(d \mid A, \lambda, \eta)}_{\text {priori }}
$$
여기서 $d$ 는 결정 목록, $x$ 는 features, $y$ 는 target, $A$ 는 pre-mined conditions 의 집합입니다. $\lambda$ 는 목록의 길이의 priori expectation 이고, $\eta$ 는 규칙 수의 prior expectation 입니다. $\alpha$ 는 prior pseudo-count 로 사전 분포의 pos & neg 클래스가 1:1 로 해두는게 좋습니다. 

사후 확률인 $p(d|x, y, A, \alpha, \lambda, \eta)$ 는 데이터와 사전 분포가 주어졌을 때 결정 목록의 확률을 나타냅니다. 이는 목록 $d$ 와 데이터 $x$ 가 주어졌을 때 outcome 인 $y$ 의 확률에 $d$ 의 사전 확률을 곱한 값에 비례합니다.

그리고 $p(y|x,d,\alpha)$ 는 목록 $d$ 와 데이터가 주어졌을 때 $y$ 의 확률 (likelihood) 입니다. BRL에서는 $y$ 가 Dirichlet-Multinomial 분포에서 생성된다고 가정하고 있습니다. 목록 $d$ 가 데이터를 잘 설명할수록 이 확률은 높아집니다. 

그리고 $p(d|A,\lambda,\eta)$ 는 목록의 사전 분포입니다. 목록의 규칙 수에 대한 poisson 분포 (모수 $\lambda$)와 조건이 갖는 피쳐 수에 대한 truncated poisson 분포 (모수 $\eta$) 를 곱해서 결합합니다. 

결정 목록은 $y$ 를 잘 설명하고 사전 분포에 대한 가정에 따라서 확률이 높을 때 사후 확률이 높습니다.

베이지안 통계의 추정은 MCMC를 사용하여 후보(candidates)를 도출하고 평가한 후 사후 추정치를 업데이트 합니다. 결정 목록의 분포에서 이 후보들을 도출해야 하기 때문에 까다롭습니다. BRL의 저자들은 먼저 초기 목록을 도출한 다음 반복적으로 수정하여 목록의 사후 분포(목록의 marcov chain)에서 목록 샘플을 생성할 것을 제안합니다. 초기 목록에 따라서 결과가 달라질 수 있으므로 이 절차를 반복해서 다양한 목록을 확보해야 합니다. 패키지에서 이 반복 횟수의 기본값(default)은 10회입니다. 아래 방법은 초기 목록을 도출하는 방법을 알려줍니다 : 
- FP-Growth 로 패턴을 pre-mining 합니다.
- Truncated Poisson 분포에서 목록 길이에 대한 모수 $m$ 을 샘플링합니다.
- 기본 규칙은 target y 의 Dirichlet-Multinomial 분포의 모수 $\theta_0$ 를 샘플링합니다.
	- 즉 다른 규칙이 적용되지 않을 때 적용되는 규칙
- 결정 목록 내 규칙 $j=1,\cdots,m$ 에 대하여 다음과 같이 수행합니다 : 
	- $j$ 번째 규칙에 대한 규칙 길이 / 조건 수 모수 $l$ 을 추출
	- pre-mined conditions 에서 규칙 길이 $l_j$ 의 조건을 추출
	- (규칙이 주어졌으니 target의 분포를 나타내는) THEN 부분에 대한 Dirichlet-Multinomoal 분포의 모수를 추출
- 데이터셋 내 각 관측치에 대하여 :
	- 결정 목록에 먼저 적용되는 규칙을 찾습니다 (top to bottom).
	- 적용되는 규칙이 제시하는 확률 분포 (Binomial) 에서 예측치를 도출합니다.

이제 다음 단계는 결정 목록의 사후 분포에서 많은 샘플을 얻기 위해 이 초기 샘플/목록으로 시작해 여러 새 목록을 생성하는 것입니다. 새 목록은 초기 목록에서 시작하여 규칙을 무작위로 다른 위치로 이동하거나 pre-mined conditions 에서 규칙을 추가하거나 제거하는 방식으로 추출됩니다. 어떤 규칙이 전환, 추가, 삭제되는지는 무작위로 선택됩니다. 각 단계에서 목록의 사후 확률 (mixture of accuracy and shortness)을 평가합니다. Metropolis Hastings 알고리즘은 사후 확률이 높은 결정 목록을 뽑을 수 있게 해줍니다. BRL은 사후 확률이 가장 높은 후보/샘플의 목록을 선택합니다.

##### Example 1

실습에서는 Yang (2017)의 Scalable BRL; SBRL 이라는 BRL 의 빠른 버전을 사용하겠습니다. 자궁 경부암 위험 예측 문제에 SBRL을 사용해 보겠습니다. 먼저 SBRL을 사용하려면 모든 input feature를 이산화해야 합니다. 따라서 값의 빈도에 따라 사분위수로 연속형 피쳐를 bining 했습니다.

그 결과로 아래와 같은 규칙을 얻었습니다 : 

> [!note]- code fold
> ```r
> cervical2 <- as.data.frame(lapply(cervical, function(x){
>   if(is.factor(x) | n_distinct(x) < 3){
>     as.factor(x)
>   } else {
>     arules::discretize(x, method = "interval", 3)
>   }
> }))
> 
> get.sbrl.rules <- function(x) {
>   res = lapply(1:nrow(x$rs), function(i) {
>     if (i == 1) 
>       sprintf("If      %s (rule[%d]) then positive probability = %.8f\n", 
>               x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
>     else if (i == nrow(x$rs)) 
>       sprintf("else  (default rule)  then positive probability = %.8f\n", 
>               x$rs$V2[nrow(x$rs)])
>     else sprintf("else if %s (rule[%d]) then positive probability = %.8f\n", 
>                  x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
>   })
>   data.frame(rules = unlist(res))
> }
> 
> cervical2$label = cervical2$Biopsy
> cervical2$Biopsy = NULL
> rules  <-  sbrl(cervical2, pos_sign = "Cancer", neg_sign = "Healthy", rule_maxlen = 2)
> rn = rules$rulenames
> rl = get.sbrl.rules(rules)
> rl
> ```


| **rules**                                                                                          |
|:---------------------------------------------------------------------------------------------------|
| If {STDs=1} (rule[259]) then positive probability = 0.16049383                                     |
| else if {Hormonal.Contraceptives..years.=[0,10)} (rule[82]) then positive probability = 0.04685408 |
| else&nbsp; (default rule)&nbsp; then positive probability = 0.27777778                             |  

THEN 부분에 있는 예측을 보면 클래스 결과가 아닌 암에 대한 예측 확률입니다. 

조건들은 FP-Growth 알고리즘으로 pre-mined 된 패턴에서 선택되었습니다. 아래 표에는 SBRL이 결정 목록을 구축하기 위해 선택할 수 있는 conditions 의 pool 이 있습니다. 제가 허용한 조건이 가지는 최대 피쳐값 수는 2개였습니다. 아래는 조건 풀 중 10가지 패턴입니다:

> [!note]- code fold
> ```r
> #### sbrl-cervical-premined
> set.seed(1)
> conditions <- sample(rules$rulenames, size = 10)
> conditions <- gsub("\\{|\\}", "", conditions)
> conditions <- gsub(",", ", ", conditions)
> conditions %>% 
>   as.data.frame()
> ```

| **pre-mined conditions**                                                   |
|:---------------------------------------------------------------------------|
| Num.of.pregnancies=[3.67, 7.33)                                            |
| IUD=0, STDs=1                                                              |
| Number.of.sexual.partners=[1, 10), STDs..Time.since.last.diagnosis=[1, 8)  |
| First.sexual.intercourse=[10, 17.3), STDs=0                                |
| Smokes=1, IUD..years.=[0, 6.33)                                            |
| Hormonal.Contraceptives..years.=[10, 20), STDs..Number.of.diagnosis=[0, 1) |
| Age=[13, 36.7)                                                             |
| Hormonal.Contraceptives=1, STDs..Number.of.diagnosis=[0, 1)                |
| Number.of.sexual.partners=[1, 10), STDs..number.=[0, 1.33)                 |
| STDs..number.=[1.33, 2.67), STDs..Time.since.first.diagnosis=[1, 8)        |  

##### Example 2 : Rental bikes

다음으로 자전거 대여 수 예측 작업에 SBRL을 적용해 보겠습니다. 이는 회귀 문제이므로 binary 분류 문제로 바꾸어야 작동합니다. 여기서 저는 임의로 하루에 대여 수가 4,000대를 초과하면 1, 그렇지 않으면 0이라는 라벨을 만들어 분류 문제로 진행했습니다.

> [!note]- code fold
> ```r
> bike2 <- bike %>% 
>   mutate(label = cnt > 4000) %>% 
>   mutate(cnt = NULL) %>% 
>   lapply(function(x){
>     if(is.factor(x) || n_distinct(x) < 3){
>       as.factor(x)
>     } else {
>       arules::discretize(x, method = 'interval', 3)
>     }
>   }) %>% 
>   as.data.frame()
> 
> rules <- sbrl(bike2, pos_sign = TRUE, neg_sign = FALSE, rule_maxlen = 3)
> rules
> ```

|  **rules**                                                                                                                                                                                                                                                 |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  If {yr=2012,temp=[19.9,32.5]} (rule[1123]) then positive probability = 0.99253731                                                                                                                                                                         |
|  else if {season=WINTER,yr=2012,temp=[-5.22,7.35)} (rule[601]) then positive probability = 0.09259259                                                                                                                                                      |
|  else if {yr=2011,atemp=[18.7,27.3)} (rule[931]) then positive probability = 0.00961538                                                                                                                                                                    |
|  else if {yr=2011,atemp=[27.3,36)} (rule[937]) then positive probability = 0.41447368                                                                                                                                                           |
|  else (default rule) then positive probability = 0.85858586|  

체감 기온이 섭씨 30도인 2011년의 어느 날의 자전거 수가 4,000대를 초과할 확률을 예측해 보겠습니다. 1st ~ 3rd 규칙은 조건을 만족하지 않으므로 적용되지 않습니다. 4th 규칙을 보면 2011년과 체감온도 섭씨 30도가 \[27.3, 36)에 포함되기 때문에 적용됩니다. 따라서 이날 4,000대 이상의 자전거가 대여될 확률 예측치는 약 41.4% 입니다.

#### 4. Advantages

이번 섹션에서는 IF-THEN 규칙에 대해서 살펴 보았습니다. 

IF-THEN 규칙은 해석하기가 쉽습니다. 아마도 해석 가능한 모델들 중 가장 해석이 쉬울 것입니다. 이 모델이 제공하는 설명은 규칙의 수가 적고, 규칙의 조건도 짧으며 규칙들이 결정 목록이나 겹치지 않는 결정 집합으로 구성되어 있기 때문에 그렇습니다. 

결정 규칙 (DR) 은 DT 만큼 표현력이 뛰어나면서도 간결합니다. 또한 어떤 규칙이 적용되었는지 확인하기 위해서 몇 개의 이진 문만 확인하면 되므로 예측이 빠릅니다.

DR 은 조건의 임계값만 변하기 때문에 피쳐의 단조로운 변환에 robust 합니다. 또한 조건 적용 여부만이 중요하기 때문에 이상치에도 robust합니다. 

IF-THEN 규칙은 보통 sparse 한 모델을 만들어내므로 많은 피쳐를 사용하지 않습니다. 

OneR 처럼 간단한 규칙은 더 복잡한 알고리즘의 baseline으로 사용할 수 있습니다.

#### 5. Disadvantages

IF-THEN 규칙은 주로 분류에 초점이 맞추어져 있습니다. 회귀 문제는 다루지 못하기 때문에 연속형 target 을 이산화해야 하는데 이 과정에서 정보를 잃게 돼버립니다. 

숫자형 피쳐를 사용하려면 이들을 범주화해야합니다. 이에 대한 기준이 명확하지 않습니다.

오래된 규칙-학습 알고리즘 중 상당수는 과적합이 발생합니다. 이번 섹션에 소개된 3가지 알고리즘은 과적합 방지를 위한 최소한의 안전장치를 가지고 있긴 합니다 : OneR은 하나의 피쳐만 사용하고 (해당 피쳐의 레벨이 너무 많거나 피쳐가 너무 많을 때만 문제가 됨), RIPPER 는 가지치기(pruning)을 수행하며, BRL은 결정 목록에 사전 분포를 사용합니다. 

결정 규칙 (DR)은 피쳐와 outcome 간 선형 관계를 설명하는 데는 좋지 않습니다. 이는 의사결정트리(DT)와 마찬가지입니다. DT와 DR은 step function 과 같은 예측 함수만 생성할 수 있고, 예측치의 변화량은 불연속(discrete)한 step이고 smooth 한 곡선이 아닙니다. 이는 input이 항상 범주형이어야 한다는 것과 관련된 문제입니다. 

#### 6. Software and Alternatives

처음 예제에서 사용했던 OneR은 R의 `OneR` 패키지로 구현됩니다. `RWeka` 패키지에서 `OneR()`이 가능합니다. SC 알고리즘의 변형인 RIPPER 도 `RWeka` 패키지에서 구현이 가능합니다. Python 에서 결정 규칙 (DR)을 하려면 BRL, CORELS, OneR, greedy rule lists 등의 규칙 기반 모델을 통합된 scikit-learn 인터페이스로 제공하는 imodels 패키지를 추천합니다. 

---

## 05-06. RuleFit

Friedman and Popescu (2008)이 제안한 RuleFit 알고리즘은 결정 규칙 (DR)의 형태로 자동 감지된 상호작용 효과를 포함하는 희소 선형 모델(sparse linear model)을 학습합니다. 

선형 회귀 모델은 피쳐 간 상호작용을 설명하지 못합니다. 선형 모델처럼 간단하며 해석이 용이한데 피쳐 간 상호작용을 통합하는 모델이 있다면 편리하지 않을까라는 아이디어에서 출발합니다. RuleFit 은 원래의 피쳐와 결정 규칙인 새로운 피쳐들을 포함한 sparse linear model 을 학습합니다. 새로운 피쳐들은 기존의 피쳐 간 상호작용을 포착합니다. RuleFit 은 DT 에서 이러한 피쳐들을 자동으로 생성해냅니다. Tree 를 통과하는 각 경로(path)는 분할된 결정을 하나의 규칙으로 결합하여 결정 규칙(DR)로 변환할 수 있습니다. 노드 예측치들은 삭제되고 분할된 결정만이 결정 규칙(DR)에 사용됩니다 : 

![[Pasted image 20240101154324.png|center]] Figure 5.21 : 3개의 터미널 노드가 있는 tree에서 4개의 규칙이 생성됩니다.

이러한 DT는 어디서 나왔을까요? Tree 는 outcome 을 예측하는 작업을 학습하면서 여러 분할을 만들어냅니다. 랜덤 포레스트와 같이 여러 개의 trees 를 만들어내는 알고리즘은 RuleFit 에 사용할 수 있습니다. 각 Tree는 결정 규칙들로 분해되어 sparse linear model (Lasso)에서 추가적인 피쳐들로 사용됩니다. 

RuleFit 의 논문에서는 Boston 주택 데이터를 사용합니다 : 목표는 Boston 지역의 주택 가격의 중앙값을 예측하는 것입니다. RuleFit으로 생성된 규칙 중 하나입니다 : IF `number of rooms > 6.64` AND `concentration of nitric oxide < 0.67` THEN 1 ELSE 0.

RuleFit은 feature importance와 규칙을 함께 제공합니다. 변수 중요도는 회귀 모델의 가중치를 통해 계산됩니다. 이 중요도 값은 기존 피쳐에 대해 집계합니다. 

Rulefit 은 **부분 의존도 플롯(partial dependence plot; PDP)**을 도입하여 피쳐값이 변함에 따라 예측값의 평균 변화량을 보여줍니다. PDP는 모든 모델에 사용할 수 있는 model-agnostic 한 방법이며, 해당 내용을 다루는 섹션에서 설명하겠습니다.

#### 1. Interpretation and Example

RuleFit은 선형모델을 추정하기 때문에 해석하는 방법은 보통의 선형 모델과 동일합니다. 차이점은 결정 규칙(DR)에서 파생된 새로운 피쳐들을 사용한다는 것입니다. 결정 규칙을 통해 파생된 피쳐들은 binary 입니다: 1이면 모든 조건이 충족되는 것이고, 그렇지 않으면 0입니다. RuleFit의 linear terms에 대해서는 선형 회귀 모델과 해석을 똑같이 합니다: 피쳐가 한 단위 증가하면 해당 피쳐의 가중치만큼 예측치가 변합니다.

아래 예제에서는 주어진 날짜에 대여된 자전거 수를 예측하기 위해 RuleFit을 사용했습니다. 표에는 RuleFit으로 만든 5개의 규칙과 해당 규칙을 피쳐로 했을 때의 Lasso 가중치와 변수 중요도가 함께 있습니다. 계산은 이 섹션의 뒷부분에서 설명하겠습니다.

> [!note]- code fold
> ```r
> ## prepare-rulefit
> X <- bike %>% select(all_of(bike.features.of.interest))
> 
> ### round features so that table is better
> X <- X %>% 
>   mutate(across(c(temp, hum, windspeed), ~round(.x, 0)))
> 
> y <- bike %>% pull(cnt)
> dat <- cbind(X,y)
> 
> mod <- pre(y ~ ., data = dat, maxdepth = 2, ntrees = 100)
> coeffs <- coef(mod)
> coeffs$description[is.na(coeffs$description)] <- coeffs$rule[is.na(coeffs$description)]
> coeffs <- left_join(coef(mod), pre::importance(mod, plot = F)$baseimp)
> coeffs <- coeffs[!is.na(coeffs$coefficient), ]
> coeffs <- coeffs %>% 
>   mutate(across(c(imp, coefficient), ~ round(.x, 1))) %>% 
>   mutate(sd = round(sd, 2))
> coeffs$rule <- NULL # rule 칼럼 삭제
> 
> coeffs <- coeffs %>% 
>   filter(!is.na(imp)) %>% 
>   arrange(desc(imp))
> 
> coeffs <- coeffs %>% 
>   mutate(description = gsub("\\%", "", description),
>          description = gsub("c\\(", "(", description))
> 
> coeffs %>% head(5) %>% select(-sd) %>% 
>   select(Description = description, Weight = coefficient, Importance = imp)
> ```

| Description                                                    | Weight | Importance |
|:---------------------------------------------------------------|:-------|:-----------|
| days_since_2011 > 111 & weathersit in ("GOOD", "MISTY")        |  862.9 |      329.2 |
|                                             37.25 <= hum <= 90 |  -14.2 |        194 |
| temp <= 12 & season in ("WINTER")                              | -444.9 |      186.5 |
| temp > 8 & days_since_2011 > 391                               |  354.1 |      170.6 |
| days_since_2011 > 499 & season in ("SPRING", "SUMMER", "FALL") |  369.5 |      169.6 |  


가장 중요한 규칙은 `days_since_2011 > 111 & weathersit %in% c("GOOD", "MISTY")`이고 가중치는 862.9 입니다. 해석하자면 `days_since_2011 > 111 & weathersit %in% c("GOOD", "MISTY")` 이라면 다른 모든 피쳐가 고정되어 있을 때 예측 대여 수가 862.9대 증가한다는 뜻입니다. 기존 8개의 피쳐에서 총 252개의 규칙 및 피쳐들이 만들어졌습니다. 꽤 많은 수지만 Lasso 덕분에 252개 중 35개의 가중치만 살아남았습니다.

Global feature imortances 를 계산해보면 온도 `temp`와 시간 추세 `days_since_2011`이 가장 중요한 피쳐임을 알 수 있습니다:

![[Pasted image 20240101165341.png|center]]Figure 5.22 : RuleFit 모형의 Global Feature importance

이러한 global 피쳐 중요도를 계산할 때는 기존의 피쳐와 해당 피쳐가 나타나는 모든 결정 규칙의 중요도가 통합됩니다.

##### Interpretation template

해석은 선형 모델과 유사하게 수행됩니다 : 다른 모든 피쳐가 고정되어 있다면 $j$ 번째 피쳐 $x_j$가 한 단위 변할 때 예측치는 $\beta_j$만큼 변합니다. 결정 규칙의 가중치 해석은 다릅니다 : $k$ 번째 결정 규칙 $r_k$ 의 모든 조건이 적용된다면 예측치는 $\alpha_k$ 만큼 변합니다. 여기서 $\alpha_k$ 는 규칙 $r_k$ 의 학습된 가중치입니다. 

분류 문제의 경우 로지스틱 회귀라고 했을 때 : 결정 규칙 $r_k$ 의 조건이 적용되면 event vs non-event 의 odds는 $\alpha_k$ 만큼 변합니다.

#### 2. Theory

RuleFit 알고리즘의 기술적 세부사항에 대해 좀 더 자세히 알아보겠습니다. RuleFit은 2가지로 구성되어 있습니다 : 첫 번째로는 DT에서 '규칙(rules)'을 만들고, 두 번째로 기존 피쳐와 새로운 규칙들을 input으로 하여 선형 모델을 fitting 하는 것입니다. 

##### Step 1: Rule generation

생성된 규칙은 예를 들어보면 IF `x2 < 3` AND `x5 < 7` THEN `1` ELSE `0` 와 같은 형태를 가집니다. 이러한 규칙은 결정 트리(DT)를 분해해서 만듭니다 : tree의 노드에 대한 모든 경로는 결정 규칙으로 변환됩니다. 규칙에 쓰이는 tree는 target 예측 모델에 적합됩니다. 따라서 분할과 규칙은 outcome 예측에 최적화됩니다. 특정한 노드로 이어지는 binary decision 들을 "AND"로 연결하면 규칙이 완성됩니다. 여러 다양한 규칙을 많이 만드는 것이 좋습니다. GBM 계열의 부스팅 방법도 가능하고, 모든 tree-ensemble 알고리즘을 사용해 RuleFit을 위한 tree를 만들 수 있습니다. Tree ensemble의 일반적인 공식은 다음과 같습니다 : 
$$
\hat{f}(x)=a_0+\sum_{m=1}^M a_m \hat{f}_m(X)
$$
여기서 $M$ 은 tree의 개수이고, $\hat{f}_m(x)$는 $m$ 번째 tree의 예측 함수입니다. 그리고 $a$ 는 가중치입니다. Bagged ensembles, random forest, AdaBoost, MART 는 tree ensemble 을 생성하므로 RuleFit에 사용 가능합니다. 

앙상블 내 모든 tree 로부터 규칙을 만들어 냅니다. $m$ 번째 규칙 $r_m$은 다음과 같은 형태입니다 : 
$$
r_m(x)=\prod_{j \in \mathrm{T}_m} I\left(x_j \in s_{j m}\right)
$$
여기서 $T_m$ 은 $m$번째 tree에 사용된 피쳐들의 집합이고, $I$는 지시함수로 $j$번째 피쳐 $x_j$가 분할 기준에 부합하는 subset 에 포함되면 1을 갖게 합니다. 숫자형 피쳐에 대해서 $s_{jm}$은 해당 피쳐의 구간입니다. 이 구간은 둘 중 하나의 형태입니다 : 
$$
\begin{aligned}
& x_{s_{j m}, \text { lower }}<x_j \\
& x_j<x_{s_{j m}, \text { upper }}
\end{aligned}
$$

피쳐를 더 분할하면 더욱 복잡한 구간이 만들어집니다. 범주형 피쳐의 경우 subset $s$ 에는 피쳐의 특정 수준들이 포함됩니다. 

자전거 대여 데이터를 예로 들면: 
$$
\begin{gathered}
r_{17}(x)=I\left(x_{\mathrm{temp}}<15\right) \cdot I\left(x_{\text {weather }} \in\{\text { good }, \text { cloudy }\}\right) \\
\cdot I\left(10 \leq x_{\text {windspeed }}<20\right)
\end{gathered}
$$
이 규칙은 3가지 조건이 모두 만족되어야만 1 값을 갖고 그렇지 않으면 0입니다. RuleFut 은 leaf node 뿐 아니라 tree 에서 가능한 모든 규칙을 뽑아냅니다 : 
$$
r_{18}(x)=I\left(x_{\text {temp }}<15\right) \cdot I\left(x_{\text {weather }} \in\{\text { good }, \text { cloudy }\}\right)
$$
$M$개의 tree ensemble에서 생성되는 규칙의 총 개수는 다음과 같습니다. 이는 $t_m$개의 터미널 노드가 있는 tree ensemble 에서 생성되는 규칙의 수 입니다 : 
$$
K=\sum_{m=1}^M 2\left(t_m-1\right)
$$

RuleFit 개발자가 도입한 트릭은 길이가 서로 다른 규칙이 많이 생성되도록 random 한 깊이를 갖는 trees 들을 학습하는 것입니다. 각 노드에서 예측된 값은 삭제하고 노드로 연결되는 조건만 유지한 다음 이로부터 규칙을 생성한다는 것에 유의하세요. 결정 규칙의 가중치는 step 2에서 수행됩니다. 

step 1에 대한 다른 시각 : Rulefit은 기존 피쳐에서 새로운 피쳐 집합을 생성합니다. 이러한 피쳐들은 binary이며 기존 피쳐로부터 복잡한 상호작용을 나타냅니다. 규칙은 예측 작업을 극대화/최적화하는 방향으로 선택됩니다. 

##### Step 2: Sparse linear model

Step 1 을 통해 많은 규칙을 얻을 수 있었습니다. Step1 은 피쳐 변환으로만 볼 수 있으므로 아직 모델 적합은 되지 않았습니다. 또한 규칙의 수가 너무 많습니다. 규칙 외에도 기존 피쳐들도 포함해서 sparse linear model을 진행합니다. 모든 규칙과 기존 피쳐는 이 선형 모델에서 features 가 되고 가중치에 대한 추정치를 얻습니다. 여기서 기존 피쳐를 추가하는 이유는 tree 가 $y$ 와 $x$ 간 단순 선형 관계를 표현하지 못하기 때문입니다. Sparse linear model 훈련 전에 기존 피쳐를 윈저화(winsorize)해서 해당 피쳐의 이상값(outlier)에 robust 하게 만들겠습니다 :
$$
l_j^*\left(x_j\right)=\min \left(\delta_j^{+}, \max \left(\delta_j^{-}, x_j\right)\right)
$$

여기서 $\delta_j^-$와 $\delta_j^+$는 $j$ 번째 피쳐 $x_j$의 $\delta$ 사분위수의 분포입니다. $\delta$를 0.05로 한다는 것은 $x_j$의 하위 5%와 상위 5%의 값들을 상/하위 5%의 사분위수로 대체하는 것입니다. 일반적으로 $\delta = 0.025$를 사용합니다. 추가적으로 선형 항은 다른 결정 규칙들과 동일한 중요도를 갖게끔 정규화(normalize) 해야 합니다: 
$$
l_j\left(x_j\right)=0.4 \cdot l_j^*\left(x_j\right) / \operatorname{std}\left(l_j^*\left(x_j\right)\right)
$$
- The 0.4 is the average standard deviation of rules with a uniform support distribution of $s_k\sim U(0,1)$.

이러한 2가지 유형의 피쳐들을 결합해서 새로운 feature matrix를 만들고, Lasso 를 이용해 sparse linear model 을 학습합니다 : 
$$
\hat{f}(x)=\hat{\beta}_0+\sum_{k=1}^K \hat{\alpha}_k r_k(x)+\sum_{j=1}^p \hat{\beta}_j l_j\left(x_j\right)
$$

여기서 $\hat{\alpha}$는 규칙 피쳐의 추정 가중치이고, $\hat{\beta}$는 기존 피쳐의 가중치 입니다. RuleFit이 Lasso를 사용하기 때문에, loss function는 일부 가중치가 0을 얻게끔 강제하는 제약이 생깁니다 : 

$$
\left(\{\hat{\alpha}\}_1^K,\{\hat{\beta}\}_0^p\right)=\operatorname{argmin}_{\{\alpha\}_1^K,\{\beta\}_0^p} \sum_{i=1}^n L\left(y^{(i)}, f\left(x^{(i)}\right)\right) \\
 +\quad \lambda \cdot\left(\sum_{k=1}^K\left|\alpha_k\right|+\sum_{j=1}^p\left|\beta_j\right|\right)
$$

그 결과로 (기존 피쳐+규칙) 모두 선형 효과를 갖는 모델이 생성됩니다. 해석은 선형 모델과 동일하지만, 일부 피쳐가 binary라는 점만 다릅니다. 

##### Step 3 (optional) : Feature Importance

기존 피쳐인 선형 항의 경우, 변수 중요도는 표준화되어 계산됩니다 : 
$$
I_j=\left|\hat{\beta}_j\right| \cdot \operatorname{std}\left(l_j\left(x_j\right)\right)
$$
여기서 $\hat{\beta}_j$는 Lasso model의 가중치이고, $std(l_j(x_j))$는 선형 항의 표준편차입니다. 

Rule terms에 대해서 중요도는 아래와 같이 계산됩니다 : 
$$
I_k=\left|\hat{\alpha}_k\right| \cdot \sqrt{s_k\left(1-s_k\right)}
$$
여기서 $\hat{\alpha}_k$는 결정 규칙에 대한 Lasso 가중치이며, $s_k$는 결정 규칙이 적용되는 데이터 포인트의 백분율인 데이터에서 피쳐의 support 입니다(여기서 $r_k(x)=1$) : 

$$
s_k=\frac{1}{n} \sum_{i=1}^n r_k\left(x^{(i)}\right)
$$
이렇게 RuleFit의 Lasso sparse linear model 에서는 피쳐가 선형항 또는 규칙항으로 나타납니다. 그렇다면 피쳐의 total / global 중요도는 어떻게 계산할까요? $j$ 번째 피쳐의 중요도 $J_j(x)$는 개별 예측치 각각에 대해 계산됩니다 : 
$$
J_j(x)=I_j(x)+\sum_{x_j \in r_k} I_k(x) / m_k
$$
여기서 $I_j$ 는 선형 항의 중요도이고, $I_k$는 $x_j$가 나타나는 규칙 항의 중요도입니다. 그리고 $m_k$는 규칙 $r_k$를 구성하는 피쳐의 개수입니다. 모든 인스턴스로부터 피쳐 중요도를 더함으로써 global feature importance를 계산할 수 있습니다 : 
$$
J_j(X)=\sum_{i=1}^n J_j\left(x^{(i)}\right)
$$
인스턴스의 subset 을 그룹으로 보고 이 그룹에 대한 변수 중요도를 계산할 수도 있습니다. 

#### 3. Advantages

RuleFit은 선형 모델에 피쳐 간 상호작용을 자동으로 추가합니다. 따라서 상호작용 항을 수동으로 추가해야 하는 기존 선형 모델의 문제를 해결하고 비선형 관계를 모델링하는 것도 어느정도 도움이 됩니다. 

RuleFit 은 분류와 회귀 작업 모두 처리할 수 있습니다. 

생성되는 규칙들은 binary 이기 때문에 해석이 쉽습니다. 다만 규칙 내 조건이 너무 많지 않아야 그렇습니다. 1~3개의 조건이 적당한 것으로 보이는데, 이는 tree ensemble의 각 tree에 대한 max depth가 3이라는 것을 의미합니다. 

모델에 많은 규칙이 있더라도 모든 인스턴스에 적용되는 것이 아닙니다. 개별 인스턴스에는 소수의 규칙만 적용됩니다(0이 아닌 가중치만!). 이렇게 함으로써 해석 가능성이 향상되는 것입니다. 

RuleFit은 다양한 diagnostic tools 를 제공합니다. 이는 model-agnostic합니다. 

#### 4. Disadvantages

때로는 RuleFit이 Lasso에서 0이 아닌 가중치를 규칙을 많이 생성하기도 합니다. 그렇게되면 자연스럽게 해석 가능성은 저하됩니다. 이는 feature effects 를 단조롭게 제한함으로써 해소할 수 있습니다. 즉, 피쳐의 증감이 예측치의 증감으로 이어지도록 말입니다. 

사례적인 단점 : 논문에서는 RuleFit의 성능이 좋다고 주장하지만 실제 적용 시 성능이 그렇게 좋지 못합니다. 

RuleFit의 최종 결과물은 규칙 피쳐가 추가된 선형 모델이지만 그렇기 때문에 가중치의 해석이 직관적이지 않습니다. 일반적인 선형 모델처럼 다른 피쳐들을 고정해야 한다는 가정이 필요합니다. 예를 들어 자전거 예측 문제에서 어떤 규칙은 다음과 같습니다 : 1) `temp > 10`, 2) `temp > 15 & weather = 'GOOD'`. 날씨가 좋고 기온이 15보다 높으면 자동으로 기온은 10도보다 높습니다. 두 번째 규칙이 적용되면 첫 번째 규칙은 자동으로 적용됩니다. 두 번째 규칙의 추정된 가중치에 대해 해석해보자면 다른 모든 피쳐가 고정돼 있을 때 예상되는 자전거 대여 수는 $\beta_2$ 만큼 변합니다. 그러나 이 규칙이 적용되면 첫 번째 규칙도 적용되기 때문에 해석이 무의미해집니다. 

#### 5. Software and Alternative

RuleFit 알고리즘은 Fokkema 와 Christoffersen (2017)에 의해 R로 구현되어 있으며 Python 버전은 [Github](https://github.com/christophM/rulefit)에 제공됩니다. 

유사한 프레임워크는 앙상블에서 규칙을 추출하는 Python 모듈인 [skope-rules](https://github.com/scikit-learn-contrib/skope-rules)입니다. 최종 규칙을 학습하는 방식이 다릅니다 : 먼저 skope-rules는 recall 과 precision의 임계값에 따라 규칙을 제거합니다. 그런 다음 logical/bool term의 다양성(variable + larger/smaller operator)과 규칙의 성능(f1-score)을 기준으로 선택을 수행해 중복 및 유사 규칙을 제거합니다. 마지막 단계는 Lasso 를 사용하지 않고 f1-score와 규칙을 구성하는 logical term 만 고려합니다.

[imodels](https://github.com/csinva/imodels) 패키지에는 Bayesian rule sets, Boosted rule sets, SLIPPER rule sets 등을 갖춘 scikit-learn 인터페이스를 갖춘 패키지를 제공합니다. 

---

## 05-07. Other Interpretable models

#### 1. Naive Bayes classifier

나이브 베이즈 분류기는 조건부 확률에 대한 베이즈 정리를 씁니다. 각 피쳐에 대해 피쳐 값에 따라 클래스에 대한 확률을 계산합니다. 이 분류기는 각 피쳐에 대한 클래스 확률을 독립적으로 계산하는데, 이는 피쳐의 조건부 독립성에 대한 강한(=naive) 가정입니다. 나이브 베이즈는 조건부 확률 모델인데 클래스 $C_k$의 확률은 다음과 같이 모델링 합니다 : 
$$
P\left(C_k \mid x\right)=\frac{1}{Z} P\left(C_k\right) \prod_{i=1}^n P\left(x_i \mid C_k\right)
$$
$Z$는 모든 클래스에 대한 확률의 합이 1이 되도록 스케일링합니다. 클래스의 조건부 확률은 해당 클래스 확률에 주어진 각 피쳐의 확률을 곱한 값으로 베이즈 정리를 통해 도출됩니다. 

나이브 베이즈는 독립성 가정으로 인해 해석 가능한 모델입니다. 모듈 수준에서 해석할 수 있습니다. 조건부 확률을 해석할 수 있기 때문에 각 피쳐가 특정 클래스 예측에 얼마나 기여하는지 명확히 알 수 있습니다. 

#### 2. K-Nearest Neighbors

KNN은 회귀 및 분류에 쓰이며 예측 모델로 데이터 포인트의 가장 가까운 이웃들을 사용합니다. 분류의 경우 knn은 인스턴스의 가장 가까운 이웃중 다수의 클래스를 할당합니다. 회귀의 경우 이웃의 결과의 평균을 취합니다. k 값 찾기와 이웃을 정의하는 거리 측정 방법에 대한 결정이 해결해야 할 문제입니다. 

KNN은 인스턴스 기반 학습 알고리즘이므로 이 책에서 소개하는 다른 해석 가능한 모델들과는 다릅니다. 학습할 모수가 없으므로 모듈 수준에서 해석을 할 수 없습니다. 또한 모델이 본질적으로 로컬 모델이고 명시적으로 학습된 글로벌한 가중치나 구조가 없어서 글로벌 모델 해석 가능성이 부족합니다. 인스턴스들이 수많은 피쳐들로 구성되어 있다면 해석이 불가능하겠지만, 적은 수의 피쳐들만으로 이루어져 있다면 NN 들을 제시하면 좋은 해석을 얻을 수 있을 것입니다. 

