---
sticker: emoji//1f525
tags:
  - Interpretability-Machine-Learning
  - ML
  - XAI
  - ICE-plot
  - LIME
  - Counterfactual
  - Anchors
  - Shapley-Values
  - SHAP
  - TreeSHAP
  - KernelSHAP
  - bike-rentals
  - youtube-spam
  - cervical-cancer
---
Local interpretation methods 는 개별 예측치를 설명합니다. 이 챕터에서는 아래의 local 방법들을 배울 것입니다 : 

- [Individual conditional expectation curves](https://christophm.github.io/interpretable-ml-book/ice.html#ice); ICE curves는 PDP의 구성 요소이며, 피쳐를 변경시키면 예측치가 어떻게 달라지는지를 설명합니다.
- [Local surrogate models](https://christophm.github.io/interpretable-ml-book/lime.html#lime); LIME은 복잡한 모델을 로컬에서 interpretable surrogate model로 대체하여 예측치를 설명합니다. 
- [Scoped rules (anchors)](https://christophm.github.io/interpretable-ml-book/anchors.html#anchors) 는 예측치를 제자리에 고정한다는 의미에서 어떤 피쳐 값이 예측치를 고정시키는지를 설명하는 규칙입니다. 
- [Counterfactual explanations](https://christophm.github.io/interpretable-ml-book/counterfactual.html#counterfactual)는 원하는 예측(치)을 달성하기 위해 어떤 피쳐를 변경해야 하는지를 검토하여 예측을 설명합니다. 
- [Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html#shapley)는 예측치를 개별 피쳐에 공정하게 할당하는 기여도(attribution) 방법입니다. 
- [SHAP](https://christophm.github.io/interpretable-ml-book/shap.html#shap)은 shapley values 의 또 다른 계산 방법이지만, 데이터 전체에 걸친 shapley values의 조합을 기반으로 한 global interpretation methods 도 제안합니다.

LIME과 Shapley values는 attribution methods로, 단일 인스턴스에 대한 예측치가 피쳐 효과들의 합으로 설명됩니다. Counterfactual explanations과 같은 방법은 example-based 방법입니다.

## 09-01. Individual Conditional Expectation ; ICE

ICE plots는 피쳐에 따라 특정 인스턴스의 예측치가 어떻게 변하는지를 보여주는 나타내는 하나의 선을 인스턴스별로 나타냅니다. 

피쳐의 평균 효과 (average effect)를 나타내는 PDP는 특정 인스턴스에 초점을 맞추지 않고 전체 평균에 관심을 두기 때문에 global method 입니다. 개별 인스턴스에 대한 PDP가 ICE plot 입니다. ICE plot은 각 인스턴스의 피쳐에 대한 예측치의 종속성을 개별적으로 시각화하여, (PDP에서 전체가 한 줄인 것인 반면) 인스턴스 당 한 줄로 표시됩니다. PDP는 ICE plot의 선들의 평균값입니다. 하나의 선 (및 하나의 인스턴스)의 값은 다른 모든 피쳐들을 동일하게 유지하고, 해당 피쳐 값을 grid 값으로 대체하여 이 인스턴스의 변형을 생성하고, 새로 생성된 인스턴스에 대해 black box model로 예측하여 계산합니다. 그 결과, grid 값으로 대체된 피쳐 값과 각각의 예측치가 포함된 인스턴스의 포인트 집합이 생성됩니다. 

PDP 대신 개별 기대값을 살펴보는 이유가 무엇일까요? PDP는 상호작용으로 인해 생성된 이질적인 (heterogeneous) 관계를 흐리게 만듭니다. PDP는 피쳐와 예측치 간 평균 관계를 보여줍니다. 이는 PDP를 계산하는 피쳐와 다른 피쳐 간 상호작용이 약한 경우에만 잘 작동합니다. 상호작용이 존재하는 경우에는 ICE plot이 더 많은 인사이트를 제공합니다. 

ICE plots에서, $\{(x_S^{(i)}, x_C^{(i)}\}_{i=1}^N$ 의 각 인스턴스에 대해 곡선 $\hat{f}_S^{(i)}$는 $x_S^{(i)}$에 대해 그려지는데, 그 동안 $x_C^{(i)}$는 고정된 값입니다.

## 1. Examples

자궁경부암 데이터셋에서 각 인스턴스의 예측치가 '나이 (`Age`)' 피쳐와 어떻게 연관되어 있는지 살펴보겠습니다. 여러 위험 요소들이 주어졌을 때, 여성의 암 발생 확률을 예측하기 위해 random forest 를 사용하겠습니다. PDP에서 50세 전후로 암 확률이 증가함을 보았지만, 이 결과가 모든 여성에게 해당될까요? ICE plot을 보면 대부분의 여성에게 `Age` 효과는 50세부터 증가하는 평균 패턴 (average pattern)을 따라가지만, 몇 가지 예외가 있음을 알 수 있습니다 : 젊은 나이에 예측 확률이 높은 소수의 여성의 경우, 예측 암 확률은 나이에 따라서 크게 변하지 않습니다. 

> [!note]- code fold
> ```r
> ## ice-cervical 
> set.seed(43)
> cervical_subset_index <- sample(1:nrow(cervical), size = 300)
> cervical_subset <- cervical %>% slice(cervical_subset_index)
> cervical.task <- makeClassifTask(data = cervical, target = 'Biopsy')
> mod <- mlr::train(learner = makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), task = cervical.task)
> pred.cervical <- Predictor$new(model = mod, data = cervical_subset, class = 'Cancer')
> ice <- FeatureEffect$new(predictor = pred.cervical, feature = "Age", method = 'ice')$plot() +
>   scale_color_discrete(guide = 'none') +
>   scale_y_continuous('Predicted cancer probability')
> ice
> ```

![[Pasted image 20240105003248.png|center]] Figure 9.1 : 연령에 따른 자궁경부암 확률의 ICE plot. 각 선은 하나의 여성을 나타냅니다. 대부분의 여성들이 연령이 증가함에 따라 예상 암 확률이 증가합니다. 그러나 예측 암 확률이 0.4 이상에서 일부 여성은 연령이 높아져도 예측 확률이 크게 변하지 않습니다. 

다음 그림은 자전거 대여 수 예측 문제에 대한 ICE plot 입니다. 사용된 예측 모델은 random forest 입니다 : 

> [!note]- code fold
> ```r
> ## ice-bike
> set.seed(42)
> bike.subset.index <- sample(1:nrow(bike), size = 300)
> bike.subset <- bike %>% slice(bike.subset.index)
> bike.task <- makeRegrTask(data = bike, target = 'cnt')
> mod.bike <- mlr::train(learner = makeLearner(cl = 'regr.randomForest', id = 'bike-rf'), task = bike.task)
> pred.bike <- Predictor$new(model = mod.bike, data = bike.subset)
> p1 <- FeatureEffect$new(predictor = pred.bike, feature = 'temp', method = 'ice')$plot() +
>   scale_x_continuous('Temperature') +
>   scale_y_continuous('Predicted bicycle rentals')
> p2 <- FeatureEffect$new(predictor = pred.bike, feature = 'hum', method = 'ice')$plot() +
>   scale_x_continuous('Humidity') +
>   scale_y_continuous('')
> p3 <- FeatureEffect$new(predictor = pred.bike, feature = 'windspeed', method = 'ice')$plot() +
>   scale_x_continuous('Windspeed') +
>   scale_y_continuous('')
> gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
> ```

![[Pasted image 20240105005945.png|center]] Figure 9.2 : 날씨 조건에 따른 대여 수 예측치의 ICE plots. PDP에서와 동일한 효과를 볼 수 있습니다. 

모든 곡선이 동일한 경로를 따르는 것처럼 보이므로 뚜렷한 상호작용이 없습니다. 즉, PDP는 이미 위 피쳐들과 예측 대여 수 사이의 관계를 잘 요약하고 있습니다. 

#### Centered ICE Plot

ICE plots/curves 는 개개의 예측치를 이용하기에 서로 다른 예측치를 가지고 시작하기 때문에 개인 간에 차이가 있는지 구분하기가 어렵다는 문제점이 있습니다. 간단한 해결책은 피쳐의 특정 값으로 중심화 해놓고 이 점까지의 예측치의 차이값만 표시하는 것입니다. 그 결과 플롯을 **centered ICE ; c-ICE** plot 이라고 합니다. 피쳐의 아래 값 끝에 곡선을 고정시키는 것이 좋습니다. 해당 곡선은 다음과 같이 정의됩니다 : 
$$
\hat{f}_{\text {cent }}^{(i)}=\hat{f}^{(i)}-\mathbf{1} \hat{f}\left(x^a, x_C^{(i)}\right)
$$
여기서 $\mathbf{1}$ 는 차원 수 (보통 1개 아니면 2개)에 해당하는 one's vector이고, $\hat{f}$ 는 적합 모델, $x^a$ 가 기준/고정점 입니다.

#### Example

나이/연령에 따른 자궁경부암 ICE plot 을 가져와서 관찰된 가장 어린 나이를 기준으로 선들을 중앙에 배치해 보겠습니다 : 

> [!note]- code fold
> ```r
> ## ice-cervical-centered
> predictor <- Predictor$new(model = mod, data = cervical_subset, class = "Cancer")
> ice <- FeatureEffect$new(predictor = predictor, feature = 'Age', center.at = min(cervical_subset$Age), method = 'pdp+ice')
> ice$plot() +
>   scale_color_discrete(guide = 'none') +
>   scale_y_continuous(sprintf("Cancer probability difference to age %i", min(cervical_subset$Age)))
> ```

![[Pasted image 20240105135643.png|center]] Figure 9.3 : 연령별 예측 암 확률에 대한 c-ICE plot입니다. 선들은 14세에서 0으로 고정되어 시작합니다. 14세와 비교했을 때 대부분의 여성에 대한 예측치가 45세까지는 변하지 않습니다. 

c-ICE plot을 사용하면 개별 사례들의 곡선을 더 쉽게 비교할 수 있습니다. 이는 (예측치의 절대적인 변화가 아닌) 피쳐의 범위 중 고정된 값과 비교한 예측치의 차이를 보고자 할 때 유용합니다. 

이번에는 자전거 대여 수 예측에 대한 c-ICE plot을 살펴보겠습니다 : 

> [!note]- code fold
> ```r
> ## ice-bike-centered
> set.seed(43)
> bike.subset.index <- sample(1:nrow(bike), 100)
> bike.subset <- bike %>% slice(bike.subset.index)
> 
> predictor <- Predictor$new(model = mod.bike, data = bike.subset)
> ytext1 <- sprintf("Different to prediction at temp = %.2f", min(bike$temp))
> ice1 <- FeatureEffect$new(predictor = predictor, feature = 'temp', center.at = min(bike$temp), method = 'pdp+ice')$plot() +
>   scale_y_continuous(ytext1)
> 
> ytext2 <- sprintf("Different to prediction at hum = %.2f", min(bike$hum))
> ice2 <- FeatureEffect$new(predictor = predictor, feature = 'hum', center.at = min(bike$windspeed), method = 'pdp+ice')$plot() +
>   scale_y_continuous(ytext2)
> 
> ytext3 <- sprintf("Different to prediction at windspeed = %.2f", min(bike$windspeed))
> ice3 <- FeatureEffect$new(predictor = predictor, feature = 'hum', center.at = min(bike$windspeed), method = 'pdp+ice')$plot() +
>   scale_y_continuous(ytext3)
> gridExtra::grid.arrange(ice1, ice2, ice3, ncol = 3)
> ```

![[Pasted image 20240105140449.png|center]] Figure 9.4 : 날씨 조건에 따른 자전거 대여 수 예측치에 대한 c-ICE plots 입니다. 선드은 각 피쳐들이 최소일 때의 예측치와 비교하여 예측치의 차이를 나타냅니다. 

#### Derivative ICE Plot

이질성 (heterogeneity)을 시각적으로쉽게 파악할 수 있는 또 다른 방법은 피쳐에 대한 예측 함수의 개별 도함수 (derivatives)를 살펴보는 것입니다. 이러한 플롯은 **derivative ICE ; d-ICE** plot이라고 합니다. 함수/곡선의 미분은 변화가 발생하는지 여부와 그 방향을 알려줍니다. d-ICE plot을 사용하면 (적어도 일부) 인스턴스에 대해 black box 예측치가 변경되는 피쳐값의 범위를 쉽게 찾을 수 있습니다. 분석된 피쳐 $x_S$와 다른 피쳐들 $x_C$ 사이에 상호작용이 없는 경우 예측 함수는 아래와 같이 표현할 수 있습니다 : 
$$
\hat{f}(x)=\hat{f}\left(x_S, x_C\right)=g\left(x_S\right)+h\left(x_C\right), \quad \text { with } \quad \frac{\delta \hat{f}(x)}{\delta x_S}=g^{\prime}\left(x_S\right)
$$
상호작용이 없으면 개별 편도함수 (partial derivatives)는 모든 인스턴스에 대해 동일해야 합니다. 다르다면 상호작용에 인한 것이며, 이는 d-ICE plot에서 관측할 수 있습니다. $S$ 의 피쳐들에 대한 예측 함수의 미분에 대한 개별 곡선을 표시하는 것 외에도 미분 값의 표준 편차를 나타내면 추정 미분에서 이질성이 있는 $S$ 의 피쳐 영역을 강조할 수 있습니다. 하지만 d-ICE plot을 계산하는 것은 시간이 많이 걸리고 비실용적입니다. 

### 2. Advantages

- ICE plot/curves는 PDP보다 훨씬 직관적으로 이해할 수 있습니다. 하나의 선은 피쳐 값에 따른 하나의 인스턴스에 대한 예측치의 변화를 나타냅니다. 

- PDPs와 달리 ICE curves는 이질적인 관계를 발견할 수 있습니다. 

### 3. Disadvantages

- ICE curves는 **하나의 피쳐만 (의미있게) 표시할 수 있는데**, 2개의 피쳐를 나타낼 수 없습니다. 
- 관심 피쳐가 다른 피쳐와 상관관계에 있는 경우, joint feature distribution에 따라 **선들의 일부 점들이 유효하지 않은 데이터 포인트가 될 수 있다** 는 점에서 ICE plots 은 PDPs 와 동일한 문제를 겪습니다. 
- 많은 ICE curves를 그리면 **플롯이 너무 빽빽해져서** 가독성이 떨어집니다. 이는 선들에 약간의 투명도를 추가하거나 몇 개의 샘플만에 대해 그리는 것입니다. 
- ICE plots 에서 **평균을 보기가 어려울 수 있습니다**. 이 문제에 대한 간단한 해결책은 PDP plot과 ICE plot을 결합해서 그리는 것입니다. 

### Software and Alternatives

ICE plots 는 R에서 `iml` 패키지로 이용이 가능하고, `ICEbox`와 `pdp` 패키지로도 구현이 가능합니다. 유사한 기능을 제공하는 것은 `condvis` 패키지입니다. Python에서는 scikit-learn 0.24.0 버전 부터 pdp 가 내장되어 있습니다.

--- 

## 09-02. Local Surrogate ; LIME

Local surrogate model은 black box ML model의 개별 예측치를 설명하는 데 사용하는 interpretable model입니다. **Local interpretable model-agnostic explanations ; LIME**은 많은 논문 저자들이 제안하는 local surrogate model의 구현입니다. 앞서 배웠다시피 surrogate model은 black box model의 예측치를 근사하도록 학습됩니다. LIME은 (global surrogate model이 아닌) 개별 예측치를 설명하기 위해 local surrogate model 을 학습하는 것에 중점을 둡니다. 

아이디어는 매우 직관적입니다. 먼저 train data는 잊어버리고, 데이터 포인트를 입력하고 모델 예측치를 얻을 수 있는 black box model 있다고 생각하면 됩니다. 이 black box는 원하는 만큼 자주 조사할 수 있다고 합시다. 목표는 ML 모델이 특정 예측치를 내뱉은 이유를 알아내는 것입니다. LIME은 ML model에 데이터의 변형을 주면 예측치가 어떻게 변하는지 테스트합니다. LIME은 교란된 (perturbed) 샘플과 black box model의 해당되는 예측치로 구성된 새로운 데이터셋을 만듭니다. 이 새로운 데이터셋에 대해 LIME은 샘플링된 인스턴스와 관심 인스턴스의 근접성 (proximity)에 따라 가중치를 부여하는 interpretable model을 학습시킵니다. 이 때 사용하는 interpretable model은 Lasso, CART와 같이 [🔥CH05. Interpretable Methods](🔥CH05. Interpretable Methods.md) 에서 다루는 모든 것이 가능합니다. 학습된 모델은 로컬에서 ML model의 예측치의 좋은 근사치여야 하지만, 반드시 좋은 global 근사치 (approximation)일 필요는 없습니다. 이러한 종류의 정확도를 **local fidelity** 라고 합니다. 

수학적으로 interpretability의 제약이 있는 local surrogate model 은 아래와 같이 표현합니다 : 
$$
\operatorname{explanation}(x)=\arg \min _{g \in G} L\left(f, g, \pi_x\right)+\Omega(g)
$$
인스턴스 $x$ 에 대한 explanation model은 손실 $L$ (예: MSE)을 최소화하는 모델 $g$ (예: 선형 회귀 모델)로 설명이 원래 모델 $f$ (예: xgboost)의 예측치에 얼마나 가까운지를 측정하는 반면, 모델 복잡도 $\Omega(g)$ 는 낮게 유지시킵니다 (예: 더 적은 수의 피쳐 선호). $G$ 는 가능한 모든 선형 회귀 모델과 같이 family of possible explanations 입니다. 근접도 (proximity) 측정값 $\pi_x$ 는 explanation에 고려하는 인스턴스 $x$ 주변의 이웃이 얼마나 큰지를 정의합니다. 실제로 LIME은 손실 부분만 최적화합니다. 사용자는 선형 회귀 모델에서 사용할 수 있는 최대 피쳐 수를 선택하는 등 복잡도를 결정해야 합니다. 

Local surrogate models 를 학습시키는 방법 : 
- Black box 예측치에 대한 설명을 얻고자 하는 관심 인스턴스들을 선택합니다
- 데이터셋을 교란시키고 (perturb) 이 새로운 포인트에 대한 black box 예측치를 얻습니다
- 관심 인스턴스와의 근접성에 따라 새 샘플에 가중치를 부여합니다
- 데이터셋에 가중치가 부여되고 interpretable model 을 변형된 데이터로 학습합니다
- Local model을 해석해 예측치를 설명합니다

예를 들어, R 및 Python에서는 선형 회귀를 interpretable surrogate model로 선택할 수 있습니다. 미리 이 모델에 포함할 피쳐 수 $K$를 선택해야 합니다. $K$ 가 낮을수록 모델을 해석하기가 더 쉬워집니다. $K$가 높을수록 충실도 (fidelity)가 높은 모델을 만들 수 있습니다. 정확히 $K$개의 피쳐를 가진 모델을 학습시키는 방법에는 여러 가지가 있습니다. Lasso 는 정규화 (regularization) parameter $\lambda$가 높으면 피쳐가 적은 모델을 만듭니다. Lasso는 $\lambda$를 천천히 감소시킴녀서 모델을 재학습하면 피쳐가 0과 다른 가중치를 얻게 됩니다. 사전에 원하는 개수 $K$ 에 도달하면 멈춥니다. 다른 전략은 피쳐를 forward 또는 backward로 선택하는 것입니다. 즉, 전체 모델 (full model) 또는 절편만 있는 모델로 시작해 가장 큰 개선 효과를 가져오는 피쳐를 하나씩 추가/제거하여 $K$개의 피쳐가 될 때까지 테스트를 반복하는 것입니다. 

데이터의 변형(variations)/ 교란은 어떻게 할까요? 이는 텍스트, 이미지, tabular 데이터 각 유형에 따라 다릅니다. 텍스트와 이미지인 경우, 단일 단어 또는 super-pixels 를 on or off 하는 것이 해결책입니다. Tabular data의 경우, LIME은 피쳐의 평균과 표준 편차를 정규분포의 모수로 해서 해당 정규분포에서 새로운 샘플을 생성합니다. 

### 1. LIME for Tabular Data

Tabular data는 테이블 형태로 제공되는 데이터로, 각 행은 인스턴스를, 각 열은 피쳐를 나타냅니다. LIME samples는 관심 있는 인스턴스 주변이 아니라 train data의 질량 중심 (mass center)에서 가져오는데, 이는 문제가 됩니다. 하지만 일부 샘플 포인트의 예측치가 관심 데이터 포인트와 다를 확률이 높아지며, LIME이 최소한의 설명을 학습할 수 있습니다. 

샘플링과 local model 학습 작동을 시각적으로 나타내 보겠습니다 : 

> [!note]- code fold
> ```r
> ## lime-fitting 
> #### Creating dataset
> ##### Define range of set
> lower_x1 <- -2; upper_x1 <- 2
> lower_x2 <- -2; upper_x2 <- 1
> 
> ##### Size of the training set for the black box classifier
> n_training <- 20000
> ##### Size for the grid to plot the decision bundaries 
> n_grid <- 100
> ##### Number of samples for LIME explanations
> n_sample <- 500
> 
> ##### Simulate y ~ x1 + x2
> set.seed(1)
> x1 <- runif(n_training, min = lower_x1, max = upper_x1)
> x2 <- runif(n_training, min = lower_x2, max = upper_x2)
> y <- get_y(x1, x2)
> ##### Add noise
> y_noisy <- get_y(x1, x2, noise_prob = 0.01)
> lime_training_df <- data.frame(x1 = x1, x2 = x2, y = as.factor(y), y_noisy = as.factor(y_noisy))
> 
> 
> ##### For scaling later on
> x_means <- c(mean(x1), mean(x2))
> x_sd <- c(sd(x1), sd(x2))
> 
> 
> ##### Learn model
> rf <- randomForest::randomForest(y_noisy ~ x1 + x2, data = lime_training_df, ntrees = 100)
> lime_training_df$predicted <- predict(rf, newdata = lime_training_df)
> 
> #### The decision boundaries
> grid_x1 <- seq(lower_x1, upper_x1, length.out = n_grid)
> grid_x2 <- seq(lower_x2, upper_x2, length.out = n_grid)
> grid_df <- expand.grid(x1 = grid_x1, x2 = grid_x2)
> grid_df$predicted <- predict(rf, newdata = grid_df) %>% as.character() %>% as.numeric()
> 
> ##### The observation to be explained
> explain_x1 <- 1
> explain_x2 <- -0.5
> explain_y_model <- predict(rf, newdata = data.frame(x1 = explain_x1, x2 = explain_x2))
> df_explain <- data.frame(x1 = explain_x1, x2 = explain_x2, y_predicted = explain_y_model)
> 
> point_explain <- c(explain_x1, explain_x2)
> point_explain_scaled <- (point_explain - x_means) / x_sd
> 
> 
> ##### Drawing the samples for the LIME explanations
> x1_sample <- rnorm(n_sample, x_means[1], x_sd[1])
> x2_sample <- rnorm(n_sample, x_means[2], x_sd[2])
> df_sample <- data.frame(x1 = x1_sample, x2 = x2_sample)
> ##### Scale the samples
> points_sample <- apply(df_sample, 1, function(x){
>   (x - x_means) / x_sd
> }) %>% t()
> 
> ##### Add weights to the samples
> kernel_width <- sqrt(dim(df_sample)[2]) * 0.15
> distances <- get_distances(point_explain = point_explain_scaled, points_sample = points_sample)
> 
> df_sample$weights <- kernel(d = distances, kernel_width = kernel_width)
> df_sample$predicted <- predict(rf, newdata = df_sample)
> 
> #### Logistic regression model
> mod <- glm(predicted ~ x1 + x2, data = df_sample, weights = df_sample$weights, family = 'binomial')
> grid_df$explained <- predict(mod, newdata = grid_df, type = 'response')
> 
> #### Logistic decision boundary
> coefs <- coefficients(mod)
> logistic_boundary_x1 <- grid_x1
> logistic_boundary_x2 <- - (1/coefs['x2']) * (coefs['(Intercept)'] + coefs['x1'] * grid_x1)
> logistic_boundary_df <- data.frame(x1 = logistic_boundary_x1, x2 = logistic_boundary_x2)
> logistic_boundary_df <- logistic_boundary_df %>% 
>   filter(x2 <= upper_x2, x2 >= lower_x2)
> 
> ##### Create a smaller grid for visualization of local model boundaries
> x1_steps <- unique(grid_df$x1)[seq(1, n_grid, length.out = 20)]
> x2_steps <- unique(grid_df$x2)[seq(1, n_grid, length.out = 20)]
> grid_df_small <- grid_df %>% 
>   filter(x1 %in% x1_steps & x2 %in% x2_steps)
> grid_df_small$explained_class <- round(grid_df_small$explained)
> 
> colors = c('#132B43', '#56B1F7')
> 
> ##### Data with some noise
> p_data <- lime_training_df %>% 
>   ggplot() +
>   geom_point(aes(x=x1, y=x2, fill=y_noisy, color=y_noisy),
>              alpha=0.3, shape=21) +
>   scale_fill_manual(values=colors) +
>   scale_color_manual(values = colors) +
>   my_theme(legend.position = 'none')
> 
> ##### The decision boundaries of the learned black box classifier
> p_boundaries <- grid_df %>% 
>   ggplot() +
>   geom_raster(aes(x=x1, y=x2, fill=predicted), alpha = 0.3, interpolate = T) +
>   my_theme(legend.position = 'none') +
>   ggtitle('A')
> 
> #### Drawing some samples
> p_samples <- p_boundaries +
>   geom_point(data = df_sample,
>              aes(x=x1, y=x2)) +
>   scale_x_continuous(limits = c(lower_x1, upper_x1)) +
>   scale_y_continuous(limits = c(lower_x2, upper_x2))
> 
> ##### The point to be explained
> p_explain <- p_samples +
>   geom_point(data = df_explain,
>              aes(x=x1, y=x2), fill='yellow', shape=21, size=4) +
>   ggtitle('B')
> 
> p_weighted <- p_boundaries + 
>   geom_point(data = df_sample,
>              aes(x=x1, y=x2, size=weights)) +
>   scale_x_continuous(limits = c(lower_x1, upper_x1)) +
>   scale_y_continuous(limits = c(lower_x2, upper_x2)) +
>   geom_point(data = df_explain,
>              aes(x=x1, y=x2), fill='yellow', shape=21, size=4) +
>   ggtitle('C')
> 
> p_boundaries_lime <- grid_df %>% 
>   ggplot() +
>   geom_raster(aes(x=x1, y=x2, fill=predicted), alpha=0.3, interpolate=T) +
>   geom_point(data = grid_df_small %>% filter(explained_class==1),
>              aes(x=x1, y=x2, color=explained),
>              size=2, shape=3) +
>   geom_point(data = grid_df_small %>% filter(explained_class==0),
>              aes(x=x1, y=x2, color=explained),
>              size=2, shape=95) +
>   geom_point(data = df_explain,
>              aes(x=x1, y=x2),
>              fill='yellow', shape=21, size=4) +
>   geom_line(data = logistic_boundary_df,
>             aes(x=x1, y=x2), color = 'white') +
>   my_theme(legend.position = 'none') +
>   ggtitle('D')
> 
> gridExtra::grid.arrange(p_boundaries, p_explain, p_weighted, p_boundaries_lime, ncol = 2)
> ```

![[Pasted image 20240105154751.png|center]] Figure 9.5 : Tabular data에 대한 LIME 알고리즘. A) x1과 x2가 주어진 랜덤포레스트 예측치: 1(dark), 0(light). B) 관심 인스턴스 (큰점)와 정규분포에서 샘플링된 데이터 (작은 점들). C) 관심 인스턴스 근처의 포인트들에 더 높은 가중치 할당. D) 그리드의 부호는 가중치가 적용된 샘플에서 로컬로 학습된 모델의 분류를 나타냅니다. 흰색 선은 결정 경계 (P(class=1) = 0.5)를 표시

한 점 주변의 의미 있는 이웃들을 정의하는 것은 어렵습니다. LIME은 현재 exponential smoothing kernel을 사용해서 이웃을 정의했습니다. Smoothing kernel은 2개의 인스턴스를 가져와 근접도 측정값을 반환하는 함수입니다. Kernel width은 이웃의 크기를 결정합니다. 이 폭이 작다는 것은 인스턴스가 local model에 영향력을 가지려면 매우 가까워야 함을 의미하고, 폭이 크다는 것은 멀리 떨어져 있는 인스턴스도 모델에 영향을 미친다는 것입니다. LIME의 [Python implementation (파일 lime/lime_tabular.py)](https://github.com/marcotcr/lime/tree/ce2db6f20f47c3330beb107bb17fd25840ca4606)을 보면 정규화된 데이터에 지수 평활 커널을 사용하고 kernel width가 train data 칼럼 수의 제곱근의 0.75배입니다. 별 상관없는 것처럼 보이지만 그렇지 않습니다. 가장 큰 문제는 최적의 커널이나 커널 폭을 찾을 수 있는 좋은 방법이 없다는 것입니다. 그리고 0.75라는 숫자의 출처는 어디서 나온 것일까요? 특정 시나리오에서는 아래 그림처럼 커널 폭을 변경하여 설명 (explanation)을 쉽게 바꿀 수 있습니다 : 

> [!note]- code fold
> ```r
> ## lime-fail
> set.seed(42)
> df <- data.frame(x = rnorm(200, mean = 0, sd = 3))
> df$x[df$x < -5] <- -5
> df$y <- (df$x + 2)^2
> df$y[df$x > 1] <- -df$x[df$x > 1] + 10 + -0.05 * df$x[df$x > 1]^2
> # df$y <- df$y + rnorm(nrow(df), sd=0.05)
> explain.p <- data.frame(x=1.6, y=8.5)
> 
> 
> w1 <- kernel(d = get_distances(data.frame(x=explain.p$x), df), kernel_width = 0.1)
> w2 <- kernel(d = get_distances(data.frame(x=explain.p$x), df), kernel_width = 0.75)
> w3 <- kernel(d = get_distances(data.frame(x=explain.p$x), df), kernel_width = 2)
> 
> 
> lm.1 <- lm(y ~ x, data = df, weights = w1)
> lm.2 <- lm(y ~ x, data = df, weights = w2)
> lm.3 <- lm(y ~ x, data = df, weights = w3)
> 
> df.all <- rbind(df, df, df)
> df.all$lime <- c(predict(lm.1), predict(lm.2), predict(lm.3))
> df.all$width <- factor(c(rep(c(0.1, 0.75, 2), each = nrow(df))))
> 
> 
> df.all %>% 
>   ggplot(aes(x=x, y=y)) +
>   geom_line(lwd = 2.5) +
>   geom_rug(sides = 'b') +
>   geom_line(aes(x = x, y = lime, group = width, color = width, linetype = width)) +
>   geom_point(data = explain.p,
>              aes(x=x, y=y), 
>              size=12, shape='x') +
>   scale_color_viridis('Kernel width', discrete = T) +
>   scale_linetype('Kernel width') +
>   scale_y_continuous('Black Box Prediction')
> ```

![[Pasted image 20240105161651.png|center]] Figure 9.6 : 인스턴스 `x=1.6`의 예측치에 대한 explanation 입니다. 단일 피쳐에 따른 blakc box model의 예측치는 굵은 선으로 표시되고 데이터의 분포는 rug로 표시됩니다. 커널 폭이 다른 3개의 local surrogate model이 있습니다. 선형 회귀 모델은 커널의 폭에 다라 다릅니다 : `x=1.6`에 대해 피쳐가 음의 영향을 미치는지, 양의 영향을 미치는지, 아니면 아무런 영향을 미치지 않는가?

이 예시는 한 개의 피쳐만 보여줍니다. 고차원 피쳐 공간에서는 더 상황이 나쁩니다. 또한 거리 측정이 모든 피쳐들을 동일하게 취급해야 하는지도 불분명합니다. 피쳐 x1의 거리 단위는 x2의 거리 단위와 똑같이 취급받습니다. 


#### Example

구체적인 예를 들어 보겠습니다. 자전거 대여 데이터에서 예측 문제를 분류 문제로 전환하겠습니다 : 시간이 지남에 따라 자전가 대여의 인기가 높아지는 추세를 고려하여, 특정 날짜에 대여 수가 추세선보다 높을지 낮을지 알고 싶습니다. 이때 'above'는 평균 자전거 수보다 많지만 추세에 맞게 조정된 것으로 해석할 수도 있습니다. 

먼저 분류 작업에서 100개의 trees를 이용해 random forest를 학습시킵니다. 날씨와 날짜 정보를 기반으로 어떤 날에 대여 수가 추세가 없는 평균보다 많을까요?

설명 (explanations)은 2개의 피쳐로 진행했습니다. 예측 클래스가 다른 두 개의 인스턴스에 대해 학습된 sparse local linear models의 결과입니다 : 

> [!note]- code fold
> ```r
> ## lime-tabular-train-black-box
> ntree <- 100
> trend_line <- lm(cnt ~ days_since_2011, data = bike)
> bike.train.resid <- factor(resid(trend_line) > 0, levels = c(F, T), labels = c('below', 'above'))
> bike.train.x <- bike %>% select(-cnt)
> model <- caret::train(x = bike.train.x, bike.train.resid,
>                       method = 'rf', ntree = ntree, maximise = F)
> n_features_lime <- 2
> 
> ## lime-tabular-example-explain-plot-1
> instance_indices <- c(295, 8)
> set.seed(44)
> bike.train.x$temp <- round(bike.train.x$temp,2)
> pred <- Predictor$new(model = model, data = bike.train.x, class = 'above', type = 'prob')
> lim1 <- LocalModel$new(predictor = pred, x.interest = bike.train.x %>% slice(instance_indices[1]), k = n_features_lime)
> lim2 <- LocalModel$new(predictor = pred, x.interest = bike.train.x %>% slice(instance_indices[2]), k = n_features_lime)
> wlim <- c(min(c(lim1$results$effect, lim2$results$effect)), max(c(lim1$results$effect, lim2$results$effect)))
> a <- lim1$plot() +
>   scale_y_continuous(limit = wlim) +
>   geom_hline(aes(yintercept = 0)) +
>   theme(axis.title.y = element_blank(),
>         axis.ticks.y = element_blank())
> b <- lim2$plot() +
>   scale_y_continuous(limits = wlim) +
>   geom_hline(aes(yintercept = 0)) +
>   theme(axis.title.y = element_blank(),
>         axis.ticks.y = element_blank())
> grid.arrange(a,b,ncol=1)
> ```

![[Pasted image 20240105163834.png|center]] Figure 9.7 : 자전거 대여 데이터셋의 두 인스턴스에 대한 LIME explanations. 따뜻한 기온과 좋은 날씨가 예측치에 긍정적인 영향을 미칩니다. X 축은 feature effect를 나타냅니다 : 가중치와 실제 피쳐값을 곱한 값


위 그림을 보면 수치형 피쳐보다 범주형 피쳐의 해석이 더 쉽다는 것을 알 수 있습니다. 한 가지 해결책은 수치형 피쳐를 binning 하는 것입니다. 

### 2. LIME for Text

Text 데이터에 사용하는 LIME은 tabular와 다릅니다. 데이터의 변형이 다르게 생성됩니다 : 원본 텍스트에서 시작하여 무작위로 단어를 제거하여 새로운 텍스트를 만듭니다. 데이터셋은 각 단어에 대해 binary features로 표현됩니다. 해당 단어가 포함되면 1, 제거되면 0입니다.

#### Example

Youtube 댓글을 스팸/정상 분류하는 예제를 다루겠습니다.

Black box model은 document-word matrix ; dtm에 대해 학습한 deep decision tree 입니다. 각 댓글은 하나의 document (= 하나의 행)이고, 각 열은 주어진 term/word의 발생 횟수입니다. 짧은 decision trees는 이해 하기 쉽지만 이번의 tree 는 매우 deep 합니다. 또한 이 tree 대신에 순환 신경망(recurrent neural network; RNN)이나 단어 임베딩 (word embeddings: abstract vectors)에 대해 학습한 SVM을 사용할 수 도 있습니다. 이 데이터셋의 두 개의 댓글과 해당 되는 클래스 (1 for spam, 0 for normal comment)를 살펴보겠습니다 : 

> [!note]- code fold
> ```r
> ## load-text-classification-lime
> example_indices <- c(267, 173)
> texts <- ycomments$CONTENT[example_indices]
> ```

|  | CONTENT | CLASS |
| :--- | :--- | ---: |
| 267 | PSY is a good guy | 0 |
| 173 | For Christmas Song visit my channel! ;) | 1 |

다음 단계는 local model에 사용되는 데이터셋의 몇 가지 변형 (variations)를 만드는 것입니다. 예를 들어, 댓글 중 한 개의 변형을 아래와 같이 만들 수 있습니다 :  

| For | Christmas | Song | visit | my | channel! | ;) | prob | weights |
|:----|:----------|:-----|:------|:---|:---------|:---|:-----|:--------|
|   1 |         0 |    1 |     1 |  0 |        0 |  1 | 0.17 |    0.57 |
|   0 |         1 |    1 |     1 |  1 |        0 |  1 | 0.17 |    0.71 |
|   1 |         0 |    0 |     1 |  1 |        1 |  1 | 0.99 |    0.71 |
|   1 |         0 |    1 |     1 |  1 |        1 |  1 | 0.99 |    0.86 |
|   0 |         1 |    1 |     1 |  0 |        0 |  1 | 0.17 |    0.57 |  
각 열은 문장의 각 단어에 해당합니다. 각 행은 원래 문장의 변형이며, 1은 해당 단어가 이 변형에 발생했음을 의미하고, 0은 해당 단어가 제거되었음을 의미합니다. 2 번째 변형 문장을 보면 `Cristmas Song visit my ;)` 입니다. `prob` 열에는 각 변형 문장에 대한 예상 스팸 확률이 표시됩니다. `weights` 열은 변형 문장과 원본 문장의 근접도를 나타내며, 1에서 제거된 단어의 비율을 뺀 값으로 계산됩니다 (예: 7개 단어 중 1개가 제거되면 근접도는 1-1/7=0.86 입니다).

아래는 두 문장(스팸 1개, 정상 1개)과 LIME 알고리즘이 찾은 추정된 로컬 가중치입니다 : 

> [!note]- code fold
> ```r
> ## lime-text-explanations
> set.seed(42)
> ycomments.predict <- get.ycomment.classifier(ycomments)
> explanations <- data.table::rbindlist(lapply(seq_along(texts), function(i){
>   explain_text(text = texts[i], pred_fun = ycomments.predict, class = 'spam', case = i, prob = 0.5)
> }))
> explanations <- data.frame(explanations)
> explanations %>% 
>   select(case, label_prob, feature, feature_weight) %>% 
>   mutate(label_prob = round(label_prob, 7),
>          feature_weight = round(feature_weight, 6))
> ```

| case | label_prob | feature | feature_weight |
| ---: | ---: | :--- | ---: |
| 1 | 0.1701170 | PSY | 0.000000 |
| 1 | 0.1701170 | guy | 0.000000 |
| 1 | 0.1701170 | good | 0.000000 |
| 2 | 0.9939024 | channel! | 6.180747 |
| 2 | 0.9939024 | $;)$ | 0.000000 |
| 2 | 0.9939024 | visit | 0.000000 |
`'channel!'` 이라는 단어는 스팸일 확률이 높음을 나타냅니다. 스팸이 아닌 댓글의 경우 어떤 단어를 제거해도 예측 클래스는 동일하므로 0이 아닌 가중치가 추정되지 않습니다. 

### 3. LIME for Images

[LIME for Images](https://christophm.github.io/interpretable-ml-book/lime.html) link!

### 4. Advantages

- 기본 (underlying) ML 모델을 바꾸더라도 해석 가능한 동일 local mode을 사용할 수 있습니다. 
	- 설명 (explanations)을 보는 사람들이 DT를 가장 잘 이해한다고 해보겠습니다. Local surrogate model을 사용하기 때문에 실제로 DT를 사용하여 예측할 필요 없이 DT를 explanation 으로 사용할 수 있습니다. 
- Lasso 또는 short trees를 사용하는 경우, **설명이 짧고 (선택적), 대조적일 수 있습니다**. 
	- 따라서 [[CH03. Interpretability|Interpretability]] 의 human-friendly explanations 가 가능합니다. 기여도 방식으로는 적절하지 않기 때문에 엄격한 기여도를 요구하는 상황에서는 LIME을 사용하지 않습니다. 
- LIME 은 tabular, text, image에 모두 작동하는 몇 안되는 방법 중 하나입니다. 
- 충실도 (fidelity) 측정값 (해석 가능한 모델이 black box 예측치를 얼마나 잘 근사화 하는지)은 해석 가능한 모델이 관심 인스턴스 인근의 black box 예측치를 설명하는 데 얼마나 신뢰할 수 있는지를 잘 보여줍니다. 
- LIME 은 Python (`lime` 라이브러리)과 R (`lime` 및 `iml` 패키지)로 잘 구현됩니다. 
- Local surrogate model로 생성된 설명 (explanations)은 기존 모델이 학습한 것과 다른 피쳐를 사용할 수 있습니다. 물론 이러한 피쳐들은 인스턴스에서 파생되어야 합니다. 
	- 텍스트 분류기는 abstract word embeddings 를 피쳐로 사용할 수 있지만, 설명은 문장에 포함된 단어 유무를 기반으로 할 수 있습니다. 
	- 회귀 모델은 일부 피쳐들의 non-interpretable transformations에 의존하기도 하지만, 설명은 기존 피쳐들로 만들 수 있습니다. 
		- 예를 들어, 설문조사 답변의 PCA 구성요소에 대해 학습하지만, LIME은 원래 설문조사 질문에 대해 학습할 수 있습니다. 
	- 이처럼 LIME에서 해석 가능한 피쳐들을 사용하는 것은 특히 해석 불가능한 피쳐들로 모델을 학습할 때 큰 이점이 될 수 있습니다. 

### 5. Disadvantages

- 이웃 (neighborhood)의 정확한 정의는 tabular data에서 LIME을 사용할 때 매우 큰 문제입니다. 제 생각에는 이것이 LIME의 가장 큰 문제이며, LIME을 신중하게 사용하기를 권장하는 이유입니다. 각 분야 및 적용 범위마다 다양한 kernel settings을 시도하고 설명이 이해가 되는지 직접 확인해야 합니다. 
- 샘플링은 현재 LIME 구현 시 개선될 수 있습니다. 데이터 포인트는 피쳐 간 상관관계를 무시하고 Gaussian 분포에서 뽑힙니다. 이로 인해 가능성이 희박한 데이터 포인트가 생성되기도 하며, 이는 local explanation model을 학습하는데 사용됩니다. 
- 설명 모델의 복잡도를 미리 정의해야 합니다. 결국 사용자가 항상 충실도 (fidelity)와 희소성(sparsity) 사이의 적절한 지점을 정의해야 하기 때문에 불편할 수 있습니다. 
- 또 다른 큰 문제는 설명의 불안정성입니다. Alvarez-Melis, David, and Tommi S. Jaakkola (2018) 은 시뮬레이션 환경에서 매우 가까운 두 점에 대한 설명이 크게 달리지는 것을 보여주었습니다. 또한 제 경험상 샘플링 과정을 반복하다보면 설명이 달리지기도 했습니다. 불안정하다는 것은 설명을 신뢰하기 어렵다는 것을 의미합니다. 
- LIME explanations는 데이터 과학자가 bias를 숨기기 위해 조작할 수 있습니다. 이러한 조작의 가능성은 LIME에서 생성된 설명을 신뢰하기 어렵게 만듭니다.
- 결론: LIME을 구체적으로 구현한 local surrogate mode은 유망합니다. 그러나 이 방법은 아직 개발 단계에 있으며 안전히 사용하기 위해선 많은 문제를 해결해야 합니다.

---

## 09-03. Counterfactual Explanations

#### *Authors : Susanne Dandl * Christoph Molnar*

Counterfactual explanation은 인과관계를 설명하는 방식입니다 : "X가 발생하지 않았다면 Y는 발생하지 않았을 것이다". 예를 들어 "내가 이 뜨거운 커피를 한 모금도 마시지 않았다면, 혀에 화상을 입지 않았을 것이다". 사건 Y는 혀에 화상을 입었다는 것이고, 원인 X는 뜨거운 커피를 마셨다는 것입니다. 반사실적(counterfactual)으로 생각하려면 관찰된 사실과 모순되는/반대되는 가상의 (hypothetical) 현실 (예: 뜨거운 커피를 마시지 않은 세계)을 상상해야 합니다.

interpretable ML에서는 개별 사례들 (individual instances)에 대한 예측치를 설명하는 데 반사실적 설명을 사용할 수 있습니다. 'Event'는 인스턴스의 outcome이고, 'causes'는 모델에 입력되어 특정한 예측치를 '유발한' 해당 인스턴스의 특정한 피쳐값입니다. 아래 그림으로 나타나는 입력과 예측 간의 관계는 매우 간단합니다 : 피쳐값이 예측의 원인이 됩니다.

![[Pasted image 20240105212427.png|center]] Figure 9.9 : 모델을 black box로 볼 때 ML 모델의 입력과 예측 간 인과관계입니다. 입력값이 예측의 원인이 됩니다 (반드시 데이터의 실제 인과관계를 반영하는 것이 아님).

실제로는 입력과 예측 간 관계가 인과적이지 않더라도 우리는 모델의 입력이 예측의 원인이라고 간주할 수 있습니다. 

위의 간단한 그림을 보면 ML 모델의 예측치에 대한 counterfactuals 를 시뮬레이션하는 방법을 쉽게 알 수 있습니다 : 예측을 하기 전에 인스턴스의 피쳐값을 바꾸고 예측치가 어떻게 변하는지 분석하기만 하면 됩니다. 우리는 예측된 클래스가 바뀌거나 (예: 학점 신청이 승인/거부됨) 예측치가 특정 임계값에 도달하는 경우 (예: 암발생 확률이 10%에 도달함)와 같이 예측치가 관련성 있는 방식으로 변하는 상황에 관심을 갖고 있습니다. **예측에 대한 반사실적 설명은 예측을 변형시키는 피쳐의 변경 사항을 설명합니다**.

Model-agnostic 과 model-specific 한 counterfactual explanation methods가 모두 있지만, 여기서는 특정 모델의 내부 구조가 아닌 모델의 입력과 출력에 대해서만 작동하는 model-agnostic method에 중점을 두겠습니다. 이는 피쳐값의 차이에 대한 요약으로 해석을 할 수 있기 때문에 ("예측을 변경하려면 피쳐 A와 B를 변경하라") 이번 챕터에 적합합니다. 그러나 counterfactual explanation은 그 자체로 새로운 인스턴스 입니다 ("인스턴스 X에서 시작하여 A와 B를 변경하여 반사실적 인스턴스를 얻는다"). Prototypes와 달리 반사실적 방법은 train data의 실제 인스턴스일 필요는 없으며, 인스턴스가 피쳐값의 새로운 조합일 수 있습니다.

Counterfactuals를 만드는 방법에 대해 다루기 전에, counterfactuals의 몇 가지 사용 사례와 좋은 counterfactual explanation은 무엇인지 설명하겠습니다.

첫 번째 예로는, Peter가 대출을 신청했다가 ML 기반의 은행 소프트웨어에 의해 거절당했습니다. 그는 자신이 거부된 이유와 대출 가능성을 높일 수 있는 방법이 궁급합니다. "왜"라는 질문은 "예측을 거절에서 승인으로 바꿀 수 있는 최소한의 변경 사항(소득, 신용카드 수, 나이 등)은 무엇인가?" 라는 반문으로 공식화됩니다. 가능한 답변이 다음과 같다고 해보겠습니다 : Peter가 연간 소득이 10,000 달러 더 많으면 대출 승인이 날 것입니다. 또는 Peter의 신용 카드 수가 적고 5년 전에 대출을 받지 않았다면 대출을 받을 수 있습니다. (은행은 투명성(black box 모델의 내부상황)에 관심이 없기 때문에 Peter는 대출이 거절된 이유를 알 수는 없습니다.)

두 번째 예에서는 counterfactual explanations로 연속형 outcome을 예측하는 모델을 설명하고자 합니다. Anna는 자신의 아파트를 임대하고 싶지만 임대료를 얼마로 책정해야 할지 몰라 ML 모델을 학습시켜 임대료를 예측하기로 합니다. 집의 크기, 위치, 반려동물 허용 여부 등에 대한 정보들을 입력하자 모델은 900 유로를 청구할 수 있다고 알려줍니다. 그녀는 1000 유로 이상을 예상했지만, 모델을 믿고 아파트의 피쳐값을 바꿔가며 아파트의 가치를 높일 방법을 찾기로 합니다. 그녀는 아파트가 15㎡ 더 크면 1000 유로 이상에 임대를 낼 수 있다는 것을 알게 됩니다. 흥미롭지만, 아파트를 넓힐 수는 없기 때문에 실행 가능한 지식은 아닙니다. 결국 그녀는 자신이 제어할 수 있는 피쳐값 (built-in kitchen yes/no, pets allowed yes/no, type of floor 등)만 조정하여 반려동물을 허용하고 단열이 더 좋은 창문을 설치하면 1000 유로를 받을 수 있다는 것을 알게 됩니다. Anna는 직관적으로 counterfactuals를 통해 outcome을 바꾼 것입니다. 

Counterfactuals는 현재와 대조적이고 선택적이기 때문에 보통 소수의 피쳐 변경을 통해 가능하기 때문에 human-friendly explanations 입니다. 하지만 counterfactuals 는 'Rashomon effect'를 갖습니다. 라쇼몽은 한 사무라이의 살인을 두고 여러 사람이 서로 다른 이야기를 들려주는 일본 영화입니다. 각 이야기는 결과를 똑같이 말하지만 서로 반대되는/모순되는 부분이 있습니다. Counterfactuals도 마찬가지인데, 보통 여러 가지 다른 counterfactual explanations가 존재하기 때문입니다. 각각의 counterfactual은 특정한 outcome에 도달할 방법에 대해 서로 다른 '이야기(stories)'를 말합니다. 하나는 피쳐 A를 바꾸라고 하고, 다른 것은 A는 그대로 두고 B를 바꾸라고 할 수 있는데, 이는 모순입니다. 이러한 multiple truths 문제는 모든 counterfactual explanations를 보고하거나 평가하고 최선의 설명을 선택하기 위한 기준을 마련함으로써 해결합니다.

기준 (criteria)에 대해 말하자면, 좋은 counterfactual explanation은 어떻게 정의할까요? 먼저, counterfactual explanation의 사용자는 인스턴스 (= the alternative reality)의 예측치에 관련된 변화를 정의합니다. 첫 번째 요건은 **counterfactual instance가 사전 정의된 예측을 최대한 가깝게 생성해야 한다는 것입니다**. 사전 정의된 (predefined) 예측으로 counterfactual를 선정하는 것이 항상 가능한 것은 아닙니다. 예를 들어, 드문 클래스와 빈번한 클래스가 있는 분류 문제에서 모델은 항상 인스턴스를 빈번한 클래스로 분류하기도 합니다. 예측 레이블이 빈번한 클래스에서 드문 클래스로 바뀌도록 피쳐값을 변경하는 것은 불가능할 수 있습니다. 따라서 counterfactual에 대한 예측이 predefined outcome과 정확히 일치해야 한다는 요건을 완화하고자 합니다. 분류 문제에서 드문 클래스의 예측 확률이 현재의 2%에서 10%로 증가하는 counterfactual을 찾을 수 있습니다. 그렇다면 문제는 예측 확률을 2%에서 10%로 변경하기 위해 피쳐의 최소한의 변경 사항은 무엇일지 확인해야 합니다.

또 다른 기준 (criterion)은 피쳐값과 관련해 **counterfactual가 인스턴스와 가능한 유사해야 한다는 것입니다**. 예를 들어 불연속형 피쳐값과 연속형 피쳐값이 모두 있는 경우 Manhattan distance 또는 Gower distance로 두 인스턴스 간 거리를 측정해야 할 수 있습니다. Counterfactual은 원본 인스턴스에 가까워야 할 뿐 아니라 가능한 적은 수의 피쳐만을 변경해야 합니다. 이 metric에서 counterfactual explanation이 얼마나 좋은지 측정하려면 단순히 변경된 피쳐의 수를 세거나 $L_0$-norm을 측정하면 됩니다. 

셋째, 의사 결정의 주체가 다양한 outcome을 도출할 수 있는 여러 가지 방법들을 접할 수 있도록 **여러 가지 다양한 counterfactual explanations를 생성하는 것이** 좋을 때가 많습니다. 예를 들어, 대출의 예시를 이어가자면 한가지 반사실적 설명은 대출을 받기 위해 소득을 두 배로 늘리는 것만 제안하고, 다른 반사실적 설명은 대출을 받기 위해 가까운 도시로 이사하여 소득을 조금만 늘리는 것을 제안합니다. 어떤 사람에게는 전자의 경우가 가능하지만, 다른 사람에게는 후자가 실행 가능할 것입니다. 따라서 이러한 다양성은 의사 결정 주체에게 원하는 결과를 얻을 수 있는 다양한 방법들을 제공하고, 그 외에도 '다양한' 개인이 자신에게 편리한 피쳐를 변경할 수 있게 해줍니다.

마지막 요건은 **counterfactual instance가 현실적이고 가능성이 있는 피쳐값을 가져야 합니다**. 임대료 예시에서 아파트 크기가 음수이거나 방의 수가 200으로 설정된 counterfactual explanation을 만드는 것은 말이 되지 않습니다. 방이 10개이고, 면적이 20㎡인 아파트와 같이 데이터의 joint distribution에 따라 counterfactual 값이 나올 가능성이 높으면 더욱 좋습니다.

### 1. Generating Counterfactual Explanations

Counterfactual explanations를 만드는 간단한 방법은 시행착오 (trail and error)를 통해 찾는 것입니다. 이 방식은 관심 인스턴스의 피쳐값을 임의로/무작위로 바꾸고 원하는 결과 (desired output)가 예측되면 중단하는 방식입니다. 
- Anna가 임대료를 더 많이 받을 수 있는 아파트를 찾으려고 했던 예와 같습니다. 
하지만 시행착오보다 더 나은 방식이 있습니다. 먼저 위에서 언급한 기준 4가지에 따라 손실 함수를 정의합니다. 이 손실 함수는 관심 인스턴스들, 반대 사실과 원하는 (반대 사실의) 결과 (outcome)를 입력으로 사용합니다. 그런 다음 최적화 알고리즘을 통해 이 손실을 최소화하는 counterfactual explanation을 찾습니다. 많은 방법이 이러한 방식으로 진행되지만 손실함수와 최적화 방법의 정의가 다릅니다. 

여기서는 그 중 두 가지, 1) counterfactual explanation을 해석 방법으로 도입한 Wacher et al. (2017)의 방법과 2) 위에서 언급한 4가지 기준을 모두 고려한 Dandl et al (2020)의 방법에 초점을 맞춥니다. 

#### Method by Watcher et al.

Watcher et al. 은 아래의 손실 함수를 최소화할 것을 제안합니다 : 
$$
L(x, x^\prime, y^\prime, \lambda) = \lambda\cdot (\hat{f}(x^\prime) - y^\prime)^2 + d(x, x^\prime)
$$
우변의 첫 번째 항은 (사용자가 미리 정의해야 하는) counterfactual $x^\prime$ 에 대한 모델 예측치 $\hat{f}(x^\prime)$과 원하는 결과 (desired outcome) $y^\prime$ 사이의 quadratuc distance 입니다. 두 번째 항은 설명할 인스턴스 $x$ 와 counterfactual $x^\prime$ 사이의 거리 $d$ 입니다. 이 손실은 counterfactual의 예측 결과가 사전에 정의된 결과와 얼마나 멀리 떨어져 있는지, 그리고 counterfactual이 관심 있는 인스턴스와 얼마나 멀리 떨어져 있는지를 측정합니다. 거리 함수 $d$ 는 각 피쳐의 inverse median absolute deviation; MAD를 가중치로 부여한 Mahattan distance 로 정의됩니다 : 
$$
d(x, x^\prime) = \sum_{j=1}^p\frac{|x_j-x^\prime_j|}{MAD_j}
$$
총 거리는 모든 p개의 피쳐별 거리, 즉 인스턴스 $x$ 와 counterfactual $x^\prime$ 사이의 피쳐값의 차이의 절대값 합입니다. 피쳐별 거리 (feature-wise distances)는 아래와 같이 정의된 피쳐 $j$ 의 median absolute deviation 의 역수로 스케일링 됩니다 : 
$$
M A D_j=\operatorname{median}_{i \in\{1, \ldots, n\}}\left(\left|x_{i, j}-\operatorname{median}_{l \in\{1, \ldots, n\}}\left(x_{l, j}\right)\right|\right)
$$
MAD는 피쳐의 분산과 동일하지만, 분산처럼 평균을 중심으로 제곱 거리의 합을 구하는 대신 중앙값을 중심으로 삼고 절대 거리의 합을 구합니다. 이 거리 함수는 Euclidean distance 에 비해 이상값에 대해 더 robust 하다는 장점이 있습니다. 
- 아파트의 크기를 평방미터로 측정하는 평방피트로 측정하든 상관 없이 모든 피쳐를 동일한 스케일로 가져오려면 MAD를 사용한 스케일링이 필요합니다. 

Parameter $\lambda$는 예측치의 거리 (첫 번째 항)와 피쳐 값의 거리 (두 번째 항)의 균형을 맞춥니다. 주어진 $\lambda$ 에 대해 손실이 해가 구해지고 counterfactual $x^\prime$ 을 반환합니다. $\lambda$ 가 높을수록 desired outcome $y^\prime$ 에 가까운 예측을 하는 counterfactual을 선호하게 되고, $\lambda$ 가 낮을수록 피쳐값에서 $x$ 와 유사한 counterfactual $x^\prime$ 을 선호합니다. $\lambda$ 가 매우 크면 $x$ 에서 얼마나 멀리 떨어져 있는지 관계없이 $y^\prime$ 에 가장 가까운 예측을 내는 인스턴스가 선택됩니다. 궁극적으로 사용자는 counterfactual 에 대한 예측이 desired outcome과 일치해야 한다는 요건과 counterfactual이 $x$ 와 유사해야 한다는 요건의 균형을 맞추는 방법을 결정해야 합니다. 이 method의 저자 Watcher는 $\lambda$ 값을 선택하는 대신, counterfactual instance의 예측이 $y^\prime$ 에서 얼마나 멀리 떨어져 있는지에 대한 허용 오차 (tolerance) $\epsilon$ 을 선택할 것을 제안합니다. 이 제약 조건은 아래와 같이 쓸 수 있습니다 : 
$$
|\hat{f}(x^\prime)-y^\prime|\le\epsilon
$$
이제 이 손실함수를 최소화하기 위해 Nelder-Mead 같은 적절한 최적화 알고리즘을 사용할 수 있습니다. ML 모델이 gradients를 이용할 수 있는 경우, ADAM과 같은 gradient-based methods를 사용할 수 있습니다. 설명할 인스턴스 $x$, desired outcome $y^\prime$, 허용 오차 parameter $\epsilon$ 를 미리 설정해야 합니다. 충분히 가까운 해를 찾을 때까지 ( = within the tolerance parameter) $\lambda$를 증가시키면서 $x^\prime$ 및 (locally) 최적의 counterfactual $x^\prime$ 에 대해 손실함수를 최소화해야 합니다 : 
$$
\operatorname{\arg}\min_{x^\prime} \max_{\lambda} L(x, x^\prime, y^\prime, \lambda)
$$
Counterfactuals를 만드는 방법 : 
1. 설명할 인스턴스 $x$, desired outcome $y^\prime$, 허용 오차 $\epsilon$ , $\lambda$의 작은 초기값을 설정합니다. 
2. Initial counterfactual로 임의의/무작위 인스턴스를 샘플링합니다. 
3. 처음에 뽑힌 counterfactual을 시작점으로 삼아 손실을 최적화합니다. 
4. $|\hat{f}(x^\prime)-y^\prime|\le\epsilon$ 이 만족되는 동안 : 
	- $\lambda$ 를 증가시킵니다.
	- 현재의 counterfactual을 시작점으로 다시 설정하여 손실을 최적화합니다. 
	- 손실을 최소화하는 counterfactual을 반환합니다.
5. 2~4단계를 반복하여 counterfactuals 목록과 손실을 최소화하는 것들을 반환합니다.

이 방법에는 몇 가지 단점이 있습니다. **첫 번째와 두 번째 기준만 고려하고** 세 번째, 네 번째 두 기준(적은 수의 피쳐 변경, 가능성 있는 피쳐값만으로 counterfactual 생성)은 고려하지 않습니다. $d$ 는 10개의 피쳐를 1씩 증가시키면 하나의 피쳐를 10씩 증가시키는 것과 동일한 거리를 $x$ 에 제공하므로 sparse 해를 선호하지 않습니다. 비현실적인 피쳐 조합은 이 방법에서 불이익을 받지 않습니다. 

이 방법은 다양한 수준의 범주형 피쳐를 잘 처리하지 못합니다. Watcher는 범주형 피쳐의 각 피쳐값 조합에 대해 이 method를 개별적으로 실행할 것을 제안했지만, 이렇게 하면 수준이 많은 범주형 변수가 여러 개 있는 경우 곤란해집니다. 

#### Method by Dandl et al.

Dandl et al. 은 4-objective loss 를 동시에 최소화할 것을 제안합니다 : 
$$
L\left(x, x^{\prime}, y^{\prime}, X^{o b s}\right)=\left(o_1\left(\hat{f}\left(x^{\prime}\right), y^{\prime}\right), o_2\left(x, x^{\prime}\right), o_3\left(x, x^{\prime}\right), o_4\left(x^{\prime}, X^{o b s}\right)\right)
$$

네 가지 목적 함수 $o_1$ ~ $o_4$ 는 앞서 언급한 4가지 요건(predefined outcome을 가능한 정확히 예측, 원래 인스턴스와 유사해야함, 다양한 counterfactual, 현실 가능성) 중 각각 하나에 해당합니다. 첫 번째 $o_1$ 은 우리가 원하는 예측값 (desired prediction) $y^\prime$ 에 최대한 근접해야 함을 반영합니다. 따라서 우리는 $\hat{f}(x^\prime)$ 과 $y^\prime$ 사이의 거리를 최소화하고자 하며, 여기서는 Manhattan metric 인 $L_1$-norm으로 계산합니다 :
$$
o_1\left(\hat{f}\left(x^{\prime}\right), y^{\prime}\right)= \begin{cases}0 & \text { if } \hat{f}\left(x^{\prime}\right) \in y^{\prime} \\ \inf _{y^{\prime} \in y^{\prime}}\left|\hat{f}\left(x^{\prime}\right)-y^{\prime}\right| & \text { else }\end{cases}
$$
두 번째 목적함수 $o_2$ 는 counterfactual 가 인스턴스 $x$ 와 가능한 한 유사해야 함을 반영합니다. 이는 $x^\prime$ 와 $x$ 사이의 거리를 Gower distance 로 정량화 합니다 : 
$$
o_2\left(x, x^{\prime}\right)=\frac{1}{p} \sum_{j=1}^p \delta_G\left(x_j, x_j^{\prime}\right)
$$
여기서 $p$는 피쳐 개수입니다. 그리고 $\delta_G$ 는 $x_j$ 의 변수 유형에 의존합니다 : 
$$
\delta_G\left(x_j, x_j^{\prime}\right)= \begin{cases}\frac{1}{\widehat{R}_j}\left|x_j-x_j^{\prime}\right| & \text { if } x_j \text { numerical } \\ \mathbb{I}_{x_j \neq x_j^{\prime}} & \text { if } x_j \text { categorical }\end{cases}
$$
숫자형 $j$ 의 거리를 관측된 값의 범위인 $\widehat{R}_j$로 나누면 모든 피쳐에 대해 $\delta_G$의 0~1의 값으로 스케일이 조정됩니다. 

Gower distance는 수치형 피쳐와 범주형 피쳐 모두를 처리할 수 있지만, 얼마나 많은 피쳐들이 변경되었는지는 계산하지 않습니다. 따라서 $L_0$-norm을 사용하여 세 번째 목표 함수 $o_3$ 에서 피쳐의 수를 계산합니다 : 
$$
o_3\left(x, x^{\prime}\right)=\left\|x-x^{\prime}\right\|_0=\sum_{j=1}^p \mathbb{I}_{x_j^{\prime} \neq x_j} .
$$
이렇게 $o_3$를 최소화함으로써 적은 수의 피쳐 변경을 가능하게 합니다.

네 번째 목적 함수 $o_4$ 는 counterfactuals에 현실 가능성이 있는 피쳐값/조합이 있어야 함을 반영합니다. Train data을 통해 데이터 포인트가 얼마나 '그럴듯 한지', 현실 가능성이 있는지를 추론할 수 있습니다. 이 데이터셋을 $X^{obs}$라고 표기합니다. 가능성 (likelihood)에 대한 근사값으로서 $o_4$ 는 $x^\prime$ 과 가장 가까운 데이터 포인트 $x^{[1]}\in X^{obs}$ 사이의 Gower distance를 측정합니다 : 
$$
o_4\left(x^{\prime}, \mathbf{X}^{o b s}\right)=\frac{1}{p} \sum_{j=1}^p \delta_G\left(x_j^{\prime}, x_j^{[1]}\right)
$$
Wachter 의 method와 비교했을 때, $L(x,x^\prime, y^\prime, X^{obs})$ 는 $\lambda$ 와 같은 balancing/weighting term이 없습니다. 우리는 $o_1, o_2, o_3, o_4$ 라는 목적함수 모두를 동시에 최적화 하고 싶습니다. 

어떻게 할 수 있을까요? **Nondominated Sorting Genetic Algorithm** 또는 줄여서 **NSGA-II**를 사용합니다. NSGA-II는 자연에서 아이디어를 얻은 알고리즘으로 Darwin의 '적자생존'의 법칙을 적용합니다. Counterfactual의 적합도 (fitness)를 objective values $(o_1,o_2,o_3,o_4)$ 의 벡터로 나타냅니다. Counterfactual에 대한 목적 함수들의 값이 낮을수록 적합한 것입니다. 

알고리즘은 정지 기준 (stopping criterion, 예: 최대 반복 횟수)에 도달할 때까지 반복되는 4개의 step으로 구성됩니다. 아래 그림은 하나의 세대 (generation)의 4 단계를 시각화한 것입니다 : 

![[Pasted image 20240106150247.png|center]] Figure 9.10 : Visualization of one generation of the NSGA-II algorithm.

첫 번째 세대 (generation)에서는, 설명할 인스턴스 X와 비교해서 일부 피쳐를 임의로/무작위로 바꾸어 counterfactual candidates 그룹을 초기화합니다. 위 그림의 신용 예시를 보자면, 하나의 counterfactual은 소득을 30,000 유로 늘리고 다른 counterfactual은 최근 5년간 연체가 없고 연령을 10세 낮추는 것을 제안합니다. 다른 모든 피쳐값은 X와 동일합니다. 그런 다음 앞선 4가지 목적 함수를 사용해 각 후보 counterfactuals 를 평가합니다. 그중에서 더 적합할 가능성이 높은 후보를 무작위로 선택합니다. 이 후보들을 쌍으로 재조합하여 수치형 피쳐값의 평균을 내거나 범주형 피쳐를 교차시켜 유사한 자식들 (children)을 생성합니다. 또한 전체 피쳐 공간을 탐색하기 위해 자식들의 피쳐값을 약간 변형합니다. 

부모와 자식 관계에 있는 2개의 결과 그룹에서 2가지 정렬 알고리즘을 사용하 가장 좋은 절반만 선택합니다. Nondominated sorting algorithm은 목적 함수 값에 따라 후보들을 정렬합니다. 후보들이 똑같이 좋은 경우, crowding distance sorting algorithm을 이용해 각 후보의 다양성 (diversity)에 따라 정렬합니다. 

두 가지 정렬 알고리즘을 통해 순위가 주어지면 가장 유망하거나  가장 다양한 후보들 중 절반을 선택합니다. 이러한 세트를 다음 세대 (generation)에도 사용하여 선택(selection), 재조합(recombination), 변형(mutation) 과정을 반복합니다. 이를 계속 반복하면 목적함수 값이 낮은 다양하고 유망한 후보 집합들이 만들어집니다. 이 집합에서 가장 만족스러운 후보를 선택하거나, 어떤 피쳐가 얼마나 자주 변경되었는지 강조 표기하여 모든 counterfactuals에 대한 요약치를 만들 수 있습니다.

### 2. Example

Dandl et al. (2020)의 신용 데이터셋을 예시로 들어보겠습니다. 독일의 Credit Risk dataset인데 이는 ML challenges platform 인 [kaggle.com](https://www.kaggle.com/datasets/uciml/german-credit)에서 찾을 수 있습니다.

저자들은 SVM with rbf kernel을 학습시켜 고객의 신용 위험 (credit risk)이 양호할 확률을 예측했습니다. 해당 데이터셋에는 522개의 complete obs와 신용과 고객 정보를 포함한 9개의 피쳐가 있습니다.

목표는 아래와 같은 고객에 대한 counterfactual explanations를 구하는 것입니다:

| age | sex | job       | housing | savings | amount | duration | purpose |     |
| --- | --- | --------- | ------- | ------- | ------ | -------- | ------- | --- |
| 58  | f   | unskilled | free    | little  | 6143   | 48       | car     |     |
|     |     |           |         |         |        |          |         |     |
SVM 모델은 이 여성이 24.2%의 확률로 양호하다고 예측합니다. Counterfactuals에서 50% 보다 큰 예측 활률을 얻으려면 피쳐가 어떻게 바뀌어야 하는지 알고 싶습니다. 

아래는 10개의 가장 좋은 counterfactuals 입니다 : 

| age | sex | job     | amount | duration | $o_2$ | $o_3$ | $o_4$ | $\hat{f}(x')$ |
| --- | --- | ------- | ------ | -------- | ----- | ----- | ----- | ------------- |
|     |     | skilled |        | -20      | 0.108 | 2     | 0.036 | 0.501         |
|     |     | skilled |        | -24      | 0.114 | 2     | 0.029 | 0.525         |
|     |     | skilled |        | -22      | 0.111 | 2     | 0.033 | 0.513         |
| -6  |     | skilled |        | -24      | 0.126 | 3     | 0.018 | 0.505         |
| -3  |     | skilled |        | -24      | 0.120 | 3     | 0.024 | 0.515         |
| -1  |     | skilled |        | -24      | 0.116 | 3     | 0.027 | 0.522         |
| -3  | m   |         |        | -24      | 0.195 | 3     | 0.012 | 0.501         |
| -6  | m   |         |        | -25      | 0.202 | 3     | 0.011 | 0.501         |
| -30 | m   | skilled |        | -24      | 0.285 | 4     | 0.005 | 0.590         |
| -4  | m   |         | -1254  | -24      | 0.204 | 4     | 0.002 | 0.506         |

처음 다섯 개의 열에는 제안된 피쳐 변경 사항들이 있고 (변경된 피쳐만 표시), 다음 세 열에는 objectives의 값 ($o_1$ 는 모든 케이스에서 0임)이 표시되며, 마지막 열에는 예측 확률이 표시됩니다. 

이 10개의 counterfactuals는 예측 확률이 50% 이상이며 서로를 지배하지 않습니다 (do not dominate each other). 여기서 서로를 지배하지 않는다는 것의 의미는 하나의 counterfactual이 다른 counterfactuals에 대해 모든 목표함수에서 우월하지 않음을 의미합니다. 각 counterfactual은 최소한 하나 이상의 목표함수에서 다른 counterfactual 보다 우수하거나, 모든 목표함수에서 최소한 동등한 성능을 보입니다. 이러한 대안들을 일련의 trade-off solutions로 간주할 수 있습니다. 

모든 counterfactuals는 48개월에서 최소 23개월로 기간을 단축할 것을 제안하고, 일부는 여성이 unskilled가 아닌 skilled가 되어야 한다고 제안합니다. 일부 counterfactuals는 성별을 남성으로 바꾸자고 제안하기도 하는데, 이는 모델의 성별 편향성을 보입니다. 이러한 변경은 연령의 감소 또한 동반합니다. 한편 일부 counterfactuals는 4가지 피쳐의 변경을 제안하지만, 그럼에도 이러한 counterfactuals가 train data에 가장 근접한 것들입니다. 

### 3. Advantages

- **Counterfactual explanations는 정말 명확합니다** : 
	- Counterfactual에 따라서 피쳐가 변경되면 예측은 predefined prediction으로 바뀝니다. 여기에는 추가적인 가정이 개입되지 않습니다. 
	- 또한 해석을 위해 local model을 어디까지 추정할 수 있는지 불분명한 LIME과 같은 방법만큼 위험하지 않습니다.
- Counterfactual로 인해 생긴 새로운 인스턴스를 보고하거나, 관심 인스턴스와 counterfactual 인스턴스 간 변경된 피쳐를 강조해서 표시할 수 있습니다.
- **Counterfactual method는 데이터나 모델에 액세스할 필요가 없습니다** : 
	- 모델의 예측 함수에 대한 액세스만 필요합니다. 이 방법은 제3자의 감사를 받거나 모델이나 데이터를 공개하지 않고 사용자에게 설명을 제공해야 할 때 유용합니다. 회사는 영업 비밀 또는 데이터 보호 등의 이유로 모델과 데이터를 보호하는 데 관심이 있습니다. Counterfactual explanations는 모델 예측을 설명하는 것과 모델 소유자의 이익을 보호하는 것 사이에 균형을 제공합니다. 
- 이 방법은 **ML을 사용하지 않는 시스템에서도 작동합니다** : 
	- 입력을 받고 출력을 반환하는 모든 시스템에 대해 counterfactuals를 만들 수 있습니다. 
- **Counterfactual explanation method는 구현이 쉽습니다**. 기본적으로 표준 최적화 라이브러리로 최적화할 수 있는 손실 함수(1개 이상의 목적함수가 있는..)이기 때문입니다. 피쳐값을 의미 있는 범위 (예: 양수인 아파트 크기)로 제한하는 몇 가지 추가 세부 사항을 고려해야 합니다.

### 4. Disadvantages

- **각 인스턴스에 대해 일반적으로 여러 가지 counterfactual explanations (Rashomon effect)를 얻게 됩니다**.
	- 대부분의 사람들은 현실 세계의 복잡성보다 간단한 설명을 선호하기 때문에 이는 불편할 수 있습니다. 또한 현실적인 문제이기도 합니다. 한 인스턴스에 대해 23개의 counterfactual explantions가 생성되었다고 하면 무엇을 보고해야할까요? 모두 상대적으로 '좋은' 설명이지만 매우 다르다면 어떻게 해야 할까요? 이러한 질문은 각 프로젝트마다 새롭게 답해야 합니다. Counterfactual explanations가 여러 개 있으면 사람이 자신의 이전 지식과 일치하는 설명을 선택할 수 있기 때문에 유리할 수도 있습니다.

### 5. Software and Alternatives

Dandl et al.의 multi-objective counterfactual explanation method는 [Github repository](https://github.com/dandls/moc/tree/master/counterfactuals)에 구현되어 있습니다.

Python에서 [`Alibi`](https://github.com/SeldonIO/alibi) 저자들은 [simple counterfactual method](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html) 뿐만 아니라 class prototypes를 사용하여 알고리즘 출력의 해석 가능성과 수렴성을 개선하는 확장 방법 ([extended method](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html))을 구현했습니다.

Karimi et al. (2020)도 [Github repository](https://github.com/amirhk/mace)에 알고리즘 MACE의 Python 구현을 제공했습니다. 이들은 적절한 counterfactuals에 필요한 기준 (criteria)을 논리적 공식으로 변환하고 satisfiability solvers를 사용하여 이를 충족하는 counterfactuals를 찾았습니다.

Mothilal et al. (2020)은 결정점 (determinantal point) 프로세스를 기반으로 다양한 counterfactual explanations 집합을 생성하기 위해 [DiCE (Diverse Counterfactual Explanation)](https://github.com/interpretml/DiCE)를 개발했습니다. DiCE는 model-agnostic과 gradient-based method 모두 구현됩니다.

Laugel el al. (2017)의 Growing Spheres 알고리즘은 논문에서 counterfactual 이라는 단어를 사용하지는 않았지만 방법이 매우 유사합니다. 이들은 피쳐값의 변화가 가능한 한 적은 counterfactual을 선호하는 손실함수를 정의합니다. 이들은 함수를 직접 최적화하는 대신, 먼저 관심 포인트 주위에 구 (sphere)를 그리고 해당 sphere 내의 포인트들을 샘플링한 다음 샘플링된 것들 중 하나가 원하는 예측을 산출하는지 확인하는 방법을 제안합니다. 그런 다음 (sparse) counterfactual이 발견될 때까지 sphere를 축소하거나 확장합니다.

- 대안 : Ribero et al. (2018)의 Anchors 는 counterfactuals과 반대되는 개념으로 [[CH09. Local Model-Agnostic Methods#09-04. Scoped Rules (Anchors)|Scoped Rules (Anchors)]] 섹션에서 다룰 것입니다.

---

## 09-04. Scoped Rules (Anchors)

Anchors method는 예측치를 '고정 (anchor)'하는 결정 규칙 (decision rule; DR)을 찾아 black box classification model의 개별 예측치들을 설명합니다. 규칙은 다른 피쳐값의 변경이 예측치에 영향을 미치지 않는 경우 예측을 고정(anchor)합니다. Anchors는 강화 학습 기법과 graph search 알고리즘을 함께 사용하여 모델 호출 횟수와 필요한 런타임을 최소한으로 줄이면서도 local optima에서 복구(recover)할 수 있습니다. 이 알고리즘을 제안한 Ribeiro, Singh, and Guestrin은 LIME 알고리즘을 제안한 연구자들입니다.

Anchors 접근법은 black box ML model의 예측치에 대한 *국소적 (local)* explanations을 만들기 위해 **perturbation-based** 전략을 사용합니다. 그러나 LIME에서 쓰는 surrogate model 대신에 **anchors**라고 하는 이해가 쉬운 IF-THEN 규칙들로 표현되는 것을 사용합니다. 이 규칙들은 범위가 정해져 있기 때문에 (since they are scoped), 재사용이 가능합니다 : Anchors에는 적용 범위 (coverage)라는 개념이 포함되어 있어서 보이지 않는 다른 인스턴스들에 정확히 어떤 규칙이 적용되는지 기술합니다. Anchors를 찾는 작업에는 강화 학습 분야에서 유래한 exploration or multi-armed bandit problem이 포함됩니다. 이를 위해 설명할 모든 인스턴스에 대해 이웃 (neighbors) 또는 교란 (perturbations)을 만들고 평가합니다. 이렇게 하면 black box의 구조와 내부 parameters를 무시할 수 있습니다. 따라서 이 알고리즘은 model-agnostic method입니다.

논문 저자들은 두 개의 알고리즘을 비교하고, 두 알고리즘이 인스턴스의 이웃을 고려해 결과를 도출하는 방식이 얼마나 다른지 시각화하여 보여줍니다. 아래 그림은 2개의 예시 인스턴스를 사용해 복잡한 binary classifier (- or +를 예측)를 로컬에서 설명하는 LIME과 Anchors의 모습을 묘사합니다. LIME은 교란 공간 (perturbation space) $D$ 가 주어진 모델을 가장 잘 근사하는 선형 결정 경계만을 학습하기 때문에 LIME 의 결과가 얼마나 faithful한지를 나타내지 않습니다. 동일한 교란 공간 $D$가 주어졌을 때, anchors 접근법은 모델 행동에 적합한 설명을 구성하고 그 경계를 명확히 표현합니다. 따라서 설계적으로 앵커는 faithful 하고, 어떤 인스턴스들에 유효한지를 정확히 표시합니다. 앵커는 이처럼 직관적이고 이해하기 쉽다는 특성을 가집니다. 

![[Pasted image 20240106163009.png|center]] Figure 9.11 : LIME vs. Anchors - A Toy Visualization. Figure from Ribeiro, Singh, and Guestin (2018).

앞서 언급했듯이 알고리즘의 결과와 설명은 앵커(anchors)라고 부르는 규칙(rules)의 형태로 제공됩니다. 아래의 예는 이러한 앵커를 설명합니다. 예를 들어, 타이타닉 참사 당시 승객의 생존 여부를 예측하는 binary black box model이 있다고 가정해 보겠습니다. 이제 이 모델이 특정한 개인에 대해 생존을 예측하는 이유를 알고 싶다고 하겠습니다. 앵커 알고리즘은 아래와 같은 설명을 제공합니다 : 

| Feature         | Value         |
| --------------- |---------------|
| Age             | 20            |
| Sex             | female        |
| Class           | first         |
| Ticket price    | 300\$         |
| More attributes | ...           |
| Survived        | true          |
이것에 해당되는 anchors explanation 은 : 
- IF `Sex = female` AND `Class = first` THEN PREDICT `survived = true` WITH PRECISION 97% AND COVERAGE 15%.

이 예시는 앵커가 모델의 예측 및 추론에 대한 인사이트를 제공하는 방식을 보여줍니다. 그 해석은 모델에서 어떤 속성을 고려했는지를 보여 주는데, 위 경우에는 여성과 first class 입니다. 정확성 (correctness)을 가장 중요시 하는 사람은 이 규칙을 사용해서 모델의 동작을 검증할 수 있습니다. 앵커는 이 규칙이 perturbation space의 인스턴스 중 15%에 적용된다 (coverage)고 추가적으로 알려줍니다. 위 경우의 설명의 정확도 (accuracy, precision)는 97%이며, 이는 표시된 조건문이 거의 전적으로 예측 결과에 대한 원인임을 의미합니다. 

Anchor $A$ 는 아래와 같이 정의됩니다 : 
$$
\mathbb{E}_{\mathcal{D}_x(z \mid A)}\left[1_{\hat{f}(x)=\hat{f}(z)}\right] \geq \tau, A(x)=1
$$
- $x$ 는 tabular data에서 하나의 행과 같은 설명할 인스턴스
- $A$ 는 조건들의 집합 : 즉, $A$에 의해 정의된 모든 피쳐 조건들이 $x$ 의 피쳐값에 해당될 때 $A(x)=1$ 이 됩니다.
- $f$ 는 설명하고자 하는 분류 모델 (예: 인공신경망)을 나타냅니다. 이것을 질의하여 설명할 인스턴스 $x$ 와 그것의 perturbations 에 대한 label 을 예측합니다.
- $D_x(\cdot | A)$ 는 $A$ 에 일치하는 인스턴스 $x$ 의 이웃 (neighbors)의 분포를 나타냅니다.
- $0\le\tau\le 1$ 은 정확도/정밀도 (precision)의 임계값을 지정합니다. 최소 $\tau$ 정도의 local fidelity를 달성하는 규칙들만 유효한 결과로 간주됩니다.

이러한 설명은 어려우니까 말로 쉽게 표현해보겠습니다 : 

> 설명할 인스턴스 $x$ 가 주어졌을 때, $x$ 에 적용되는 규칙 또는 앵커들의 집합 $A$를 찾고, 동일한 $A$ 가 적용되는 $x$ 의 이웃들 중에 최소 $\tau$ 정도의 비율에 대해 $x$ 와 동일한 클래스를 예측해야 합니다. 규칙의 정확도 (precision)는 제공된 ML 모델 (지시 함수 $1_{\hat{f}(x)=\hat{f}(z)}$로 표시됨)을 사용하여 neighbors or perturbations ($D_x(z|A)$에 따른) 를 평가한 결과입니다. 


### 1. Finding Anchors

앵커의 수학적 설명은 명확하고 간단해 보이지만, 특정한 규칙들을 만드는 것은 실현 불가능합니다. 이를 위해서는 모든 $z\in D_x(\cdot|A)$ 에 대해 $1_{\hat{f}(x)=\hat{f}(z)}$ 를 평가해야 하는데, 이는 연속적이거나 큰 input spaces에서는 불가능합니다. 따라서 저자는 확률적 정의를 위해 parameter $0\le\delta\le 1$ 를 도입할 것을 제안합니다. 이렇게 하면 precision에 대한 통계적 신뢰도가 확보될 때까지 샘플을 추출할 수 있습니다. 확률적 정의는 아래와 같습니다 : 
$$
P(\operatorname{prec}(A) \geq \tau) \geq 1-\delta \quad \text { with } \quad \operatorname{prec}(A)=\mathbb{E}_{\mathcal{D}_x(z \mid A)}\left[1_{\hat{f}(x)=\hat{f}(z)}\right]
$$
앞의 두 가지 정의는 coverage의 개념으로 결합 및 확장됩니다. 그 이유는 모델 입력 공간의 가급적 많은 부분에 적용되는 규칙을 찾는 데 있습니다. Coverage는 공식적으로 앵커가 neighbors, 즉 perturbation space 에 적용될 확률로 정의됩니다 : 
$$
\operatorname{cov}(A)=\mathbb{E}_{\mathcal{D}_{(z)}}[A(z)]
$$
이를 포함하면 coverage의 최대화를 고려해 앵커를 정의할 수 있습니다 : 
$$
\max _{A \text { s.t. } P(\operatorname{prec}(A) \geq \tau) \geq 1-\delta} \operatorname{cov}(A)
$$
따라서 이 절차는 모든 적합한 규칙들 (확률적 정의에서 주어진 precision threshold를 만족하는 모든 규칙들) 중에서 coverage가 가장 높은 규칙을 찾습니다. 이러한 규칙들은 모델의 많은 부분을 설명하므로 중요합니다. 조건이 많은 규칙은 조건이 적은 규칙보다 정확도가 높은 경향이 있습니다. 특히 $x$ 의 모든 피쳐를 고정시키는 평가된 이웃을 동일한 인스턴스로 줄입니다. 따라서 모델은 모든 이웃을 동일하게 분류하고 규칙의 정확도는 1이 됩니다. 동시에 많은 피쳐를 고정하는 규칙은 지나치게 구체적이어서 일부 인스턴스에만 적용됩니다. 따라서 **정확도와 coverage 사이에는 trade-off가 있습니다.**

앵커 방식은 4가지 주요 구성 요소를 사용해 설명 (explanations)을 찾습니다 : 
- **Candidate Generation (후보 생성)** : 
	- 새로운 설명 후보들을 만듭니다.
	- 첫 번째 라운드에서는, 설명할 인스턴스 $x$ 의 피쳐당 하나의 후보가 만들어지고, 가능한 perturbations의 각각의 값을 고정합니다. 이후 라운드에서는 이전 라운드의 최적 후보들에 아직 포함되지 않은 피쳐 조건들을 하나씩 추가합니다.
- **Best Candidate Identification (최적 후보 식별)** : 
	- 어떤 규칙이 인스턴스 $x$ 를 가장 잘 설명하는지에 대해 candidate rules를 비교해야 합니다. 이를 위해 현재까지 관찰된 규칙과 매치하는 perturbations을 만들고 모델을 호출해 평가합니다. 그러나 이러한 호출은 계산의 부담이 있어 가능한 적게 해야합니다. 이것이 바로 이 component의 핵심인 pure-exploration Multi-Armed-Bandit; MAB; KL-LUCB 의 이유입니다. MAB는 순차적 선택 (sequential selection)을 통해 다양한 전략들 (슬롯 머신의 팔이라고 부름)을 효율적으로 탐색하고 활용하는 데 사용됩니다. 주어진 세팅/환경에서, 각 후보 규칙은 당길 수 있는 팔 (arm that can be pulled)로 간주됩니다. 팔을 당길 때 마다 각각의 이웃 규칙들이 평가되고, 이를 통해 해당 후보 규칙의 보상(payoff, 앵커의 경우 정확도)에 대한 정보를 얻을 수 있습니다. 따라서 정확도는 규칙이 설명하고자 하는 인스턴스를 얼마나 잘 설명하는지를 나타냅니다.
- **Candidate Precision Validation (후보 정확도 검증)** : 
	- 후보가 임계값 $\tau$ 를 넘는다는 통계적 확신/근거가 아직 없는 경우 더 많은 샘플들을 가져옵니다.
- **Modified Beam Search** : 
	- 위의 구성 요소들은 graph search algorithm이자 breadth-first algorithm의 변형인 beam search로 조합됩니다. 이 알고리즘은 각 라운드에서 가장 좋은 후보 $B$ 를 다음 라운드로 넘깁니다 (여기서 $B$는 Beam Width라고 함). 이 $B$ 개의 최적 규칙들은 새로운 규칙을 만드는 데 사용됩니다. 각 피쳐는 최대 한 번만 규칙에 포함될 수 있으므로 beam search 는 최대 $featureCount(x)$ 라운드까지 수행됩니다. 따라서 매 $i$ 번째 라운드마다 정확히 $i$ 개의 조건을 가진 후보들이 생성되고, 그 중 가장 좋은 $B$ 개가 선택됩니다. 따라서 $B$ 를 높게 잡으면 알고리즘이 local optima를 피할 수 있습니다. 결과적으로 많은 수의 모델 호출이 필요하므로 계산 부하가 증가합니다. 

이러한 4개의 구성 요소들은 아래의 그림과 같이 나타납니다 : 

![[Pasted image 20240106175617.png|center]] Figure 9.12 : 앵커 알고리즘의 구성 요소들과 그들의 상호 관계 (단순 버전)

이 방식은 어떤 시스템/모델이 왜 그렇게 인스턴스를 분류했는지에 대한 통계적으로 타당한 정보를 효율적으로 도출하기 위한 완벽한 방법으로 보입니다. 이 방법은 모델의 입력을 체계적으로 실험하고 각 출력을 관찰하여 결론을 내립니다. 잘 정립되고 연구된 ML methods (MABs)를 사용해 모델에 대한 호출 횟수를 줄입니다. 이를 통해 알고리즘의 런타임을 크게 줄일 수 있습니다.

### 2. Complexity and Runtime

앵커 방식의 런타임의 근사적인 동작을 알수 있으면 특정한 문제에 대해 얼마나 잘 수행될지 평가할 수 있습니다. $B$ 는 beam width를 나타내고, $p$ 는 피쳐의 수를 나타냅니다. 그러면 앵커 알고리즘은 아래와 같습니다 : 
$$
\mathcal{O}\left(B \cdot p^2+p^2 \cdot \mathcal{O}_{\mathrm{MAB}[B \cdot p, B]}\right)
$$
이 한계는 통계적 신뢰도 $\delta$ 와 같이 문제와 무관한 hyperparameter로 부터 abstract 됩니다. Hyperparameters를 무시하면 boundary의 복잡성을 줄이는 데 도움이 됩니다. MAB는 각 라운드에서 $B\cdot p$ 개의 후보들 중에서 가장 좋은 $B$ 개를 뽑기 때문에 MAB의 런타임은 다른 모수보다 $p^2$를 곱합니다.

따라서 피쳐가 여러 개일 때 알고리즘의 효율성이 감소한다는 단점이 있습니다.

### 3. Tabular Data Example

자전거 대여 데이터를 사용해서 선택된 인스턴스들에 대한 ML 예측을 설명하는 앵커 방식을 다뤄 보겠습니다. 이를 위해 회귀를 분류 문제로 바꾸고 random forest를 black box model로 학습하겠습니다. 대여한 자전거의 수가 추세선(trend line) 위에 있는지 여부를 분류하는 것입니다.

Anchor explanations 를 만들기 전에 perturbation function을 정의해야 합니다. 쉬운 방법 중 하나는 train data에서 샘플링하여 만들 수 있는 intuitive default perturbation space를 사용하는 것입니다. 이러한 기본적인 방식은 인스턴스를 교란시킬(perturb) 때, anchors의 조건들에 맞는 피쳐값을 유지하면서 고정되지 않은 (non-fixed) 피쳐들을 지정된 확률로 무작위 샘플링된 다른 인스턴스에서 가져온 값으로 대체합니다. 이 과정을 통해 설명한 인스턴스와 유사하지만, 다른 무작위 인스턴스에서 일부 값을 채택한 새로운 인스턴스가 생성됩니다. 따라서 이러한 인스턴스들은 설명된 인스턴스와 이웃 인스턴스들과 유사합니다. 

> [!note]- code fold
> ```r
> set.seed(1)
> colPal = c("#555555","#DFAD47","#7EBCA9", "#E5344E", "#681885", "#d25d97", "#fd3c46", "#ff9a39", "#6893bf", "#42c3a8")
> 
> ## 회귀 -> 분류 전환
> bike$target <- factor(resid(lm(cnt ~ days_since_2011, data = bike)) > 0,
>                       levels = c(F, T), labels = c("below", "above"))
> bike$cnt <- NULL
> 
> # Make long factor levels shorter
> levels(bike$weathersit)[3] <- "BAD"
> 
> 
> bike.task <- makeClassifTask(data = bike, target = "target")
> 
> mod <- mlr::train(learner = makeLearner(cl = 'classif.randomForest',
>                                         id = 'bike-rf'),
>                   task = bike.task)
> bikeDisc <- list(
>   integer(),
>   integer(),
>   integer(),
>   integer(),
>   integer(),
>   integer(),
>   integer(),
>   c(0, 7, 14, 21, 28),
>   c(30, 60, 69, 92),
>   c(5, 10, 15, 20, 25),
>   integer()
> )
> 
> bike.explainer <- anchors(bike, model = mod, target = "target",
>                           bins = bikeDisc, tau = 0.9, batchSize = 1000,
>                           beams = 1)
> bike.explained.instances <- bike[c(40, 475, 610, 106, 200, 700), ]
> bike.explanations <- anchors::explain(x = bike.explained.instances, explainer = bike.explainer)
> plotExplanations(bike.explanations, colPal = colPal)
> ```

![[Pasted image 20240106213759.png|center]] Figure 9.13 : 자전거 대여 데이터셋의 6개 인스턴스를 설명하는 Anchors입니다. 각 행은 하나의 explanation/anchor 를 나타내며, 각 막대는 해당 설명에 포함된 피쳐의 조건들을 나타냅니다. x 축은 규칙의 정확도(precision)를 표시하고 막대의 두께 (thickness)는 해당 규칙의 coverage를 나타냅니다. "base" 기본 규칙에는 조건이 포함되어 있지 않습니다. 이 anchors는 모델이 예측을 위해 주로 온도를 고려한다는 점을 보여줍니다.

이러한 결과는 직관적으로 해석할 수 있으며 설명된 각 인스턴스에 대해서 모델의 예측에 가장 중요한 피쳐가 무엇인지 보여줍니다. 앵커들에는 조건이 몇 개 없기 떄문에 높은 coverage를 가지고 있고, 그로인해 다른 경우에도 적용될 수 있습니다. 위 그림의 규칙들은 $\tau=0.9$로 만들어졌습니다. 따라서 평가된 perturbations이 최소 90%의 정확도의 anchors를 요청합니다. 또한 수치형 피쳐들의 표현력과 적용 가능성을 높이기 위해 이산화(discretization)를 사용했습니다.

위에서 모든 규칙들은 모델이 몇 가지 피쳐에 근거하여 확실하게 결정을 내린 인스턴스들에 대해 생성되었습니다. 그러나 다른 인스턴스들은 더 많은 피쳐들이 중요하기 때문에 모델에 의해 뚜렷하게 분류되지 않습니다. 이런 경우 anchors는 더 구체적이고 더 많은 피쳐들로 구성되며 더 적은 수의 인스턴스에 적용됩니다 : 

> [!note]- code fold
> ```r
> bike.explainer_edge <- anchors(x = bike, model = mod, target = "target", tau = 1,
>                                batchSize = 1000, beams = 1, allowSuboptimalSteps = F)
> bike.explained.instances_edge <- bike[c(452, 300), ]
> bike.explanations_edge <- anchors::explain(x = bike.explained.instances_edge, explainer = bike.explainer_edge)
> plotExplanations(bike.explanations_edge, colPal = colPal)
> ```

![[Pasted image 20240106215002.png|center]] Figure 9.14 : 결정 경계 (decision boundaries)에 가까운 인스턴스를 설명하면 더 많은 수의 피쳐 조건들로 구성된 규칙과 더 낮은 coverage가 나옵니다. 또한 empty rule, 즉 base feature의 중요도가 낮아집니다. 이는 인스턴스가 변동이 심한 지점에 위치하기 때문에 결정 경계에 대한 표시로 해석할 수 있습니다. 

Default perturbation space를 선택하는 것은 편한 선택이지만 알고리즘에 큰 영향을 미치므로 편향된 (biased) 결과를 초래할 수 있습니다. 예를 들어, train set이 불균형한 경우 (imbalanced), 그것의 perturbation space 도 그러합니다. 이러한 상태/조건은 규칙을 찾는 것(rule-finding)과 결과의 정확도에 더 큰 영향을 미칩니다. 

자궁경부암 데이터셋이 이러한 상황을 보여주는 좋은 예시입니다. Anchors 알고리즘을 적용하면 다음 상황 중 하나가 발생합니다 : 
- *Healthy*으로 label 지정된 인스턴스를 설명하면 생성된 모든 이웃 (neighbors)이 *Healthy* 로 평가되기 때문에 empty rules가 만들어집니다.
- *Cancer* 로 라벨링된 인스턴스에 대한 설명이 지나치게 구체적입니다. 즉, perturbation space가 대부분 Healthy 인스턴스들의 값을 포함하기 때문에 많은 피쳐 조건들로 구성됩니다.

> [!note]- code fold
> ```r
> ### cervical anchors
> set.seed(1)
> cervical.sampled.healthy <- cervical[sample(which(cervical$Biopsy == 'Healthy'), 20), ]
> cervical.balanced <- rbind(cervical[cervical$Biopsy == 'Cancer', ], cervical.sampled.healthy)
> 
> set.seed(1)
> cervical.task <- makeClassifTask(data = cervical, target = "Biopsy")
> mod <- train(learner = makeLearner(cl = "classif.randomForest", id = "cervical-rf", 
>                                    predict.type = "prob"),
>              task = cervical.task)
> cancer.explainer <- anchors(x = cervical, model = mod, beams = 1)
> cancer.explanation <- anchors::explain(x = cervical[c(1,7), ], explainer = cancer.explainer)
> 
> set.seed(1)
> cancer.explainer.balanced <- anchors(x = cervical.balanced, model = mod, tau = 1, beams = 2,
>                                      delta = 0.05, epsilon = 0.05, batchSize = 1000,
>                                      emptyRuleEvaluations = 1000)
> cancer.explanation.balanced <- anchors::explain(x = cervical.sampled.healthy[2:5, ], explainer = cancer.explainer.balanced)
> plotExplanations(explanations = cancer.explanation, colPal = colPal)
> ```

![[Pasted image 20240107144712.png|center]] Figure 9.15 : Unbalnced perturbation spaces 내에 anchors를 구성하면 표현력이 떨어지는 결과를 초래합니다.

이러한 원치 않는 결과에 대한 해결책으로는 여러 가지가 있습니다. 예를 들어, custom perturbation space를 정의할 수 있습니다. Custom perturbation은 불균형한 데이터셋으로부터 혹은 정규분포로부터 샘플링할 수 있습니다. 그러나 이 경우 샘플링된 이웃(neighbors)이 대표성이 없고 적용 범위 (coverage's scope)가 변할 수 있다는 단점이 있습니다. 또는 MAB의 신뢰도 $\delta$ 와 error parameter $\epsilon$ 을 바꾸는 방법도 있습니다. 이렇게 하면 MAB가 더 많은 샘플을 추출하게 되어 궁극적으로 소수 집단 (minority)가 더 많이 샘플링될 수 있습니다.

이번 예제에서는 대부분이 *cancer*로 분류되는 자궁경부암 데이터셋의 부분집합을 사용하겠습니다. 그런 다음 이로부터 해당 perturbation space 를 생성하는 프레임워크를 사용하게 됩니다. 이제 perturbations는 다양한 예측들로 이어질 것이며, anchors 알고리즘은 중요한 피쳐들을 식별할 수 있게 됩니다. 하지만 coverage의 정의는 perturbation space 내에서만 정의된다는 점을 고려해야 합니다. 이전 예제에서는 train set을 perturbation space의 기초/기저 (basis)로 사용했습니다. 여기서는 이것의 부분집합을 사용하기 때문에 결과로 나오는 coverage 가 높다고 해서 해당 rule importance가 global 하게 높다는 것을 의미하지 않습니다.

```r
plotExplanations(explanations = cancer.explanation.balanced, colPal = colPal)
```

![[Pasted image 20240107145641.png|center]] Figure 9.16 : Anchors를 구성하기 전에 데이터셋의 균형을 맞추면, 소수 케이스에 대해서 모델이 어떻게 의사 결정을 내리는지 추론할 수 있습니다.

### 4. Advantages

Anchors approach 는 LIME과 비교했을 때 여러 가지 장점들이 있습니다.

- 일반인도 규칙(rules)을 **해석하는 것이 쉽기 때문에** 알고리즘의 출력/결과 (algorithm's output)를 더 쉽게 이해할 수 있습니다.
	- 또한 Anchors 는 **하위 설정이 가능하며 (subsettable)** coverage 개념을 포함함으로써 중요도의 척도/범위를 명시할 수도 있습니다.
- Anchors approach는 **모델 예측이 인스턴스의 이웃에서 비선형적 (non-linear)이거나 복잡할 (complex)때 효과적**입니다. 이 방식은 surrogate model을 fitting 하는 대신 강화 학습 방법을 사용하므로 모델이 과소적합 (underfit)될 가능성이 적습니다.
- 이 알고리즘은 **model-agnostic**이므로 모든 모델에 적용이 가능합니다.
- Batch sampling을 지원하는 MAB (예: BatchSAR)를 사용하여 **병렬화 (parallelized)가 가능하므로 매우 효율적**입니다.

### 5. Disadvantages

- 이 알고리즘은 대부분의 perturbation-based explainer와 마찬가지로 설정이 **매우 까다롭고**, 설정의 영향력이 매우 크다는 단점이 있습니다. 의미 있는 결과 얻기 위해서는 beam width or precision threshold와 같은 hyperparameters를 tuning 해야하고, 뿐만 아니라 하나의 도메인/사용 사례에 맞게 perturbation function을 명시적으로 설계해야 합니다 (need to be explicitly desigend). Tabular data의 perturbation 방식을 image data에 적용할 수 있을까요? 불가능합니다. 다행히 일부 도메인 (예: tabular data)에서는 default approaches를 사용할 수 있으므로 초기 설명 설정 (initial explanation setup)이 용이할 수 있습니다.
- **많은 경우 이산화 (discretization)가 필요한데**, 그렇지 않으면 결과가 너무 구체적(specific)이고, 모델을 이해하는 데 기여하지 못합니다. 이산화는 도움이 될 수는 있지만, 잘못하면 결정 경계가 모호해져 (blur) 정반대의 효과를 가져올 수도 있습니다. 최적의 이산화 기법은 없으므로 사용자는 데이터를 이산화할 때 데이터에 대해 잘 알고 있어야 좋은 결과를 얻을 수 있습니다. 
- Anchors를 구축하기 위해선 **모든 perturbation-based explainers와 마찬가지로 ML model을 여러 번 호출해야 합니다**. 이 알고리즘은 호출 횟수를 최소화하기 위해 MAB를 사용하지만, 런타임은 여전히 모델의 성능에 따라 크게 달라집니다.
- **몇몇 도메인에서는 coverage의 개념이 정의되지 않습니다**. 예를 들어, 한 이미지의 superpixels가 다른 이미지의 superpixels와 어떻게 비교할 수 있는지에 대한 명확하고 보편적인 정의는 없습니다.

### 6. Software and Alternatives

현재 이용가능한 것은 두 가지로, Python의 [`anchor`](https://github.com/marcotcr/anchor)와 [Java implementation](https://github.com/viadee/javaAnchorExplainer)이 있습니다. `anchor`는 앵커 알고리즘의 저자가 참여한 것이고, Java 버전은 이 섹션에서 사용된 R 인터페이스와 함께 제공되는 고성능의 구현입니다. 현재까지 앵커는 tabular data만 지원하지만 이론적으로는 모든 도메인/유형에 대해 구축할 수 있습니다.

---

## 09-05. Shapley Values

인스턴스의 각 피쳐값이 게임의 'player'이고, 예측치가 보상 (payout)인 게임을 가정함으로써 하나의 예측을 설명할 수도 있습니다. 연합 게임 이론 (coalitional game theory)의 한 방법인 Shapley values는 피쳐값 사이에 '보상 (payout)'을 공정하게 분배하는 알려줍니다.

### 1. General Idea

- 다음과 같은 상황을 가정하겠습니다 : 
	- 당신은 아파트 가격을 예측하는 ML 모델을 학습시켰습니다. 특정 아파트에 대해서 30만 유로를 예측했고 이 예측치에 대해 설명을 해야 하는 상황입니다. 이 아파트의 면적은 50㎡ 이고, 2층에 있으며, 근처에 공원이 있고 고양이는 출입금지되어 있습니다.

![[Pasted image 20240107153132.png|center]] Figure 9.17 : 50㎡, 2층, 공원 근처의 고양이 금지 아파트의 예측 가격은 30만 유로입니다. 우리의 목표는 각 피쳐값들이 그러한 예측치에 얼마나/어떻게 기여했는지를 설명하는 것입니다.

모든 아파트의 예측치의 평균 (average prediction)은 31만 유로입니다. 이 평균 예측값과 비교했을 때, 각 피쳐값이 해당 예측치에 얼마나 기여했을까요?

선형 회귀 모델의 경우 간단합니다. 각 피쳐의 효과는 피쳐 가중치에 피쳐값을 곱한 값 (=feature effect)입니다. 이는 모델의 선형성 때문에 잘 작동합니다. 복잡한 모델의 경우 다른 방법이 필요합니다. 예를 들어, LIME은 효과(effect) 추정을 위해 local model을 사용합니다. 또 다른 방법은 협동 게임 이론 (cooperative game theory)에 있습니다 : Shapley (1953)가 창안한 Shapley values는 total payout에 대한 기여도에 따라 플레이에게 보상금을 할당하는 방법입니다. 플레이어는 연합 (coalition)을 이루어 협력하고 이 협력을 통해 일정한 이익을 얻습니다.

'Game'은 데이터셋의 단일 인스턴스에 대한 모델의 예측 작업 (prediction task)을 말합니다. 'Gain'은 이 인스턴스에 대한 예측치에서 모든 인스턴스에 대한 평균 예측치를 뺀 값입니다. 'Players'는 gain을 얻기 위해 협력하는 인스턴스의 피쳐값들 입니다. 아파트 예시에서는 `park-nearby`, `cat-banned`, `area-50`, `floor-2nd` 라는 피쳐값들이 함께 작업하여 30만 유로라는 예측치를 만들었습니다. 우리의 목표는 실제 예측치 (30만 유로)와 평균 예측치 (31만 유로)의 차이값, 즉 -10,000 유로라는 차이를 설명하는 것입니다.

다음과 같은 답이 가능합니다 : `park-nearby`는 30,000유로, `area-50`은 10,000유로, `floor-2nd`는 0유로, `cat-banned`는 -50,000유로를 기여했음. 기여도 (contributions)를 합산해서 -10,000 유로라는 예측치에서 평균 예측치를 뺀값을 계산합니다.

#### How do we calculate the Shapley value for one feature?

Shapley value는 가능한 모든 연합 (coalitions, = all features)에서 피쳐값의 평균 주변 기여도 (average marginal contribution)를 나타냅니다. 

아래 그림에서는 `park-nearby`와 `area-50`의 연합 (coalition)에 `cat-banned` 라는 피쳐값을 추가했을 때 해당 피쳐값의 기여도를 평가합니다. 데이터에서 다른 아파트를 무작위로 추출하고 해당 아파트의 층 (floor) 피쳐값을 이용하여 `park-nearby`, `cat-banned`, `area-50` 만이 연합 (coalition)에 속하는 경우를 시뮬레이션 합니다. `floor-2nd` 라는 값은 무작위로 뽑힌 `floor-1st`로 대체되었습니다. 그런 다음 이 조합으로 아파트 가격 31만 유로를 예측합니다. 두 번째 단계에서는 무작위로 추출된 아파트의  cat allowed/banned 피쳐의 임의 값으로 대체하여 연합 (coalition)에서 cat allowed/banned 를 제거합니다. 예시에서는 `cat-allowed`인 아파트 였지만, 다시 `cat-banned`가 될 수 있습니다. `park-nearby`와 `area-50`의 연합에 대한 아파트 가격을 32만 유로로 예측합니다. `cat-banned`의 기여도는 31만 - 32만 = -10,000유로입니다. 이 추정치는 고양이 출입 가능 여부 피쳐에 대한 'donor' 역할을 한 무작위로 뽑힌 아파트의 피쳐값과 층별 피쳐값에 따라 달라집니다. 이러한 샘플링 단계를 반복하고 기여도들을 평균내면 더 나은 추정치를 얻을 수 있습니다.

![[Pasted image 20240107160506.png|center]] Figure 9.18 : `park-nearby`와 `area-50`의 연합에 `cat-banned`가 추가되었을 때 예측치에 대한 `cat-banned`의 기여도를 추정하기 위해 샘플링을 한 번 반복합니다.

가능한 모든 연합들 (= 가능한 모든 피쳐들의 조합들)에 대해 이러한 계산을 반복합니다. Shapley value는 가능한 모든 연합에 대한 모든 주변 기여도의 평균입니다. 계산 시간은 피쳐 수에 따라 기하급수적으로 증가합니다. 계산 시간을 관리 가능한 (manageable) 수준으로 유지하는 한 방법은 가능한 연합 중 몇 개의 샘플에 대해서만 기여도를 계산하는 것입니다. 

아래 그림은 `cat-banned`에 대한 Shapley value를 결정하는 데 필요한 모든 피쳐값들의 연합을 보여줍니다. 첫 번째 행은 피쳐값이 없는 연합을, 두 번째, 세 번째, 네 번째 행은 '|'로 구분하여 연합 크기가 커짐에 따라 서로 다른 연합을 보여줍니다. 가능한 전체 연합은 아래와 같습니다 : 
- `No feature values`
- `park-nearby`
- `area-50`
- `floor-2nd`
- `park-nearby` + `area-50`
- `park-nearby` + `floord-2nd`
- `area-50` + `floor-2nd`
- `park-nearby` + `area-50` + `floor-2nd`

이러한 각 연합에 대해서 피쳐값 `cat-banned`인 경우와 아닌 경우의 아파트 예측 가격을 계산하고 그것의 차이를 계산해 주변 기여도를 구합니다. Shapley value는 이러한 주변 기여도들의 가중 평균 (weighted average)입니다. ML 모델이 예측치를 얻기 위해 연합에 포함되지 않은 피쳐의 값을 아파트 데이터셋의 임의의 피쳐값으로 대체합니다. 

![[Pasted image 20240107161355.png|center]] Figure 9.19 : `cat-banned`라는 피쳐값의 Shapley value를 계산하기 위해 모두 8가지 연합이 필요합니다. 

모든 피쳐값들에 대해서 Shapley values를 추정하면 피쳐값들 사이에 평균 예측치를 뺀 예측치들의 전체 분포를 얻을 수 있습니다.

### 2. Examples and Interpretation

$j$ 번째 피쳐값에 대한 Shapley value의 해석은 다음과 같습니다 : 데이터셋의 평균 예측치 (average predition)과 비교하여 특정 인스턴스의 예측치에 대해 $j$ 번째 피쳐는 $\phi_j$ 만큼 기여했습니다. 

Shapley value 는 분류 (확률을 다룸)와 회귀 작업 모두에 작동합니다.

자궁경부암 데이터셋에 Random Forest 모델을 사용하여 예측치들을 분석하는 것에 Shapley value를 사용해 보겠습니다 : 

> [!note]- code fold
> ```r
> ## shapley-cervical-prepare
> ntree <- 30
> cervical.x <- cervical %>% select(-Biopsy)
> model <- caret::train(x = cervical.x,
>                       cervical$Biopsy,
>                       method = 'rf', ntree = ntree, maximise = F)
> predictor <- Predictor$new(model = model, class = "Cancer", data = cervical.x, type = 'prob')
> instance_indicies <- 326 # 326번 인스턴스
> x.interest <- cervical.x %>% slice(instance_indicies)
> 
> avg.prediction <- predict(model, type = "prob")[, 'Cancer'] %>% mean()
> actual.prediction <- predict(model, newdata = x.interest, type = 'prob')['Cancer']
> diff.prediction <- actual.prediction - avg.prediction
> 
> 
> ## shapley-cervical-plot
> shapley2 <- Shapley$new(predictor = predictor, x.interest = x.interest, sample.size = 100)
> shapley2$plot() +
>   scale_y_continuous("Feature value contribution") +
>   ggtitle(sprintf("Actual prediction: %.2f\nAverage prediction: %.2f\nDifference: %.2f", actual.prediction, avg.prediction, diff.prediction)) +
>   scale_x_discrete("")
> ```

![[Pasted image 20240107163036.png|center]] Figure 9.20 : 자궁경부암 데이터셋의 한 여성에 대한 Shapley values 입니다. 예측치는 0.57로 이 여성의 암 확률은 평균 예측치인 0.54보다 0.03 높습니다. `STDs`가 이 확률을 가장 크게 높였습니다. 기여도의 합은 실제 예측치와 평균 예측치의 차이 값 0.54를 산출합니다.

자전거 대여 데이터셋의 경우, 날씨와 날짜 정보가 주어지면 하루 동안 대여된 자전거 수를 예측하기 위해 random forest 를 학습시켰습니다. 특정 날짜의 RF 예측치에 대해 생성된 설명은 아래와 같습니다 : 

> [!note]- code fold
> ```r
> ## shapley-bike-prepare
> ntree <- 30
> bike.train.x <- bike %>% select(-cnt)
> model <- caret::train(bike.train.x, bike$cnt,
>                       method = 'rf', ntree=ntree, maximise = F)
> predictor <- Predictor$new(model, data = bike.train.x)
> 
> # 관심 인스턴스 : 295번, 285번
> instance_indices <- c(295, 285)
> 
> avg.prediction <- predict(model) %>% mean()
> actual.prediction <- predict(model, newdata = bike.train.x %>% slice(instance_indices[2]))
> diff.prediction <- actual.prediction - avg.prediction
> x.interest <- bike.train.x %>% slice(instance_indices[2], )
> 
> ## shapley-bike-plot
> shapley2 <- Shapley$new(predictor, x.interest = x.interest)
> shapley2$plot() +
>   scale_y_continuous("Feature value contribution") +
>   ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
>   scale_x_discrete("")
> shapley2 <- Shapley$new(predictor, x.interest = x.interest)
> shapley2$plot() +
>   scale_y_continuous("Feature value contribution") +
>   ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
>   scale_x_discrete("")
> ```

![[Pasted image 20240107164205.png|center]] Figure 9.21 : 285 일의 Shapley values 입니다. 대여된 자전거 수가 이 날의 예측치는 2409대로, 평균 예측치 4518대보다 -2108대가 적습니다. 날씨 상황과 습도가 가장 큰 음의 기여도를 보입니다. 이 날의 기온은 양의 기여도를 보입니다. Shapley values의 합은 실제 예측치와 평균 예측치의 차이값 -2108을 산출합니다. 

Shapley values 를 올바르게 해석하는 것에 주의해야 합니다 : Shapley value는 서로 다른 연합 (coalitions)에서 예측치에 대한 피쳐값의 평균 기여도 (average contribution)입니다. 즉, Shapley value는 피쳐를 제거했을 때 예측치의 차이가 아닙니다.

### 3. The Shapley Value in Detail

우리는 특정 데이터 포인트의 예측치에 각 피쳐가 어떤 영향을 미치는지에 관심이 있습니다. 선형 모델에서는 쉽게 개별 효과를 계산할 수 있습니다. 아래는 하나의 데이터 인스턴스에 대한 선형 모델 예측치 입니다 : 
$$
\hat{f}(x)=\beta_0+\beta_1 x_1+\ldots+\beta_p x_p
$$
여기서 $x$ 는 기여도를 계산하고자 하는 인스턴스입니다. 각 $x_j$ 는 피쳐값으로, $j=1,\ldots,p$ 입니다. $\beta_j$ 는 피쳐 $j$ 에 해당하는 가중치입니다.


예측치 $\hat{f}(x)$ 에 대한 $j$ 번째 피쳐의 기여도 $\phi_j$ 는 아래와 같습니다 : 
$$
\phi_j(\hat{f})=\beta_j x_j-E\left(\beta_j X_j\right)=\beta_j x_j-\beta_j E\left(X_j\right)
$$
여기서 $E(\beta_jX_j)$ 는 피쳐 $j$ 에 대한 효과 추정치의 기대값입니다. 기여도는 피쳐 효과 (feature effect)에서 평균 효과 (average effect)를 뺀 차이값입니다. 이제 각 피쳐가 예측치에 얼마나 기여했는지 알 수 있습니다. 한 인스턴스에 대해서 모든 피쳐들의 기여도를 합산한 결과는 아래와 같습니다 : 
$$
\begin{aligned}
\sum_{j=1}^p \phi_j(\hat{f}) & =\sum_{j=1}^p\left(\beta_j x_j-E\left(\beta_j X_j\right)\right) \\
& =\left(\beta_0+\sum_{j=1}^p \beta_j x_j\right)-\left(\beta_0+\sum_{j=1}^p E\left(\beta_j X_j\right)\right) \\
& =\hat{f}(x)-E(\hat{f}(X))
\end{aligned}
$$
이는 데이터 포인트 $x$ 에서의 예측치에서 평균 예측치를 뺀 값입니다. 이는 음수가 될 수도 있습니다.

이러한 방법은 model-agnostic 한 방법이 될 수 있을까요? 또한 Shapley value 는 모든 ML 모델의 단일 예측치에 대한 피쳐 기여도를 계산하기 위한 솔루션입니다.

#### The Shapley Value

Shapley value는 집합 $S$ 안에 있는 players 에 대한 $val$ 함수를 통해 정의됩니다. 

하나의 피쳐값에 대한 Shapley value는 가능한 모든 피쳐값 조합에 대해 가중치를 부여하고 합산하여 payout 에 대한 기여도들 (contributions)을 나타냅니다 : 
$$
\phi_j(v a l)=\sum_{S \subseteq\{1, \ldots, p\} \backslash\{j\}} \frac{|S| !(p-|S|-1) !}{p !}(\operatorname{val}(S \cup\{j\})-\operatorname{val}(S))
$$
여기서 $S$ 는 모델에 쓰인 피쳐들의 부분집합이고, $x$는 설명하고자 하는 인스턴스의 피쳐값들의 벡터이며, $p$ 는 피쳐 수입니다. $val_x(S)$ 는 집합 $S$ 내 피쳐값들에 대한 예측치로 , 집합 $S$ 에 포함되지 않은 피쳐값에 대한 주변 값입니다 : 
$$
\operatorname{val}_x(S)=\int \hat{f}\left(x_1, \ldots, x_p\right) d \mathbb{P}_{x \notin S}-E_X(\hat{f}(X))
$$
실제로는 $S$ 에 포함되지 않은 각 피쳐에 대해서 여러 번의 적분을 수행합니다 : ML 모델은 4개의 피쳐값 $x_1,x_2,x_3,x_4$ 로 작동하고, 피쳐값 $x_1$과 $x_3$ 으로 구성된 연합 (coalition) $S$ 에 대한 예측치를 평가합니다 : 
$$
\operatorname{val}_x(S)=\operatorname{val}_x(\{1,3\})=\int_{\mathbb{R}} \int_{\mathbb{R}} \hat{f}\left(x_1, X_2, x_3, X_4\right) d \mathbb{P}_{X_2 X_4}-E_X(\hat{f}(X))
$$
이는 선형 모델에서의 피쳐 기여도와 비슷해 보입니다!

"값 (value)"이라는 단어가 많이 쓰이니 혼동되지 마세요 : 피쳐값은 피쳐과 인스턴스의 숫자형 또는 범주형 값이고, Shapley value는 예측치에 대한 피쳐의 기여도이며, value function ($val(\cdot)$) 는 players' coalition (피쳐값들)에 대한 payout 함수입니다.

Shapley value는 **효율성 (Efficiency), 대칭성 (Symmetry), Dummy, 가산성 (Additivity)** 의 속성을 만족하는 유일한 기여도 방식 (attribution method) 이며, 이는 공정한 payout 에 대한 정의로 간주할 수 있습니다. 

- **Efficiency** : 피쳐 기여도(들)은 모두 합산하면 해당 인스턴스에서의 예측치와 평균 예측치를 뺀 차이값이 되어야 합니다 : 
$$
\sum_{j=1}^p \phi_j=\hat{f}(x)-E_X(\hat{f}(X))
$$
- **Symmetry** : 두 피쳐 $j$ 와 $k$ 가 가능한 모든 연합에 동일하게 기여하는 경우, 두 피쳐의 기여도는 동일해야 합니다 : 
	- If $val(S\cup \{j\}) = val(S\cup \{k\})$ for all $S \subseteq \{1,\ldots,p\}\backslash\{j,k\}$ then $\phi_j=\phi_k$.
- **Dummy** : 어떠한 피쳐값 연합 (coalition)에 추가되도 예측치를 변경하지 못하는 피쳐 $j$ 의 Shapley value 는 0이어야 합니다 : 
	- If $val(S\cup\{j\})=val(S)$ for $S\subseteq\{1,\ldots,p\}$ then $\phi_j = 0$.
- **Additivity** : 총 payouts 가 $val + val^+$ 인 게임의 경우 각각의 Shapley values 는 다음과 같습니다 : $$\phi_j +\phi_j^+$$
Random Forest 모델을 학습시켰다고 가정하겠습니다. 가산성 속성 (additivity)은 피쳐값에 대해 각 tree의 Shapley value를 개별적으로 계산하고 평균을 낸 다음 random forest의 피쳐값에 대한 Shapley value를 구하는 것입니다. 

#### Intuition

다음 설명은 Shapley value를 직관적으로 이해하는 법입니다 : 피쳐값은 무작위 순서 (random order)로 방에 들어옵니다. 방에 있는 모든 피쳐값은 게임에 참여합니다 (= contribution to the prediction). 한 피쳐에 대한 Shapley value는 이미 방에 있는 연합이 그 피쳐가 합류할 때 받는 예측치의 평균 변화입니다.

#### Estimating the Shapley Value

정확히 Shapley value를 계산하려면 가능한 모든 피쳐 연합 (집합)을 $j$ 번째 피쳐를 포함한 경우와 포함하지 않은 경우 모두에 대해 평가해야 합니다. 피쳐가 몇 개 이상 추가되면 가능한 연합의 수가 기하급수적으로 증가하기 때문에 이 문제에 대한 exact solution이 어려워집니다. Strumbelj et al. (2014)는 몬테카를로 샘플링을 사용한 근사치를 제안합니다 : 
$$
\hat{\phi}_j=\frac{1}{M} \sum_{m=1}^M\left(\hat{f}\left(x_{+j}^m\right)-\hat{f}\left(x_{-j}^m\right)\right)
$$
여기서 $\hat{f}(x_{-j}^m)$ 는 $x$ 에 대한 예측치이지만, 피쳐 $j$ 의 각 값을 제외한 무작위 데이터 포인트 $z$ 의 피쳐값으로 대체됩니다. 이 $x$-vector $x_{-j}^m$ 은 $x_{+j}^m$ 과 거의 동일하지만, $x_j^m$ 도 샘플링된 샘플링된 $z$ 에서 가져옵니다. 이러한 각 $M$ 개의 새로운 인스턴스들은 두 인스턴스에서 조합된 일종의 '프랑켄슈타인의 괴물'과 같은 것입니다. 아래 나오는 알고리즘에서 피쳐의 순서는 실제로 변경되지 않으며, 각 피쳐는 예측 함수에 전달될 때 동일한 벡터 위치에 유지됨을 유의하세요. 순서는 여기서 'trick'으로만 사용됩니다 : 피쳐들에 새로운 순서를 부여함으로써 '프랑켄슈타인의 괴물'을 조합하는 데 도움이 되는 random mechanism을 사용할 수 있습니다. 피쳐 $x_j$ 의 왼쪽에 나타나는 피쳐들에 경우 원래 관측값을 사용하고, 오른쪽에 있는 피쳐들은 random instance에서의 값을 사용합니다. 

##### Approximate Shapley estimation for single feature value : 
- Output : $j$ 번째 피쳐 값에 대한 Shapley value
- Required : Number of iterations $M$, instance of interest $x$, feature index $j$, data matrix $X$, ML model $f$
	- For all $m=1,\ldots,M$ :
		- data matrix $X$ 에서 random instance $z$ 를 뽑으세요
		- 피쳐값들에서 random permutation $o$ 를 고르세요
		- Order instance $x$ : $x_o = (x_{(1)},\ldots,x_{(j)},\ldots,x_{(p)})$
		- Order instance $z$ : $z_o = (z_{(1)},\ldots,z_{(j)},\ldots,z_{(p)})$ 
		- 두 개의 새로운 인스턴스를 만드세요 : 
			- With $j$ : $x_{+j} = (x_{(1)},\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\ldots,z_{(p)})$
			- Without $j$ : $x_{-j} = (x_{(1)},\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\ldots,z_{(p)})$ 
		- 주변 기여도 (marginal contribution)을 계산하세요 : $\phi_j^m = \hat{f}(x_{+j})-\hat{f}(x_{-j})$ 
- 평균 내어서 Shapley value를 계산 : $\phi_j(x) = \frac{1}{M}\sum_{m=1}^M\phi_j^m$

먼저 관심 인스턴스 $x$와 피쳐 $j$, 반복 횟수 $M$ 을 선택합니다. 각 반복마다 데이터에서 임의의 인스턴스 $z$ 를 뽑고, 피쳐의 random order $o$ 가 만들어집니다. 두 개의 새로운 인스턴스는 관심 인스턴스 $x$와 샘플 $z$ 의 순서를 결합해서 만듭니다. $x_{+j}$ 인스턴스가 관심 인스턴스이지만, 피쳐 $j$ 다음 순서의 모든 값은 샘플 $z$ 의 피쳐값으로 대체됩니다. $x_{-j}$ 인스턴스는 $x_{+j}$와 동일하지만, 피쳐 $j$ 가 샘플 $z$ 의 피쳐 $j$ 로 대체됩니다. 그런 다음 black box model 예측치에서 차이값이 아래와 같이 계산됩니다 : 
$$
\phi_j^m = \hat{f}(x_{+j})-\hat{f}(x_{-j})
$$
$M$ 개의 이 차이값들은 모두 평균내어 다음과 같이 계산됩니다 : 
$$
\phi_j(x) = \frac{1}{M}\sum_{m=1}^M\phi_j^m
$$
이렇게 평균을 내는 것은 $X$ 의 확률분포에 따라 샘플에 내재적으로 가중치를 부여하는 것입니다. 

모든 Shapley values 를 얻으려면 각 피쳐에 대해 이러한 절차를 반복해야 합니다.

### 4. Advantages

- 예측치와 평균 예측치의 차이값은 인스턴스의 피쳐값 사이에 **공평하게 분포되어** 있는데, 이것이 바로 Shapley value의 Efficiency property 입니다. 이 속성은 Shapley value를 LIME 과 같은 다른 methods 과 구별합니다. LIME은 예측치가 피쳐값 간에 공평하게 분포되어 있다는 것을 보장하지 않습니다. Shapley value는 완전한 설명 (full explanation)을 제공하는 유일한 방법일 수 있습니다. 
- Shpley value 는 **대조적인 설명 (contrastive explanations)** 이 가능합니다. 예측치를 전체 데이터셋의 평균 예측치와 비교하는 대신 부분집합 또는 단일 데이터 포인트와 비교할 수 있습니다. 이러한 대조성 (contrastiveness)은 LIME과 같은 local models 에는 없는 기능입니다. 
- Shapley value는 **확고한 이론 (solid theory)** 이 있는 유일한 explanation method입니다. Efficiency, symmetry, dummy, additivity 등의 공리 (axioms)를 통해 설명에 합리적인 기초를 제공합니다. LIME과 같은 방법들은 ML 모델이 로컬에서 선형적으로 동작한다고 가정하지만, 왜 이것이 작동하는지에 대한 이론은 없습니다. 

### 5. Disadvantages

- Shapley value는 **계산 시간이 많이 듭니다**. 실제 문제의 99.9%에서는 approximate solutions 만이 가능합니다. Shapley value의 정확한 계산은 피쳐의 가능한 연합이 $2^k$ 개 이고, 특정한 피쳐값의 'absence'는 무작위 인스턴스를 뽑아서 시뮬레이션해야 하기 때문에 계산 비용이 많이 들며, 이는 Shapley value 추정치의 분산을 증가시킵니다. 연합의 많은 수는 연합을 샘플링하고 반복 횟수 $M$ 을 제한시켜서 처리해야 합니다. $M$ 을 줄이면 계산 시간은 단축되지만 Shapley value 의 분산이 증가합니다. 반복 횟수 $M$ 에 대한 좋은 rule of thumb 은 없습니다 . $M$ 은 Shapley value를 정확하게 추정할 수 있을 만큼 충분히 커야 하지만 적절한 시간 내에 계산을 완료할 수 있을 만큼 작아야 합니다. Chernoff bounds 를 기반으로 $M$ 을 선택할 수 있어야 하지만, ML 예측치들을 위한 Shapley values 에 대해 이렇게 하는 것에 대한 논문은 아직 보지 못했습니다.
- Shapley value는 **잘못 해석될 수 있습니다**. 피쳐값의 Shapley value는 모델 학습에서 피쳐를 제거한 후의 예측값 차이가 아닙니다. 해석이 중요합니다 : 현재의 피쳐값 집합이 주어졌을 때 실제 예측치와 평균 예측치 간의 차이에 대해서 피쳐값의 기여도가 Shapley value 의 추정치입니다. 
- 희소 설명 (sparse explanations, 피쳐가 많이 포함되지 않은 설명)을 찾는 경우 Shapley value는 잘못된 explanation method 입니다. Shapley value로 만든 설명은 **항상 모든 피쳐들을 사용**합니다. 사람들은 보통 LIME 이 생성하는 것과 같이 선택적인 (selective) 설명을 선호합니다. 일반인이 처리해야 하는 설명에는 LIME이 더 나을 수도 있습니다. 이에 대한 해결책은 Lundberg and Lee (2016)이 도입한 [SHAP](https://github.com/shap/shap)인데, 이는 Shapley value를 기반으로 하지만 적은 수의 피쳐들만으로 설명을 제공할 수 있습니다. 
- Shapley value는 피쳐별로 단순한 값을 반환하지만 **LIME과 같이 예측 모델을 제공하지는 않습니다**. 즉, 다음과 같이 입력값의 변화에 따른 예측치의 변화에 설명하는 데 사용할 수 없습니다 : "연간 300 유로를 더 벌면 신용 점수가 5점 상승할 것이다." 라는 식 말입니다.
- 또 다른 단점은 새로운 인스턴스에 대한 Shapley value를 계산하려면 **데이터에 액세스해야 한다는 것**입니다. 관심 있는 인스턴스의 일부를 무작위로 추출한 데이터 인스턴스의 값으로 대체하려면 데이터가 필요하기 때문에 예측 함수에 액세스하는 것만으로는 충분치 않습니다. 이 문제는 실제 데이터 인스턴스처럼 보이지만 train data에서 실제 인스턴스가 아닌 인스턴스를 만들 수 있는 경우에만 피할 수 있는 문제입니다.
- 다른 많은 permutation-based interpretation methods 와 마찬가지로, Shapley value 도 피쳐가 **상호 연관될 때 비현실적인 인스턴스가 포함되는 문제가 있습니다**. 연합에서 피쳐값이 누락되는 것처럼 시뮬레이션 하려면 해당 피쳐를 marginalize 해야 합니다. 이는 피쳐의 주변 분포에서 값을 추출/샘플링하여 수행됩니다. 피쳐들이 독립이라면 괜챃지만, 종속적인 경우 이 인스턴스에 적합하지 않은 피쳐값을 샘플링할 수 있습니다. 하지만 이 샘플을 사용하여 해당 피쳐의 Shapley value를 구할 수는 있습니다. 한 가지 해결책은 상호 연관된 피쳐들을 함께 permute하여 하나의 mutual Shapley value를 구하는 것입니다. 또 다른 방법은 conditional sampling 입니다 : 이미 팀에 있는 피쳐에 따라 조건부로 피쳐를 샘플링하는 것입니다. 조건부 샘플링은 비현실적인 데이터 포인트 문제를 해결하지만, 새로운 문제가 발생합니다 : 결과 값이 symmetry axiom을 위반하기 때문에 더 이상 예측 작업 (game)에서의 Shapley value가 아니며, 이는 Sundararajan et al. (2019)가 발견하고 Janzing et al. (2020)이 추가로 논의한 바 있습니다.

### 6. Software and Alternatives

- Shapley value 방법은 R에서는 `iml` 과 `fastshap` 패키지에서 구현이 가능합니다.
- SHAP은 Shapley values에 대한 대안 추정 방법입니다. 다음 섹션에서 다루겠습니다.
- 또 다른 접근법은 **breakDown** 이라고 하며, `breakDown` 이라는 패키지로 구현되어 있습니다. BreakDown도 예측치에 대한 각 피쳐의 기여도를 나타내지만 단계별로 계산합니다. 게임 비유를 다시 들어보겠습니다 : empty team으로 시작하여 예측치에 가장 큰 기여를 하는 피쳐값을 추가하고, 모든 피쳐가 추가될 때까지 이를 반복합니다. 각 피쳐값이 얼마나 기여하는지는 이미 '팀'에 있는 각 피쳐값에 따라 달라지는데, 이거싱 breakDown 메서드의 큰 단점입니다. 이 방법은 Shapley value 보다 빠르며, 상호작용이 없는 모델의 경우 결과는 동일합니다. 

---

## 09-06. SHAP (SHapley Additive exPlanations)

Lundberg and Lee (2017)에 도입한 SHAP (SHapley Additive exPlanations)은 개별 예측치들을 설명하는 방법입니다. SHAP 은 게임 이론에 기반을 둔 최적의 shapley values 를 찾습니다.

SHAP이 Shapley value의 하위 섹션이 아닌 독자전인 섹션을 갖게된 데에는 두 가지 이유가 있습니다. 1) SHAP 저자들은 local surrogate model에서 영감을 얻은 Shapley values에 대한 대안인 kernel-based 추정 방법인 KernelSHAP을 제안했습니다. 그리고 tree-based models에 효율적인 TreeSHAP 도 제안했습니다. 2) SHAP은 Shapley values의 집계 (aggregations)를 기반으로 하는 다양한 global interpretation 방법을 제공합니다. 이번 섹션에서는 새로운 추정 방식과 global interpretation 방법 모두 설명할 것입니다.

## 1. Definition

SHAP 의 목표는 예측치에 대한 각 피쳐의 기여도를 계산하여 인스턴스 $x$ 의 예측치를 설명하는 것입니다. SHAP은 연합 게임 이론으로 Shapley values를 계산합니다. 데이터 인스턴스의 피쳐값은 연합에서 player 역할을 합니다. Shapley values는 피쳐들 간 "payout" (= the prediction)을 공정하게 배분하는 방법을 알려줍니다. 한 player는 tabular data와 같이 개별 피쳐일 수 있습니다. 어떤 player는 피쳐들의 그룹일 수도 있습니다. 예를 들어 이미지를 설명하기 위해 픽셀들을 superpixels로 그룹화하고 예측치를 이 superpixels에 분배할 수도 있습니다. SHAP이 제공하는 한 가지 혁신적인 것은 Shapley value 설명이 선형 모델처럼 가법적으로 (additive) 피쳐 기여도를 표현할 수 있다는 점입니다. 이러한 관점한 LIME 과 Shapley values를 연결합니다. SHAP 은 설명을 다음과 같이 지정합니다 : 
$$
g(z^\prime) = \phi_0+\sum_{j=1}^M\phi_jz_j^\prime
$$

여기서 $g(\cdot)$ 은 설명 모델 (explanation model), $z^\prime\in\{0,1\}^M$ 은 coalition vector, $M$ 은 최대 연합 크기, $\phi_j \in\mathbb{R}$는 피쳐 $j$ 의 기여도 (즉 shapley value) 입니다. 여기서 "coalition vector"라고 부르는 것은 SHAP 논문에서는 "simplified features" (단순화된 피쳐들)라고 부릅니다. 이미지 데이터의 경우 이미지가 pixel 단위로 표현되는 것이 아니라 superpixels로 집계되기 (aggregated) 때문에 이러한 이름을 사용하는 것 같습니다. $z$'s를 연합을 설명하는 것으로 생각하면 좋을 것 같습니다 : coalition vector에서, 1은 해당 피쳐가 '있음 (present)'을 의미하고, 0은 '없음 (absent)'을 의미합니다. Shapley value에서는 이 개념이 익숙할 것입니다. Shapley values 를 계산하기 위해서 일부 피쳐들만 play ("present")하고, 일부는 그러지 않는 ("absent") 시뮬레이션을 하는 것입니다. 연합들의 선형 모델로 표현하는 것은 $\phi$'s의 계산을 위한 트릭입니다. 관심 인스턴스인 $x$ 에 대해서, coalition vector $x^\prime$ 는 모든 원소가 1인 벡터, 즉 모든 피쳐가 존재하는 벡터입니다. 이 공식은 다음과 같이 단순화됩니다 : 
$$
g(x^\prime) = \phi_0+\sum_{j=1}^M\phi_j
$$
이 공식은 앞선 Shapley value 에 대한 섹션에서 비슷한 표기로 찾을 수 있습니다. 이것들을 추정하는 방법에 대해 자세히 알아보기 전에 먼저 $\phi$'s 의 속성 (properties)에 대해 설명하겠습니다.

Shapley values 는 Efficiency, Symmetry, Dummy, Additivity 를 만족하는 유일한 방법입니다. SHAP은 Shapley values를 계산하기 때문에 이러한 속성들을 만족합니다. SHAP 논문에서는 SHAP의 속성들과 Shapley 속성 간의 불일치를 확인할 수 있습니다. SHAP은 아래와 같은 바람직한 3가지 속성을 나타냅니다 : 

1) **Local accuracy** : 
$$
\hat{f}(x) = g(x^\prime) = \phi_0+\sum_{j=1}^M\phi_jx_j^\prime
$$
여기서 만일 $\phi_0=E_X(\hat{f}(x))$로 정의하고 모든 $x_j^\prime$ 을 1로 두면, 이것은 Shapley의 Efficient property 입니다. 오직 다른 점은 속성의 이름과 coalition vector를 사용했다는 점입니다. 
	$$\hat{f}(x) = \phi_0+\sum_{j=1}^M\phi_jx_j^\prime = E_X(\hat{f}(x))\sum_{j=1}^M\phi_j$$
2) **Missingness** : $$x_j^\prime = 0 \Longrightarrow \phi_j=0$$
	Missingness는 누락된 피쳐는 기여도가 0이라고 말합니다. $x_j^\prime$ 가 0이라는 것은 피쳐가 없음을 나타내는 연합입니다. 연합의 notation에서 설명하고자 하는 인스턴스의 모든 피쳐 $x_j^\prime$ 는 1이어야 합니다. 0이면 관심 있는 인스턴스에 대해 그 피쳐값이 없음을 의미합니다. 이러한 속성은 "일반적인(normal)" Shapley values의 속성에는 속하지 않습니다. 그렇다면 SHAP은 왜 이러한 속성이 필요할까요? Lundberg는 이를 ["minor book-keeping property"](https://github.com/shap/shap/issues/175#issuecomment-407134438) 라고 부릅니다. 누락된 피쳐는 $x_j^\prime=0$을 곱하기 때문에 이론적으로는 local accuracy를 손상시키지 않고 Shapley value를 계산할 수 있습니다. Missingness 속성은 누락된 피쳐가 0이라는 Shapley value를 갖도록 강제합니다. 실제로는 상수 (constant)인 피쳐에만 해당됩니다. 


3) **Consistency** : 
	$\hat{f}_x(z^\prime)=\hat{f}(h_x(z^\prime))$이고 $z_{-j}^\prime$ 이 $z_j^\prime=0$을 나타낸다고 해봅시다. 그렇다면 임의의 두 모델 $f$ 과 $f^\prime$ 에 대해서 아래를 만족합니다 : $$\hat{f}^\prime_x(z^\prime)-\hat{f}^\prime_x(z_{-j}^\prime)\ge \hat{f}_x(z^\prime)-\hat{f}_x(z_{-j}^\prime)$$ all inputs $z^\prime\in\{0,1\}^M$, then : $$\phi_j(\hat{f}^\prime, x)\ge \phi_j(\hat{f},x)$$
	Consistency 속성은 모델이 바뀌어도 피쳐값의 주변 기여도가 증가하거나 동일하게 유지된다면 (다른 피쳐들과 관계없이), Shapley value도 증가/유지함을 말합니다. Consistency로부터 Linearity, Dummy, Symmetry라는 Shapley properties를 따르며, 이는 Lundberg and Lee 의 논문의 Appendix에 설명되어 있습니다. 
## 2. KernelSHAP

KernelSHAP은 예측치에 대한 각 피쳐의 기여도를 인스턴스 $x$ 에 대해 추정합니다. KernelSHAP은 5개의 단계로 구성됩니다 : 
- $z_k^\prime\in\{0,1\}^M,\quad k\in\{1,\ldots,K\}$ 에서 연합들 (coalitions)을 뽑으세요 (1 = 피쳐가 연합에 존재, 0 = 피쳐가 연합에 없음).
- 먼저 $z_k^\prime$ 을 기존/원래의 피쳐 공간으로 변환 (converting, $h_x(z_k^\prime)$)한 다음 모델 $\hat{f}$ 를 적용하여 각 $z_k^\prime$ 에 대한 예측치를 구하세요  : $\hat{f}(h_x(z_k^\prime))$ 
- SHAP kernel을 사용해서 각 $z_k^\prime$ 에 대한 가중치를 계산하세요.
- Weighted linear model을 적합하세요.
- 선형 모델의 계수인 Shapley values $\phi_k$ 를 반환하세요.

0's 과 1's의 연쇄 (chain)가 될 때까지 동전 던지기를 반복하여 무작위 연합 (random coalition)을 만들 수 있습니다. 예를 들어, (1,0,1,0)이라는 벡터는 첫 번째와 세 번째 피쳐가 있는 연합을 의미합니다. 샘플로 추출된 $K$ 개 연합들이 회귀 모델의 데이터셋이 됩니다. 이 회귀 모델의 목적은 연합에 대한 예측입니다 ("이 모델은 이 binary coalition data에 대해 학습되지 않았기 때문에 예측을 할 수 없다"고 질문할 수 있습니다). 피쳐의 연합들에서 유효한 데이터 인스턴스로 가져오려면 $h_x(z^\prime)=z$  (where $h_x \; : \; \{0,1\})^M\to\mathbb{R}^p$) 라는 함수가 필요합니다. 함수 $h_x(\cdot)$ 는 1들을 설명하고자 하는 인스턴스 $x$에 해당되는 값에 매핑합니다. 즉, "피쳐가 없음"을 "피쳐가 데이터에서의 임의의 피쳐로 대체됨"과 동일시한다는 것입니다. Tabular data의 경우 아래 그림에서 나타내는 것처럼 피쳐값으로의 매핑을 표현합니다 : 
![](Pasted image 20240108165245.png) Figure 9.22 : 함수 $h_x(\cdot)$은 연합을 유효한 인스턴스에 매핑합니다. 존재하는 피쳐 (present features, 즉 1의 값을 갖는 피쳐들)에 대해서 $h_x$ 는 $x$ 의 피쳐 값들에 매핑됩니다. 존재하지 않는 피쳐 (즉 0을 갖는 피쳐들)에 대해서 $h_x$ 는 무작위로 샘플링된 데이터 인스턴스의 피쳐값에 매핑합니다.


Tabular data에서의 $h_x(\cdot)$ 는 피쳐 $X_j$ 와 $X_{-j}$ (다른 피쳐들)를 독립이라고 취급하고, 주변 분포에 대해 적분합니다 : 
$$
\hat{f}(h_x(z^\prime)) = E_{X_{-j}}[\hat{f}(x)]
$$
주변 분포에서 샘플링한다는 것은 present feautures 와 absent features 사이의 종속 구조 (dependence structure)를 무시한다는 것입니다. 따라서 KernelSHAP은 모든 permutation-based methods과 동일한 문제를 겪습니다. 이 추정법은 비현실적인 인스턴스들에 지나치게 많은 가중치를 부여합니다. 그렇기 때문에 그 결과는 신뢰할 수 없게 될 것입니다. 그러나 주변 분포에서 표본을 추출할 필요는 있습니다. 해결책은 조건부 분포에서 샘플링하는 것인데, 이는 value function ($val(\cdot)$) 를 변경하여 Shapley value가 해가 되는 게임이 되는 것입니다 : 예를 들어 모델에서 전혀 사용되지 않았을 수도 있는 피쳐는 조건부 샘플링을 사용하면 0이 아닌 Shapley value를 가질 수 있습니다. Marginal game의 경우, 이 피쳐는 항상 0의 Shapley value를 갖게 되는데 이를 통해 Dummy axiom을 만족합니다.  

아래 그림은 이미지이에서 가능한 매핑함수를 설명합니다 : 
![](assets/CH09. Local Model-Agnostic Methods/Pasted image 20240108170351.png)Figure 9.23 : 매핑 함수 $h_x$ 는 superpixels (`sp`)의 연합들을 이미지에 매핑합니다. Superpixels는 pixel 들의 그룹입니다. 피쳐가 있는 경우 (1), $h_x$ 는 원본 이미지의 해당 부분을 반환합니다. 피쳐가 없는 경우 (0), 원본 이미지의 해당 영역을 회색으로 처리합니다. 주변 pixels의 평균 색상 등을 할당하는 것도 옵션이 될 수 있습니다. 

LIME 과의 가장 큰 차이점은 회귀 모델에서 인스턴스의 가중치를 부여하는 방식입니다. LIME 은 인스턴스가 원본 인스턴스에 얼마나 가까운지에 따라 인스턴스에 가중치를 부여합니다. 연합 벡터에 0이 많을수록 LIME의 가중치는 작아집니다. SHAP은 연합이 Shapley value estimation에서 얻을 수 있는 가중치에 따라서 샘플링된 인스턴스에 가중치를 부여합니다. 작은 연합들 (1이 적은 경우)과 큰 연합들 (1이 많은 경우)은 가장 큰 가중치를 받습니다. 개별 피쳐들을 따로 떼어내어 연구할 수 있다면 개별 피쳐에 대해 가장 많은 것을 배울 수 있습니다. 연합이 단일 피쳐로 구성된 경우, 예측치에 대한 이 피쳐의 고립된 주 효과에 대해 알 수 있습니다. 연합이 한 개의 피쳐를 뺀 모든 피쳐로 구성된 경우, 이 피쳐의 total effect (주효과 + 상호작용 효과)에 대해 알 수 있습니다. 연합이 절반의 피쳐로 구성된 경우, 가능한 연합들이 많기 때문에 개별 피쳐의 기여도에 대해 거의 알 수 없습니다. Shapley를 준수하는 가중치 부여방식을 위해서 Lundberg et al. 은 아래와 같은 SHAP kernel을 제안했습니다 : 
$$
\pi_x\left(z^{\prime}\right)=\frac{M-1}{\left(\begin{array}{c}
M \\
\left|z^{\prime}\right|
\end{array}\right)\left|z^{\prime}\right|\left(M-\left|z^{\prime}\right|\right)}
$$
여기서 $M$ 은 최대 연합 크기이고, $|z^\prime$| 은 인스턴스 $z^\prime$에 존재하는 피쳐 수입니다. Lundberg and Lee 는 이 커널 가중치를 사용한 선형 회귀가 Shapley values를 계산함을 보여줍니다. 만약 coalition data에 LIME 과 함께 SHAP kernel을 사용한다면 LIME 도 Shapley values를 추정할 것입니다.

우리는 연합들의 샘플링을 좀 더 스마트하게 할 수 있습니다 : 가장 작고 큰 연합들이 가중치의 대부분을 차지합니다. 무작위로 샘플링하지 말고 대신에 sampling buget $K$ 의 일부를 사용하여 가중치가 높은 연합을 포함하면 더 나은 Shapley value 추정치를 얻을 수 있습니다. 1개와 $(M-1)$ 개 피쳐를 갖는 가능한 모든 연합으로 시작해 총 $2\cdot M$개 연합을 만듭니다. Budget 이 충분히 남으면 (현재 예산은 $K-2M$), 피쳐가 2개인 연합과 $M-2$ 피쳐가 있는 연합들을 포함시킵니다. 나머지 연합 크기에서 가중치를 재조정하여 샘플링합니다. 

이제 데이터, target, 가중치 등 weigted linear regression model 을 구축하는 데 필요한 모든 것이 갖춰졌습니다 : 
$$
g(z^\prime) = \phi_0+\sum_{j=1}^M\phi_jz_j^\prime
$$
아래 손실함수 $L$ 을 최적화하여 선형 모델 $g$ 를 학습시키면 됩니다 : 
$$
L(\hat{f},g,\pi_x) = \sum_{z^\prime \in Z}\left [\hat{f}(h_x(z^\prime))-g(z^\prime)\right]^2\pi_x(z^\prime)
$$
여기서 $Z$ 는 train data를 나타냅니다. 위 식은 일반적으로 선형 모델이 최적화하는 제곱오차의 합입니다. 모델 추정 계수인 $\phi_j$ 는 Shapley values 입니다. 

선형 회귀 setting 이기 때문에 예를 들어 regularization terms 따위를 추가해 모델을 sparse하게 만들 수도 있습니다. L1-penalty를 손실함수 $L$ 에 추가하면 sparse explanations 를 만들 수 있습니다. 하지만 결과로 나온 계수가 유효한 Shapley values가 될지는 확실하지 않습니다.

## 3. TreeSHAP

Lundberg et al. (2018)은 DT, Random Forest, GBM과 같은 tree-based ML models를 위한 SHAP의 변형, TreeSHAP을 제안했습니다. TreeSHAP은 KernelSHAP의 빠른 model-specific 대안으로 도입되었지만, 직관적이지 않은 피쳐 기여도를 생성할 수 있다는 것이 밝혀졌습니다.

TreeSHAP 은 marginal expectation 대신에 조건부 기대치 $E_{X_j|X_{-j}}\left[\hat{f}(x)|x_j)\right]$ 를 사용하여 value function을 정의합니다. 이러한 조건부 기대치의 문제점은 예측 함수 $f$ 에 영향을 미치지 않는 피쳐가 0이 아닌 TreeSHAP 추정치를 얻을 수 있다는 점입니다. 0이 아닌 추정치는 피쳐가 실제로 예측에 영향을 미치는 피쳐들과 상관관계에 있을 때 발생합니다.

TreeSHAP은 얼마나 빠를까요? Exact KenelSHAP 과 비교했을 때, 계산 복잡도를 $O(T L 2^M)$ 에서 $O(TLD^2)$ 로 줄입니다. 여기서 $T$ 는 tree의 수, $L$ 은 각 tree의 최대 잎 수, $D$ 는 tree의 최대 깊이 입니다. 

TreeSHAP 은 조건부 기대값 $E_{X_j|X_{-j}}\left[\hat{f}(x)|x_j)\right]$ 을 사용하여 효과를 추정합니다. 단일 tree로 인스턴스 $x$ 와 피쳐 부분집합 $S$ 에 대한 예측치의 기대값을 계산하는 방법을 직관적으로 설명해 보겠습니다. 모든 피쳐에 조건을 붙일 경우, 즉 $S$ 가 모든 피쳐의 집합일 경우, 인스턴스 $x$ 가 속하는 node의 예측치가 기대 예측치가 될 것입니다. 어떠한 피쳐에 대해서도 조건을 걸지 않을 경우, 즉 $S$ 가 empty 일 경우, 모든 터미널 노드의 예측치의 가중 평균 (weighted average)를 사용합니다. $S$ 에 일부의 피쳐가 있으면 연결할 수 없는 (unreachable) 노드들의 예측치는 무시합니다. 여기서 unreachable은 이 노드로 이어지는 의사 결정 경로 (decision path)가 $x_S$ 의 값과 모순/반대된다는 것을 의미합니다. 나머지 터미널 노드들로부터 노드 크기 (즉, 해당 노드의 train sample 수)에 따라 가중치를 부여한 예측치들의 평균을 구합니다. 노드당 인스턴스 수로 가중치를 부여한 나머지 터미널 노드의 평균이 주어진 $S$ 에 대한 기대 예측치입니다. 문제는 피쳐의 가능한 각 부분집합 $S$ 에 대해 이 절차를 적용해야 한다는 것입니다. 자세한 내용은 원본 논문을 참고하길 바랍니다. 

## 4. Examples

자궁경부암의 위험을 예측하기 위해 100 trees로 random forest classifier를 학습시켜 보겠습니다. 개별 예측치를 설명하기 위해 SHAP을 사용하겠습니다. Random Forest 는 trees의 ensemble이므로 느린 KernelSHAP 대신 빠른 TreeSHAP을 사용하겠습니다. 하지만 이 예제에서는 조건부 분포에 의존하는 대신 주변 분포를 사용합니다. 이 내용은 패키지에는 설명되어 있지만 원본 논문에는 나와 있지 않습니다. Python의 TreeSHAP 함수는 주변 분포를 사용하면 속도는 느려지지만 (데이터의 행에 따라 선형적으로 스케일링되기 때문에) 여전히 KernelSHAP보다는 빠릅니다. 

여기서는 주변 분포를 사용하기 때문에 해석은 Shapley value 섹션과 동일합니다. Python의 SHAP 패키지를 사용하면 다양한 시각화가 가능합니다 : Shapley values 와 같은 피쳐 기여도를 '강제로' 시각화할 수 있습니다. 각 피쳐는 예측치를 증가시키거나 감소시키는 힘 (force)입니다. 예측치는 baseline에서 시작합니다. Shapley values에 대한 baseline은 모든 예측치의 평균입니다. 플롯에사 각 피쳐에 대한 Shapley value는 예측치를 증가시키거나 (양수 값) 감소시키는 (음수값) 화살표로 나타납니다. 이러한 힘 (force)은 인스턴스의 실제 예측치에서 서로 균형을 맞춥니다. 

아래 그림은 자궁경부암 데이터셋의 두 여성에 대한 SHAP explanation force plots을 나타냅니다 : 

![](assets/CH09. Local Model-Agnostic Methods/Pasted image 20240108225207.png) Figure 9.24 : 두 여성의 예측 암 확률을 설명하는 SHAP 입니다. Baseline (평균 예측 확률)은 0.066 입니다. 첫 번째 여성의 예측 위험도는 0.06으로 낮습니다. `STDs` 와 같은 위험 증가 효과는 `Age`와 같은 감소 효과로 상쇄됩니다. 두 번째 여성의 예측 위험도는 0.71로 높습니다. 51세라는 나이와 34년간의 흡연으로 인해 예측 암 위험이 증가합니다. 

이러한 것들은 개별 예측치에 대한 설명입니다. 

Shapley values는 전역적 설명 (global explanations)로 결합할 수 있습니다. 모든 인스턴스들에 대해 SHAP을 실행하면 Shapley values의 행렬을 얻을 수 있습니다. 이 행렬의 Shapley values를 분석해 전체 모델을 해석할 수 있습니다.

먼저 SHAP feature importance부터 살펴보겠습니다.

## 5. SHAP Feature Importance

SHAP feature importance의 개념은 간단합니다 : Shapley values의 절대값이 큰 피쳐가 가장 중요하다는 것입니다. Global importance를 원하기 때문에 데이터 전체에서 피쳐별 Shapley values의 절대값의 평균을 구합니다 : 
$$
I_j = \frac{1}{n}\sum_{i=1}^n|\phi_j^{(i)}|
$$
그런 다음 중요도가 낮아지는 순서대로 피쳐들을 정렬하고 이를 플로팅합니다. 아래 그림은 자궁경부암 예측 작업을 위해 앞서  학습한 random forest의 SHAP feature importance를 보여줍니다.
![](assets/CH09. Local Model-Agnostic Methods/Pasted image 20240108235133.png) Figure 9.25 : SHAP feature importance는 Shapley values의 절대값들의 평균으로 측정됩니다. 호르몬 피임약 복용 기간은 가장 중요한 피쳐로, 예측 암 확률의 절대값을 평균적으로 2.4%p 변화시켰습니다.

SHAP feature importance는 permutation feature importance의 대안이 됩니다. 두 방시에는 큰 차이가 있습니다 : Permutation importance는 모델 성능의 감소를 기반으로 합니다. SHAP은 피쳐 기여도의 크기를 기반으로 합니다. 

피쳐 중요도 플롯은 유용하지만 중요도 이상의 정보는 포함하지 않습니다. 더 많은 정보를 제공하는 플롯을 위해 이제 summary plot을 살펴보겠습니다.

## 6. SHAP Summary Plot

Summary plot은 피쳐 중요도와 피쳐 효과를 결합합니다. Summary plot의 각 포인트는 피쳐와 인스턴스에 대한 Shapley value 입니다. y축의 위치는 피쳐에 의해 결정되고 x축의 위치는 Shapley value로 결정됩니다. 색상은 낮은 값부터 높은 값까지 피쳐값을 나타냅니다. 겹치는 포인트들은 y축 방향으로 흔들리므로 (jittered) 피쳐별 Shapley value의 분포를 파악할 수 있습니다. 피쳐들은 중요도에 따라 정렬됩니다. 
![](assets/CH09. Local Model-Agnostic Methods/Pasted image 20240109000047.png) Figure 9.26 : SHAP summary plot 입니다. 호르몬 피임약을 복용한 기간이 짧으면 예측 암 위험이 감소하고, 기간이 길면 위험이 증가합니다. 유의할 점 : 모든 효과들은 모델의 동작을 설명하며 실제 세계에서 반드시 인과관계인 것은 아닙니다. 

Summary plot에서는 먼저 피쳐와 예측치 사이의 영향 관계를 볼 수 있습니다. 그러나 관계의 정확한 형태를 보려면 SHAP dependence plot을 살펴봐야 합니다.

## 7. SHAP Dependence Plot

SHAP feature dependence는 가장 간단한 global interpertation plot 입니다 : 1) 피쳐를 고르고, 2) 각 인스턴스에 대해서 x축에는 피쳐값을, y축에 해당 Shapley value를 사용하여 점을 그립니다. 3) 완료.

수학적으로, 플롯은 다음과 같은 포인트들을 포함합니다 : $\left\{\left(x_j^{(i)},\phi_j^{(i)}\right)\right\}_{i=1}^n$ 
아래 그림은 호르몬 피임약 복용 년수에 대한 SHAP feature dependence 를 보여줍니다 : 
![](assets/CH09. Local Model-Agnostic Methods/Pasted image 20240109001230.png) Figure 9.26 : 호르몬 피임약에 대한 연도별 SHAP dependence plot 입니다. 0년과 비교하여 몇 년이 지나면 예측 확률이 낮아지고, 몇 년이 지나면 예측 암 확률이 높아집니다.

SHAP dependence plots는 PDP와 ALE의 대안입니다. PDP와 ALE plot은 평균 효과를 보여주는 반면, SHAP dependence는 y축의 분산도 보여줍니다. 특히 상호작용이 있는 경우 SHAP dependence plot은 y축에 훨씬 더 퍼져 있습니다 (높은 분산). 이러한 피쳐 상호작용을 강조하면 dependence plot을 개선할 수 있습니다.

## 8. SHAP Interaction Values

상호작용 효과는 개별 피쳐 효과를 고려한 후 추가로 결합된 피쳐 효과입니다. 게임 이론의 Shapley interaction index는 아래와 같이 정의합니다 : 
$$
\phi_{i,j} = \sum_{S\subseteq\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)
$$
$i\ne j$ 일 때 : 
$$
\delta_{ij}(S)=\hat{f}_x(S\cup\{i,j\})-\hat{f}_x(S\cup\{i\}) - \hat{f}_x(S\cup\{j\})+\hat{f}_x(S)
$$
위 공식은 개별 효과를 고려한 후 순수한 상호작용 효과를 얻기 위해 피쳐의 주효과를 뻅니다. Shapley value 계산에서와 같이 모든 피쳐 연합 $S$ 에 대한 값의 평균을 구합니다. 모든 피쳐에 대한 SHAP interaction values를 계산하면 인스턴드당 $M\times M$인 하나의 행렬을 얻게 되는데 이 때 $M$ 은 피쳐 개수입니다. 

Interaction index는 어떻게 사용할 수 있을까요? 예를 들어, 상호작용이 가장 강한 SHAP feature dependence plot에 자동으로 색상을 지정하는 데 사용할 수 있습니다 : 

![](assets/CH09. Local Model-Agnostic Methods/Pasted image 20240109002639.png) Figure 9.28 : 상호작용에 대한 시각화가 있는 SHAP feature dependence plot입니다. 호르몬 피임약 복용 년수는 `STDs`와 상호작용합니다. 0년에 가까운 경우, 성병이 발생하면 예측 암 위험이 증가합니다. 피임약 복용 기간이 길어질수록 성병 발생은 예측 위험을 감소시킵니다. 다시 말하지만, 이것은 인과관계가 있는 모델이 아닙니다. 효과들은 교락 (counfounding)에 인한 것일 수 있기 때문입니다 (예: 성병과 낮은 암 위험은 의사를 더 많이 방문하기 때문일 수 있음).

## 9. Clustering Shapley Values

Shapley values를 사용하여 데이터를 clustering할 수 있습니다. 클러스터링의 목표는 유사한 인스턴스들의 그룹을 찾는 것입니다. 일반적으로 클러스터링은 피쳐들을 기반으로 합니다. 피쳐들은 종종 서로 다른 척도로 측정됩니다. 예를 들어, 높이는 미터 단위로, 색의 강도 (color intensity)는 0 ~ 100으로, 일부 센서 출력은 -1 ~ 1로 측정됩니다. 이렇게 서로 비교 불가능한 피쳐를 가진 인스턴스 간의 거리를 계산하는 것이 어렵습니다. 

SHAP clustering은 각 인스턴스의 Shapley values를 클러스터링하는 방식으로 작동합니다. 즉, 설명 유사성 (explanation similarity)에 따라 인스턴스들을 군집화하는 것입니다. 모든 SHAP values는 예측 공간 (prediction space)의 단위와 동일한 단위를 갖습니다. 클러스터링의 모든 방법들을 사용할 수 있습니다. 아래 예는 hierarchical agglomerative clustering 을 사용하여 인스턴스의 순서를 지정합니다. 

이 플롯은 여러 개의 힘 플롯 (force plots)으로 구성되어 있으며, 각 플롯은 인스턴스의 예측치를 설명합니다. 이 force plots를 수직으로 돌리고 clustering similarity에 따라 나란히 배치합니다. 
![](assets/CH09. Local Model-Agnostic Methods/Pasted image 20240109004509.png) Figure 9.29 : Explanation similarity에 따라 클러스터링된 stacked SHAP explanations 입니다. x축의 각 위치는 데이터 인스턴스입니다. 빨간색 SHAP values는 예측치를 증가시키고, 파란색은 감소시킵니다. 하나의 클러스터가 눈에 띕니다 : 오른쪽은 예측 암 위험이 높은 그룹입니다. 

## 10. Advantages

- SHAP 은 Shapley values를 계산하기 때문에 그것의 모든 장점들이 적용됩니다 : SHAP은 게임 이론의 **탄탄한 이론적 기반**을 가지고 있습니다. 예측치는 피쳐 사이에 **공평하게 분포**되어 있습니다. 예측치와 평균 예측치를 비교하는 **대조적인 설명**을 얻을 수 있습니다. 
- SHAP은 **LIME과 Shapley values를 연결합니다**. 이는 두 방법을 더 잘 이해하는 데 유용합니다. 또한 interpretable ML 분야를 통합하는 데에도 도움이 됩니다. 
- SHAP은 **tree-based models를 빠르게 구현할 수 있습니다**. Shapley values를 채택하는 데 있어서 가장 큰 장벽이 계산 속도가 느리다는 것이기 때문에 이 점이 SHAP의 인기 비결이라고 생각합니다. 
- 빠른 계산을 통해 **global model interpretation**에 필요한 많은 Shapley values를 계산할 수 있습니다. Global interpretation methods에는 변수 중요도, dependence, interaction, clustering, summary plot이 포함됩니다. Shapley values는 global interpretations의 ''기본 단위 (atomic unit)"이기 때문에 SHAP을 사용하면 global 해석이 local 설명과 일치합니다. Local explanations에 LIME을 사용하고 PDP와 permutation feature importance를 global 설명으로 사용하는 경우 공동의 기반 (= 두 가지가 일치한다는)이 부족합니다. 

## 11. Disadvantages

- **KernelSHAP**은 느립니다. 따라서 많은 인스턴스에 대한 Shapley values를 계산하려는 경우 KernelSHAP을 사용하기에는 비실용적입니다. 또한 SHAP feature importance와 같은 모든 global SHAP methods는 많은 인스턴스에 대한 Shapley values를 계산해야 합니다. 
- **KernelSHAP 은 feature dependence를 무시합니다**. 대부분의 다른 permutation based 해석 방법에는 이 문제가 있습니다. 피쳐값을 무작위 인스턴스의 피쳐로 대체하면 보통 주변 분포에서 무작위로 샘플링하는 것이 더 쉽습니다. 그러나 피쳐가 상관관계에 있는 경우, 이는 비현실적인 데이터 포인트에 너무 많은 가중치를 부여하게 됩니다. TreeSHAP은 조건부 기대 예측을 명시적으로 모델링하여 이를 해결합니다. 
- **TreeSHAP은 직관적이지 않은 피쳐 기여도를 생성할 수 있습니다**. TreeSHAP은 가능성이 낮은 데이터 포인트로 외삽하는 (extrapolate) 문제를 해결하지만 value function을 변경하는 방식으로 해결하기 때문에 게임의 판도가 약간 달라집니다. TreeSHAP은 조건부 기대 예측치에 의존하여 value function을 변경합니다. Value function이 변경되면 예측치에 영향을 미치지 않는 피쳐가 0이 아닌 TreeSHAP 값을 얻을 수 있습니다.
- Shapley values의 단점은 SHAP 에도 적용됩니다 : Shapley values는 잘못 해석될 수 있으며, 새로운 데이터를 계산하기 위해서는 데이터에 대한 액세스가 필요합닏 (TreeSHAP 제외).
- SHAP은 편향 (biases)을 숨길 수 있기 떄문에 **의도적으로 오해의 소지가 있는 해석을 할 수 있습니다**. 

