---
sticker: emoji//1f4d5
date: 2023-11-14
tags:
  - Anomaly_Detection
  - 이상탐지
  - Auto-encoder
  - 1-SVM
  - SVDD
  - ML
---
#  Auto-Encoder, 1-SVM, SVDD

1-SVM과 SVDD는 특정한 조건이 만족한다는 가정하에서는 솔루션이 똑같은데, 접근 방법이 약간 다른 방법론이다. 

## Auto-Encoder Anomaly Detection

먼저 Auto encoder를 통한 anomaly detection 방법에 대해 알아보자. 

#### Auto Encoder : Auto-Associative Neural Network

여기서는 Feed-forward NN을 기준으로 설명하지만, 이미지에 대한 이상 탐지를 할 경우에는 Convolution auto encoder를 사용할 수도 있고, 또는 sequence를 이용해서 시계열 데이터나 텍스트 데이터를 이용한 이상 탐지에서는 RNN 기반의 auto encoder를 사용할 수 있다. 

이 경우 모두 적용되는 원칙이 있는데, NN을 학습할 때 input을 output으로 최대한 가깝게 reproduction 하는 것이 목적이다. 이 때 사용되는 목적함수를 살펴보자.

- FFNN trained to <font style="color:skyblue">reproduce</font> its input at the output layer
	- Loss fuunction : $$l\left(f(\mathbf{x})\right) = \frac{1}{2}\sum_{k}(\hat{x}_k-x_k)^2$$
	- 여기서 $x_k$ 는 input, $\hat{x}_k$ 는 reproduction이다. 

![[Pasted image 20240119182947.png|center]]

가장 기본적인 FFNN을 기준으로 살펴보자. Input data $\mathbf{x}$ 가 들어왔을 때 이 d-차원 데이터가 weight $\mathbf{W}$ 를 통해서 hidden vector $\mathbf{h}(\mathbf{x})$ 로 변환이 된다. 이 과정을 정보를 축약하는 <font style="color:skyblue">Encoding</font> 과정이라고 하고 Encoder라고 부른다. 핵심은 $h<d$, 즉 반드시 Encoding 과정에서 <font style="color:skyblue">정보의 축약</font> 이 일어나야 한다는 것이다. 이렇게 압축된 데이터를 다시 펼치는 것을 <font style="color:skyblue">Decoding</font> 과정이라 하고 이를 수행하는 부분을 Decoder라고 부른다. 정보를 encoding하고 decoding하는 과정에서 정보의 축약이 발생했기 때문에 원래의 정보를 충분히 잘 보존할 수 있는 것을 잠재 벡터 (여기서는 hidden layer의 nodes 값) latent vector라고 부른다. 이 latent vector는 정상 데이터에 대해서는 충분히 학습했을 것이므로 정상 데이터가 들어오면 reproduction을 잘할 것이고, 이상 데이터가 들어오게 되면 reproduction이 제대로 되지 않아 loss 값이 커지게 될 것이다. 

따라서 이 loss 값을 anomaly score로 사용한다. 

- Overcomplete and Undercomplete hidden layers for AE

![[Pasted image 20240119183647.png|center]]

Hidden layer로 불리는 bottleneck layer가 반드시 존재해야 하는데, 이 bottleneck layer는 input의 정보량보다 적어야 한다는 특징을 갖는다. 즉 반드시 정보의 축약이 일어나야 한다는 것이다. 

- Example 

![[Pasted image 20240119184223.png|center]]

Input이 $32\times32$ 라면, $\mathbf{x}\in \mathbb{R}^{32^2}$이 되고, hidden layer $\mathbf{h}\in \mathbb{R}^h$이 되는데 이때 정보의 축약을 위해 $h < 32^2$ 이어야 한다. Auto encoder가 잘 작동한다는 가정에서, embedding/latent vector $h$ 는 원래의 input 보다 정보가 축약되어 있으니까 이를 **차원 축소 기법**으로 사용할 수도 있다. 하지만 이상 탐지에서는 decoding 과정을 추가로 수행한다. 위 그림에서는 '2'라는 숫자를 잘 reproduction 하는 상황이다. 만약 뜬금없이 '4'라는 숫자가 reproduct 된다면 그 데이터의 loss 인 $(\mathbf{x}-\mathbf{\hat{x}})^2$ 가 굉장히 커져서 큰 anomaly score를 갖게된다. 

## Support Vector-based Novelty Detection

앞서 밀도 기반의 방법론들(LOF, Parzen window, KNN 등)은 특정한 데이터가 들어왔을 때 그 객체가 정상 범주일 확률을 내뱉는 것이 일반적인데, SV-based 방법론은 그것과는 다르게 어떠한 함수를 찾는다. 그 함수는 정상과 비정상을 구분하는 경계면을 찾는 함수다. 

#### Support vector-based novelty detection

- 정상과 이상 데이터를 구분짓는 함수를 직접 (directly) 찾음으로써 정상 영역에 대한 경계면을 구한다. 
	- 보통의 score 기반의 이상 탐지 방법과는 달리 명시적으로 (explicitly) '경계면 안에 있으면 정상이고, 이를 넘으면 이상치다' 라는 판단을 내리는 함수를 찾는 것
	- 이 함수를 찾기 위한 두 가지 방법 : 
		1) 1-SVM (One-class SVM)
		2) SVDD (Support vector data description)

![[Pasted image 20240119185408.png|center]]

## One-Class Support Vector Machine

#### 1-SVM

- 1-SVM의 목적 : kernel에 맞게 데이터를 Feature space로 mapping 하는데, 정상 데이터를 원점에서 최대한 멀어지도록 분류 경계면 (hyperplane)을 설정하는 것

##### Optimization problem : 

$$
\begin{aligned}
\min_\mathbf{w} &\frac{1}{2}||\mathbf{w}||^2+\frac{1}{\nu l}\sum_{i=1}^l\xi_i-\rho\\
s.t. \;&\mathbf{w}\cdot\Phi(\mathbf{x}_i)\ge \rho - \xi_i\\
i= &1,2,\cdots,l,\quad \xi\ge 0
\end{aligned}
$$
- $-\rho$의 의미 : 원점에서 최대한 멀리 떨어진 hyperplane과의 거리를 찾기 위함
- 두 번째 항 $\frac{1}{\nu l}\sum_{i=1}^l\xi_i$ 의미 : $\xi_i$ 가 개별적인 객체의 원점과의 거리가 $\rho$ 보다 가까울 때, 즉 정상 데이터는 원점으로부터 hyperplane 보다 바깥에 있어야 하는데, 그러지 못했을 때 부여되는 penalty. 
	- 즉 경계면 조건을 만족하지 못하는 정상 데이터에 부여되는 penalty
	- 이 penalty가 $s.t.$ 부분의 좌변에 대해 부여되는데 $\rho$ 보다 가까운 경우에 대해서 도달하지 못한만큼의 penalty를 부여하는 방식으로 구현 - Soft margin SVM과 비슷
- 첫 번째 항 $\frac{1}{2}||\mathbf{w}||^2$ 의미 : 모델의 변동성 감소
- $0\le \nu\le 1$ : C-SVM에서는 $\frac{1}{\nu l}$ 대신 $C$를 $\xi$ 의 summation 계수로 사용하는데, $C$ 는 데이터셋에 따라서 얼마를 사용해야 하는지 감이 잘 잡히지 않지만, 0~1의 값을 가지므로, 어느 정도 값을 가질 때 decision boundary가 어떤식으로 만들어질지에 대한 감이 잡힘
	- C-SVM을 먼저 배우는 이유는 수식을 풀어감에 있어서 더 수월하기 때문

##### Decision function : 

$$
f(\mathbf{x}_i) = sign(\mathbf{w}\cdot\Phi(\mathbf{x}_i)-\rho)
$$

![[Pasted image 20240119214942.png|center]]

##### Primal Lagrangian problem - Minimize

$$
L = \frac{1}{2}||\mathbf{w}||^2+\frac{1}{\nu l}\sum_{i=1}^l\xi_i-\rho - \sum_{i=1}^l\alpha_i(\mathbf{w}\cdot\Phi(\mathbf{x}_i)-\rho+\xi_i)-\sum_{i=1}^l\beta_i\xi_i
$$
- $L$ 의 처음 3개의 항이 원래 가지고 있던 목적함수에 해당하는 것
- Constraint 1 : 개별 정상 객체가 원점으로부터의 hyperplane 바깥쪽에 있어야 하고, 그렇지 않은 객체들에 대해 penalty를 부여 ⇒ $\alpha_i$ 가 있는 항
- Constraint 2 : $\xi_i$ 라는 penalty는 0 이상이어야 한다는 제약 ⇒ $\beta_i$ 가 있는 항

##### KKT condition

- 찾아야할 미지수 : $\mathbf{w}$, $\xi_i$, $\rho$  ⇒ 이들로 $L$ 을 편미분
- $\nu$ : 사용자가 지정해야할 hyperparameter
- $l$ : 가지고 있는 정상 데이터의 수

$$
\begin{aligned}
\frac{\partial L}{\partial\mathbf{w}} &= \mathbf{w} - \sum_{i=1}^l\alpha_i\Phi(\mathbf{x}_i) = 0 &&\Rightarrow\quad \mathbf{w} = \sum_{i=1}^l\alpha_i\Phi(\mathbf{x}_i)\\
\frac{\partial L}{\partial\xi_i} &=\frac{1}{\nu l}-\alpha_i-\beta_i = 0 &&\Rightarrow\quad\alpha_i=\frac{1}{\nu l}-\beta_i\\
\frac{\partial L}{\partial\rho} &= -1+\sum_{i=1}^l\alpha_i = 0 &&\Rightarrow\quad \sum_{i=1}^l\alpha_i=1
\end{aligned}
$$

##### Dual Lagrangian problem - Maximize 

KKT condition을 통해 계산된 결과를 primal lagrangian $L$ 에 대입하면 아래와 같이 변해서 dual lagrangian problem 으로 풀 수 있다 : 
$$
\begin{aligned}
L = &\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)+\frac{1}{\nu l}\sum_{i=1}^l\xi_i-\rho\\
&-\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j) + \rho\sum_{i=1}^l\alpha_i-\sum_{i=1}^l\alpha_i\xi_i-\sum_{i=1}^l\beta_i\xi_i
\end{aligned}
$$

위 식에서 $\xi_i$ 가 있는 항들만 묶어서 보면 아래와 같다 : 
$$
\sum_{i=1}^l\left(\frac{1}{\nu l}-\alpha_i-\beta_i\right)\xi_i
$$
이는 KKT condition에 의해 0이 된다. 

그리고 $\rho$ 와 관련된 항들을 묶으면 $\rho\left(-1+\sum_{i=1}^l\alpha_i\right)$ 가 되므로 이 또한 KKT condition에 의해 0이 된다. 최종적으로 $L$ 은 아래와 같이 된다 : 
$$
L = -\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)
$$
이를 부호를 바꾸어서 아래 식을 최소화하는 문제로 바꿀 수 있다 : 
$$
\begin{aligned}
\min\;&L = \frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)\\
s.t.\;&\sum_{i=1}^l\alpha_i,\quad 0\le\alpha_i\le\frac{1}{\nu l}
\end{aligned}
$$
이렇게 dual problem이 minimization problem으로 바뀌면서 $\alpha$ 에 대한 깔끔한 Quadratic covex 함수가 된다. 

##### Employ Kernel Trick for a non-linear mapping

![[Pasted image 20240119223417.png|center]]
![[Pasted image 20240119223431.png|center]]

이제 이 $\alpha$ 값들의 의미에 대해 살펴보자. 우선 우리가 알고 있는 조건들은 아래와 같다 : 
$$
\sum_{i=1}^l\alpha_i=1,\quad 0\le\alpha_i\le\frac{1}{\nu l},\quad \alpha_i+\beta_i=\frac{1}{\nu l},\quad \sum_{i=1}^l\left(\frac{1}{\nu l}-\alpha_i-\beta_i\right)\xi_i=0
$$
- Location of point w.r.t. $\alpha_i$ : 
	- Case 1 (흰색 포인트) : $\alpha_i=0$ ⇒ $\beta_i=\frac{1}{\nu l}\ne 0$ ⇒ $\xi_i=0$ ⇒ a non-support vector
	- Case 2 (검은색 포인트) : $\alpha_i=\frac{1}{\nu l}$ ⇒ $\beta_i=0$ ⇒ $\xi_i>0$ ⇒ Support vector - outside the hyperplane
	- Case 3 (회색 포인트) : $0<\alpha_i<\frac{1}{\nu l}$ ⇒ $\beta_i > 0$ ⇒ $\xi_i=0$ ⇒ Suppor vector - on the hyperplane

![[Pasted image 20240119225414.png|center]]

##### The role of $\nu$ 

$$
0\le\alpha_i\le\frac{1}{\nu l},\quad \sum_{i=1}^l\alpha_i=1
$$
- $\alpha_i$의 가능한 최대값은 $\alpha_i=\frac{1}{\nu l}$
- $\alpha_i$가 최대값을 가지면서 합이 1이되려면 $\nu l$ 개가 필요하다
	- 만약 $l=1000$, $\nu=0.1$이라면 가능한 최대값은 $\alpha_i=\frac{1}{100}$ 이 되고, 그 합이 1이 되려면 그 최대값인 $\alpha_i$가 100개는 있어야 한다
	- 이는 $\alpha_i=\frac{1}{100}$ 인 것이 100개, $\alpha_i=0$ 인 것이 900개가 있다는 의미
	- 즉, $\nu$ 를 정함에 따라 적어도 $\nu l$ 개의 suppor vector를 가져야 한다는 의미 
- outside the hyperplane 이 최대 $\nu l$ 개까지 가능하다.  
	- 예를 들어 동일한 상황에서  $\alpha_i=\frac{1}{200}$ 인 것이 200개가 있다고 하면 SV의 수가 200개인데, $\alpha_i=\frac{1}{\nu l}=\frac{1}{100}$ 인 것이 하나도 없다고 하자. 그렇다면 $\xi_i>0$ 인 SV는 없는 것이다. 
	- 즉 margin을 넘어서 penalty를 적용받는 객체 (SV)의 최대 갯수는 $\nu l$ 개까지 이다. \
- 따라서 $\nu$ 의 역할은 <font style="color:skyblue">support vectors 비율 (fraction)의 하한 (lower bound)</font> 이면서 <font style="color:skyblue">errors ($\xi_i$라는 penalty를 적용받는 support vectors)의 비율의 상한 (upper bound)</font> 이다. 

예를 들어, $\nu=0.1$ 로 설정하면, 전체 정상 데이터의 10% 이상은 support vectors 임을 알 수 있고, penalty가 부여되는 support vectors의 최대 비율은 10%가 됨을 알 수 있다. 즉, (높은 $\nu$로 인해) support vectors가 많을수록 그리고 penalty를 부여받는 support vectors가 많을수록 정상 데이터의 영역을 원점으로부터 멀리 떨어지게 할 수 있다. 그리고 $\nu$ 가 작을수록 정상 영역의 boundary가 넓어져 generalization이 잘되고, $\nu$ 가 클수록 specailization에 특화된다. 따라서 $\nu$ 를 설정하면서 결과물에 대한 어느정도 추정이 가능하다. 

- $\nu$ 가 높을수록 분류 경계면 (decision boundary)는 더 복잡해진다 : 

![[Pasted image 20240119231224.png|center]]

## Support Vector Data Description

직관적인 이해는 1-SVM 보다 SVDD가 더 쉽다. 

- Feature space에서 모든 정상 데이터들을 감싸는 hypersphere를 찾는 것 - 가장 작은 것으로!

![[Pasted image 20240119232642.png|center]]

데이터가 주어졌을 때 hypersphere의 중심 $a$ 와 반지름 $R$ 을 찾아서 이 안에 데이터들을 모아야 한다. 그럼에도 불구하고 너무 동떨어진 객체 ($x_i$)에 대해서는 $\xi_i$ 라는 penalty를 부여하겠다는 것이다. SVDD에서의 미지수는 hypersphere의 중심인 $a$, 반지름 $R$, 개별 penalty $\xi_i$ 가 된다. 

위 그림을 통해서 1-SVM과 SVDD의 목적을 파악할 수 있다. 데이터가 주어졌을 때 1-SVM은 정상 데이터를 원점으로부터 멀리 위치하게끔 하는 hyperplane을 찾는 것이고, SVDD는 정상 데이터의 영역을 최대한 감싸는 hypersphere를 찾는 것이다. 

##### Optimization function : 

$$
\begin{aligned}
\min_{R,\,\mathbf{a},\,\xi_i}R^2&+C\sum_{i=1}^l\xi_i\\
s.t.\;||\Phi(\mathbf{x}_i)-\mathbf{a}||^2\le\; &R^2 + \xi_i,\quad \xi_i\ge 0,\quad \forall i.
\end{aligned}
$$
- C-SVM의 형태로 살펴보자. 
- 반지름 $R$를 최소화하는 compact한 구를 찾되 구가 감싸지 못하는 객체들에 대해서는 penalty를 부여
- 제약 : 개별 객체 ($\Phi(\mathbf{x}_i)$)에서 구의 중심 ($\mathbf{a}$) 까지의 거리의 제곱은 $R^2$ 보다 작아야하는데 구가 감싸지 못하면 penalty ($\xi_i$)를 부여함

##### Decision function : 
$$
f(\mathbf{x}) = sign(R^2 - ||\Phi(\mathbf{x}_i)-\mathbf{a}||^2)
$$

양수면 정상, 음수면 이상 데이터

##### Primal Lagrangian problem - Minimization 

$$
\begin{aligned}
L = R^2+C\sum_{i=1}^l\xi_i-\sum_{i=1}^l\alpha_i\Bigg(R^2+&\xi_i-\left(\Phi(\mathbf{x}_i)\cdot\Phi(\mathbf{x}_i)-2\mathbf{a}\cdot\Phi(\mathbf{x}_i)+\mathbf{a}\cdot\mathbf{a}\right)\Bigg)-\sum_{i=1}^l\beta_i\xi_i\\
&\alpha_i\ge0,\quad\beta_i\ge0
\end{aligned}
$$

##### KKT condition 

위 Primal Lagragian $L$ 을 미지수 $R$, $\mathbf{a}$, $\xi_i$에 대해 편미분하여 계산하면 아래와 같다 : 
$$
\begin{aligned}
\frac{\partial L}{\partial R} &= 2R-2R\sum_{i=1}^l\alpha_i=0 &&\Rightarrow\quad\sum_{i=1}^l\alpha_i=1\\
\frac{\partial L}{\partial \mathbf{a}} &= 2\sum_{i=1}^l\alpha_i\cdot\Phi(\mathbf{x}_i)-2\mathbf{a}\sum_{i=1}^l\alpha_i=0 &&\Rightarrow\quad \mathbf{a}=\sum_{i=1}^l\alpha_i\cdot\Phi(\mathbf{x}_i)\qquad \because\sum_{i=1}^l\alpha_i=1\\
\frac{\partial L}{\partial\xi_i} &= C-\alpha_i-\beta_i=0,\quad \forall i
\end{aligned}
$$

##### Dual Lagrangian problem - Maximization 

![[Pasted image 20240119235146.png|center]]

##### Dual Lagrangian problem - Minimization : 

$$
L = \sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_j\Phi(\mathbf{x}_i)\Phi(\mathbf{x}_j)-\sum_{i=1}^l\alpha_i\Phi(\mathbf{x}_i)\cdot\Phi(\mathbf{x}_i)\quad (0\le\alpha_i\le C)
$$

이제 우리가 알고 있는 제약은 아래와 같다 : 
$$
C-\alpha_i-\beta_i=0,\quad \beta_i\xi_i=0
$$

- Location of a point w.r.t. $\alpha_i$ : 
	- Case 1 (흰색 포인트) : $\alpha_i=0$ ⇒ $\beta_i=C>0$ ⇒ $\xi_i=0$ ⇒ a non-support vector (구의 내부에 있음)
	- Case 2 (검은색 포인트) : $\alpha_i=C$ ⇒ $\beta_i=0$ ⇒ $\xi_i>0$ ⇒ Support vector - outside the hypersphere
	- Case 3 (회색 포인트) : $0<\alpha_i<C$ ⇒ $\beta_i>0$ ⇒ $\xi_i=0$ ⇒ Support vector - on the hypersphere

![[Pasted image 20240119235809.png|center]]

- SVDD with Gaussian (rbf) kernels : $$K(\mathbf{x}_i,\mathbf{x}_j)=\exp\left(\frac{-||\mathbf{x}_i-\mathbf{x}_j||^2}{s^2}\right)$$
	- Kernel width $s$ 가 작을수록 flexible한 decision boundary가 만들어지고, 클수록 단순하게 만들어진다. 

![[Pasted image 20240120000033.png|center]]


- 만일 모든 데이터가 unit norm vector로 normalized 된다면 1-SVM과 SVDD는 동일하다 : 

![[Pasted image 20240120000133.png|center]]

- 1-SVM 과 마찬가지로, $\nu$ -SVDD도 가능하다
- 아래 그림에서 $D$ 가 $\nu$의 역할을 한다 : 

![[Pasted image 20240120000305.png|center]]

마찬가지로 $\nu$ 가 작을수록 generalization에 특화되고, 클수록 specialization에 집중된 boundary를 생성할 수 있다. 