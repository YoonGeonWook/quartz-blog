---
sticker: emoji//1f4d5
tags:
  - Anomaly_Detection
  - ì´ìƒíƒì§€
  - density-based
  - Gaussian
  - Mixture_of_Gaussian
banner: Anomaly Detection/image/gaussian.png
---
# ğŸ² Density-based Novelty Detection

## Purpose
- ì´ìƒì¹˜ íƒì§€ì˜ Abnormal dataì— ëŒ€í•œ ë‘ ë²ˆì§¸ ì •ì˜ì— ì¤‘ì ì„ ë‘” ë°©ì‹
	- ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í™œìš©í•´ì„œ ì •ìƒì¸ Normal dataê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ë¶„í¬ë¥¼ ë¨¼ì € ì¶”ì •í•œ ë‹¤ìŒì— ê·¸ ì¶”ì •ëœ ë¶„í¬ë¥¼ í†µí•´ì„œ ìƒˆë¡œìš´ ê°ì²´ê°€ ë“¤ì–´ì™”ì„ ë•Œ ê·¸ ê°ì²´ê°€ ë°œìƒí•  í™•ë¥ ì´ ë†’ìœ¼ë©´ Normal dataë¡œ íŒë³„í•˜ê³  ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš° Abnormal dataë¡œ íŒë³„í•˜ëŠ” ê²ƒì´ë‹¤.
- [x] Estimate the data-driven density function
- [x] If a new instance has a **low probability** according the trained density function, it will be identified as novel.
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112101704.png)
Figure 1 : 1ì°¨ì› ë°ì´í„°ì— ëŒ€í•´ì„œ ìœ„ì™€ ê°™ì´ íˆìŠ¤í† ê·¸ë¨ì´ ê·¸ë ¤ì§„ë‹¤ê³  í•˜ì. ì‹¤ì§ˆì ìœ¼ë¡œëŠ” ë¶„í¬ê°€ Gaussian ì¸ì§€ ì•„ë‹Œì§€ ì•Œ ìˆ˜ ì—†ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  Gaussian distributionì„ ê°€ì •ì„ í•˜ê³ , ë…¸ë€ìƒ‰ìœ¼ë¡œ í‘œì‹œëœ ê²ƒê³¼ ê°™ì€ ì •ê·œë¶„í¬ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ë¶„í¬ë¥¼ ì¶”ì •í•œ ë’¤ì— ìƒˆë¡œìš´/test ê°ì²´ê°€ íŒŒë€ìƒ‰ê³¼ ê°™ì´ ë¶„í¬ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” í™•ë¥ ì´ ë†’ì€ ê²½ìš°ì—ëŠ” Normalë¡œ íŒë‹¨ë˜ëŠ” ë°˜ë©´ ë¹¨ê°„ìƒ‰ê³¼ ê°™ì€ ê³³ì˜ ê²½ìš° Normal ë°ì´í„°ë¡œë¶€í„° ìƒì„±ë  í™•ë¥ ì´ ìƒë‹¹íˆ ë‚®ê¸° ë•Œë¬¸ì— Abnormal ë¡œ íŒì •í•œë‹¤.

![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112102715.png)

- Kernel Density Estimation : Parzen Window Density Estimation
	- ê°ê°ì˜ ê°ì²´ëŠ” ëª¨ë‘ Gaussian distributionì˜ ì¤‘ì‹¬ì„ì„ ê°€ì •í•˜ê³ , ê·¸ë¡œë¶€í„° ì£¼ì–´ì§„ ì •ìƒ ë°ì´í„° ì˜ì—­ì˜ ë°€ë„ í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ê² ë‹¤ëŠ” ê²ƒ 

# ğŸ² Gaussian Density Estimation

- **Assume** that the observed data are drawn from a Gaussian distribution
	- ì‹¤ì œê°€ ì•„ë‹ˆë¼ ê°€ì •ì„ í•˜ëŠ” ê²ƒì„!
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112103126.png)

$$
p(\mathbf{x}) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp\left[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})\right]
$$
- ì°¾ì•„ì•¼í•  ë¯¸ì§€ìˆ˜ (ëª¨ìˆ˜) : 
	- ì—¬ê¸°ì„œ $\mathbf{x}_i\in X^+$ ëŠ” Normal dataì˜ ì§‘í•© $X^+$ì˜ ì›ì†Œë“¤ë§Œì„ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ê²ƒ
	- mean vector : $\boldsymbol{\mu}=\frac{1}{n}\sum_{\mathbf{x}_i\in\mathbf{X}^+}\mathbf{x}_i$
	- covariance matrix : $\Sigma=\frac{1}{n}\sum_{\mathbf{x}_i\in\mathbf{X}^+}(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^T$ 
	- ê²°ë¡ ì ìœ¼ë¡œ normal data $X^+$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ normal dataì˜ í‰ê· ê³¼ ê³µë¶„ì‚°ì„ ê³„ì‚°í•´ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤


#### Advantages : 
1) **Insensitive to scaling of the data**
	- ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ê°€ ìˆë‹¤ê³  í•´ë³´ì : $$\begin{array}{cccc} & V_1 & V_2 & V_3 \\X_1 & 1.0 & 1000 & 0.01 \\X_2 & 1.1 & 980 & 0.02 \\\vdots & \vdots & \vdots & \vdots \\X_n & 0.9 & 1020 & 0.05\end{array}$$
	- Covariance matrixì˜ ì—­í–‰ë ¬ì„ ì´ìš©í•˜ê¸° ë•Œë¬¸ì— ë³€ìˆ˜ë³„ë¡œ ë‹¨ìœ„ê°€ ë‹¤ë¥¸ ê²ƒì´ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ = insensitive / robust
	- ì¦‰, ë³€ìˆ˜ì— ëŒ€í•œ normalizationì„ í•˜ì§€ ì•Šì•„ë„ë¨
2) **Possible to compute analyrically the optimal threshold**
	- rejectionì— ëŒ€í•œ ì œ 1ì¢…ì˜¤ë¥˜ë¥¼ ë¨¼ì € ì •ì˜í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì‹ ë¢°ìˆ˜ì¤€ 95%ê¹Œì§€ë§Œ í¬í•¨í•˜ê² ë‹¤ ë¼ê³  í•˜ë©´ ì²˜ìŒë¶€í„° ì£¼ì–´ì§„ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ 5% ì •ë„ëŠ” rejectionì´ ëœë‹¤ëŠ” ê²ƒ, ì‹¤ì œë¡œëŠ” ì •ìƒ ë°ì´í„°ì´ì§€ë§Œ abnormalë¡œ reject ë˜ëŠ” ê²ƒìŒ ê°ìˆ˜í•˜ê³  boundary/cut-offë¥¼ ê³„ì‚°í•œë‹¤ëŠ” ê²ƒì„
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112104053.png)

#### Maximum Likelihood Estimation ; MLE
- 1ì°¨ì› ë°ì´í„°ì¸ ê²½ìš°, ì‹¤ì§ˆì ìœ¼ë¡œ ì¶”ì •í•´ì•¼ í•  ë¯¸ì§€ìˆ˜/ëª¨ìˆ˜ : 
	- Parameter : $\mu$ and $\sigma^2$
	$$\begin{aligned}L&=\prod_{i=1}^NP(x_i|\mu,\sigma^2)=\prod_{i=1}^N\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)\\\log L&=-\frac{1}{2}\sum_{i=1}^N\frac{(x_i-\mu)^2}{\sigma^2}-\frac{N}{2}\log(2\pi \sigma^2)\end{aligned}$$
	- Let's set $\gamma = 1/\sigma^2$ : 
	$$\begin{aligned}\log L&=-\frac{1}{2}\sum_{i=1}^N\gamma(x_i-\mu)^2-\frac{N}{2}\log(2\pi)+\frac{N}{2}\log(\gamma) \\\frac{\partial\log L}{\partial\mu}&=\gamma\sum_{i=1}^N(x_i-\mu)=0\to \mu=\frac{1}{N}\sum_{i=1}^Nx_i\\\frac{\partial\log L}{\partial\gamma}&=-\frac{1}{2}\sum_{i=1}^N(x_i-\mu)^2+\frac{N}{2\gamma}=0\to\sigma^2=\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2\end{aligned}$$
		- **$\mu$ ì˜ ì¶”ì •ì¹˜ëŠ” Train dataì˜ normalë“¤ì˜ í‰ê· , $\sigma^2$ì˜ ì¶”ì •ì¹˜ëŠ” normalë“¤ì˜ ë¶„ì‚°**
- In general, (multivariate case) : 
$$\boldsymbol{\mu}=\frac{1}{N}\sum_{i=1}^N\mathbf{x}_i,\qquad \boldsymbol{\Sigma}=\frac{1}{N}\sum_{i=1}^N(\mathbf{x}_i-\boldsymbol{\mu})(\mathbf{x}_i-\boldsymbol{\mu})^T$$
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112110946.png)

- ì´ì œ ì—¬ê¸°ì—ì„œ í•œ ê°€ì§€ ì´ìŠˆê°€ ìˆëŠ”ë°, ì‹¤ì§ˆì ìœ¼ë¡œ ê³µë¶„ì‚° í–‰ë ¬ì„ ë³€ìˆ˜ì˜ ê°œìˆ˜ë§Œí¼ì˜ ì°¨ì›ì„ ê°–ëŠ” squared matrix ì´ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠëƒì— ë”°ë¼ì„œ ì¶”ì •ë˜ëŠ” ë¶„í¬ì˜ ëª¨ì–‘ (shape)ì´ ì•½ê°„ì”© ë‹¬ë¼ì§€ê²Œ ëœë‹¤. 
- The shape of Gaussian distribution according to the Covariance matrix type
	- **Spherical** : $$\Sigma=\sigma^2\left[\begin{array}{ccc}1 & \cdots & 0 \\\vdots & \ddots & \vdots \\0 & \cdots & 1\end{array}\right]$$
		- ëª¨ë“  ë³€ìˆ˜ê°€ ë™ì¼í•œ ë¶„ì‚°ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê°€ì •ì„. ì´ëŠ” ë³€ìˆ˜ë“¤ê°„ ë…ë¦½ì„ ê°€ì •í•˜ê³  ìˆìŒ
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112111249.png)

- **Diagonal** : $$\Sigma=\left[\begin{array}{ccc}\sigma_1^2 & \cdots & 0 \\\vdots & \ddots & \vdots \\0 & \cdots & \sigma_d^2\end{array}\right]$$
	- ì—¬ì „íˆ ë…ë¦½ì´ì§€ë§Œ ë³€ìˆ˜ë³„ë¡œ ë‹¤ë¥¸ ë¶„ì‚°ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112111456.png)
- **Full** : $$\Sigma=\left[\begin{array}{ccc}\sigma_{11} & \cdots & \sigma_{1 d} \\\vdots & \ddots & \vdots \\\sigma_{d 1} & \cdots & \sigma_{d d}\end{array}\right]$$
	- ë” ì´ìƒ ë³€ìˆ˜ë“¤ ê°„ ë…ë¦½ì´ ì•„ë‹Œ ì–´ëŠì •ë„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112111545.png)

ì—¬ê¸°ê¹Œì§€ê°€ ì „ì²´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ Gaussian ë¶„í¬ë¡œ ê°€ì •í•˜ê³ , ê·¸ë¡œë¶€í„° abnormal scoreë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ë•Œ abnormal scoreë¼ í•¨ì€ ì¶”ì •ëœ pdfë¡œ ë¶€í„° ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ì‚°ì¶œë˜ëŠ” í™•ë¥ ë¶„í¬ì˜ ê°’ì´ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ê·¸ ê°’ì´ ë‚®ì„ìˆ˜ë¡ abnormalì¼ í™•ë¥ ì´ ì»¤ì§€ê³ , ê·¸ ê°’ì´ ë†’ì„ìˆ˜ë¡ abnormalì¼ í™•ë¥ ì€ ë‚®ì•„ì§„ë‹¤. 
- Anomaly Score = 1 - p(x)
# ğŸ² Mixture of Gaussian Density Estimation

- Mixture of Gaussian (MoG) Density Estimation : 
	- Gaussian Density Estimation - assumes **a very strong model** of the data : <font style="color:blue">unimodal and convex</font>
	- MoG : 
		- an extension of Gaussian that allows <font style="color:blue">multi-modal</font> distribution
		- <font style="color:blue">a linear combination of normal distributions</font>
		- Has a smaller bias than the single Gaussian distribution, but requires far more data for training
			- unimomalì˜ ê²½ìš°ì—ëŠ” í‰ê· ê³¼ ë¶„ì‚° 2ê°œë§Œ ê³„ì‚°í•˜ë©´ ë˜ì—ˆì§€ë§Œ, MoGì—ì„œëŠ” ê° ê°€ìš°ì‹œì•ˆì— ëŒ€í•´ì„œ $w_i,\mu_i,\sigma_i$ ë¥¼ ì¶”ì •í•´ì•¼ ë˜ë¯€ë¡œ Kê°œì˜ Gaussianì„ ì‚¬ìš©í•œë‹¤ë©´ $3\times K$ê°œë¥¼ ê³„ì‚°í•´ì•¼ í•˜ê³ , ì ì ˆí•œ $K$ì— ëŒ€í•´ train dataì—ì„œ íƒìƒ‰í•´ì•¼ í•¨
$$
f(x) = w_1\cdot \mathcal{N}(\mu_1,\sigma_1^2) + w_2\cdot \mathcal{N}(\mu_2,\sigma_2^2) + w_3\cdot \mathcal{N}(\mu_3,\sigma_3^2)
$$
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112112338.png)

#### MoG example

![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112112732.png)

#### Components of MoG : 
- ì–´ë–¤ ê°ì²´ê°€ normal classì— ì†í•  í™•ë¥  : $$p(\mathbf{x}|\lambda)=\sum_{m=1}^Mw_mg(\mathbf{x}|\boldsymbol{\mu}_m,\boldsymbol{\Sigma}_m)$$
- ì´ ë•Œ $g$ ëŠ” ê°ê°ì˜ Gaussian modelì´ê³ , $\lambda$ ëŠ” ë¯¸ì§€ìˆ˜ë“¤ì˜ ì§‘í•©ì´ë‹¤ ($M$ : Gaussian modelì˜ ìˆ˜) : $$
\begin{gathered}
g\left(\mathbf{x} \mid \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m\right)=\frac{1}{(2 \pi)^{d / 2}\left|\boldsymbol{\Sigma}_m\right|^{1 / 2}} \exp \left[-\frac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}_m\right)^T \boldsymbol{\Sigma}_m^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_m\right)\right] \\\\
\lambda=\left\{w_m, \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m\right\}, \quad m=1, \cdots, M
\end{gathered}
$$
#### Expectation-Maximization Algorithm (EM algorithm)

MLì—ì„œ ë¯¸ì§€ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ë²•ë¡  ì¤‘ ëŒ€í‘œì ì¸ gradient-descent ì•Œê³ ë¦¬ì¦˜ê³¼ ë”ë¶ˆì–´ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” EM algorithmì´ë‹¤. 

í˜„ì¬ ìš°ë¦¬ê°€ ì¶”ì •í•´ì•¼ í•˜ëŠ” ë¯¸ì§€ìˆ˜ familyê°€ Aì™€ Bë¼ëŠ”ê²Œ ìˆë‹¤ê³  í•  ë•Œ, Aì™€ BëŠ” ë™ì‹œì— ìµœì í™”í•  ìˆ˜ê°€ ì—†ëŠ” ìƒí™©ì´ë¼ê³  í•˜ì. ê·¸ëŸ¬ë©´ ì´ ì‘ì—…ì„ í•˜ë ¤ë©´ Aë¥¼ ê³ ì •ì‹œí‚¤ê³ , Bë§Œ ìµœì í™”í•œë‹¤. ê·¸ëŸ° ë‹¤ìŒ Bë¥¼ ê³ ì •í•˜ê³  Aë¥¼ ìµœì í™”í•œë‹¤. Aê°€ ë§Œì•½ ë°”ë€Œì—ˆë‹¤ë©´ / ê°±ì‹ ë˜ì—ˆë‹¤ë©´ ê·¸ê²ƒì„ ê³ ì •í•˜ê³  Bë¥¼ ìµœì í™”í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´, Aì™€ Bê°€ ë¶ˆë³€/ìˆ˜ë ´í•˜ê²Œ ëœë‹¤.

- MoGì—ì„œ ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²• : 
	- A : $p(m|x)$ - ê°ì²´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $m$ ë²ˆì§¸ Gaussian ë¶„í¬ì˜ í™•ë¥ 
	- B : $w_m,\mu_m,\Sigma_m$ 

- <font style="color:blue">E-step</font> : Given the current estimate of the parameters, compute the conditional probabilities
- <font style="color:blue">M-step</font> : Update the parameters to maximize the expected likelihood found in the E-step
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112113924.png)E-stepì—ì„œëŠ” Gaussianì„ ê³ ì •í•˜ê³  ê° Gaussianì— ì†í•  ê°ì²´ì˜ í™•ë¥ ì„êµ¬í•˜ê³ , M-stepì—ì„œëŠ” ê°ì²´ì˜ í™•ë¥ ì„ ê³ ì •í•˜ê³ , Gaussian parameterë¥¼ updateí•˜ëŠ” ê²ƒ

- Example : 
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/EM.gif)  
ìœ„ ì‹œë®¬ë ˆì´ì…˜ì„ ë³´ë©´ $M=2$ê°œì¸ ê²½ìš°ë¡œ, ì²˜ìŒì—ëŠ” ë¶€ì ì ˆí•œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ê°–ë‹¤ê°€ ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ì ì ˆí•œ ë¶„í¬ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ëª‡ ê°œì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬, ì¦‰ $M$ì„ ëª‡ìœ¼ë¡œ í•´ì•¼ ê°€ì¥ ì¢‹ì€ì§€ëŠ” ëª¨ë¥´ê¸° ë•Œë¬¸ì— $M$ ì„ ë°”ê¾¸ì–´ê°€ë©° EM algorithmì„ ìˆ˜í–‰í•˜ê³  likelihood $L$ì„ ìµœëŒ€í™”í•˜ëŠ” $M$ì„ ì„ íƒí•´ì•¼ í•œë‹¤.

#### EM algorithm for MoG 

- Expectation (E-step) : $$p\left(m \mid \mathbf{x}_i, \lambda\right)=\frac{w_m g\left(\mathbf{x}_i \mid \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m\right)}{\sum_{k=1}^M w_k g\left(\mathbf{x}_t \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)}$$
	- ë¶„ì - $m$ë²ˆì§¸ ë¶„í¬ì—ì„œ ìƒì„±ë  í™•ë¥ 
	- ë¶„ëª¨ - $M$ê°œì˜ ëª¨ë“  ë¶„í¬ì—ì„œ ìƒì„±ë  í™•ë¥ ì˜ í•©
	- ì—¬ê¸°ì„œ $w_m, \mu_m$, $\Sigma_m$ ì€ ìµœì ì´ ì•„ë‹Œ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í•˜ê³  ì§„í–‰
- Maximization (M-step) : 
	- Mixture weight $$w_m^{(\text {new })}=\frac{1}{N} \sum_{i=1}^N p\left(m \mid \mathbf{x}_i, \lambda\right)$$
	- Means and Variances$$\boldsymbol{\mu}_m^{(n e w)}=\frac{\sum_{i=1}^N p\left(m \mid \mathbf{x}_i, \lambda\right) \mathbf{x}_i}{\sum_{i=1}^N p\left(m \mid \mathbf{x}_i, \lambda\right)}, \quad \sigma_m^{2(n e w)}=\frac{\sum_{i=1}^N p\left(m \mid \mathbf{x}_i, \lambda\right) \mathbf{x}_i^2}{\sum_{i=1}^N p\left(m \mid \mathbf{x}_i, \lambda\right)}-\boldsymbol{\mu}_m^{2(n e w)}$$
	- ì´ ë•Œ $p(m|\mathbf{x}_i,\lambda)$ëŠ” ê³ ì •
- The shape of MoG according to the covariance matrix : 
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112120809.png)
![](assets/(Mixture%20of)%20Gaussian%20Density%20Estimation/Pasted%20image%2020240112120818.png)

- MoGì—ì„œë„ Full covarianceê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì´ì§€ë§Œ, Full covariance $\Sigma$ ê°€ non-singularì¼ ë•Œë§Œ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— í˜„ì‹¤ì ìœ¼ë¡œëŠ” ì–´ë ¤ì›€ì´ ìˆë‹¤. ê°€ì¥ ì¢‹ì€ ëŒ€ì•ˆì€ Diagonal covarianceë¡œ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤.